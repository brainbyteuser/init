<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: http://leoeditor.com/leo_toc.html -->
<leo_file xmlns:leo="http://leoeditor.com/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="jonathanhudson.20200131125633.1"><vh>@settings</vh>
<v t="jonathanhudson.20200131123442.1"><vh>@bool tree-declutter = True</vh></v>
<v t="jonathanhudson.20200131123459.1"><vh>@data tree-declutter-patterns</vh>
<v t="jonathanhudson.20200131125049.1"><vh>declutter: demo pattern</vh></v>
<v t="jonathanhudson.20200214080742.1"><vh>Highligh Questions in Red</vh></v>
<v t="jonathanhudson.20200131124925.1"><vh>Check Jira Header</vh></v>
</v>
</v>
<v t="jonathanhudson.20201006084250.1"><vh>Tech</vh>
<v t="jonathanhudson.20201006084250.2"><vh>Leo Editor</vh></v>
<v t="jonathanhudson.20201006084250.3"><vh>Ansible</vh>
<v t="jonathanhudson.20201006084250.4"><vh>adreload</vh></v>
<v t="jonathanhudson.20201006084250.5"><vh>ansible commands</vh></v>
<v t="jonathanhudson.20201006084250.6"><vh>ansible dzdo home folder</vh></v>
<v t="jonathanhudson.20201006084250.7"><vh>ansible inline dicts</vh></v>
<v t="jonathanhudson.20201006084250.8"><vh>ansible priviledged access</vh></v>
<v t="jonathanhudson.20201006084250.9"><vh>ansible sudoers</vh></v>
<v t="jonathanhudson.20201006084250.10"><vh>ansible test</vh></v>
<v t="jonathanhudson.20201006084250.11"><vh>bulgaria build steps</vh></v>
<v t="jonathanhudson.20201006084250.12"><vh>callback_plugins</vh></v>
<v t="jonathanhudson.20201006084250.13"><vh>centrify permissions</vh></v>
<v t="jonathanhudson.20201006084250.14"><vh>commands</vh></v>
<v t="jonathanhudson.20201006084250.15"><vh>copy files to location and reconcile using rsync</vh></v>
<v t="jonathanhudson.20201006084250.16"><vh>copy files to location and reconcile using zip</vh></v>
<v t="jonathanhudson.20201006084250.17"><vh>cygwin ansible readme</vh></v>
<v t="jonathanhudson.20201006084250.18"><vh>cygwin64</vh>
<v t="jonathanhudson.20201006084250.19"><vh>etc</vh>
<v t="jonathanhudson.20201006084250.20"><vh>ansible</vh>
<v t="jonathanhudson.20201006084250.21"><vh>group_vars</vh></v>
</v>
</v>
<v t="jonathanhudson.20201006084250.22"><vh>home</vh>
<v t="jonathanhudson.20201006084250.23"><vh>jonathanhudson</vh>
<v t="jonathanhudson.20201006084250.24"><vh>.ssh</vh></v>
<v t="jonathanhudson.20201006084250.25"><vh>ansible</vh>
<v t="jonathanhudson.20201006084250.26"><vh>commands</vh></v>
<v t="jonathanhudson.20201006084250.27"><vh>filtering ansible dicts</vh></v>
<v t="jonathanhudson.20201006084250.28"><vh>hello world</vh></v>
<v t="jonathanhudson.20201006084250.29"><vh>howto shell with_items show all output</vh></v>
<v t="jonathanhudson.20201006084250.30"><vh>howto write to logfile</vh></v>
<v t="jonathanhudson.20201006084250.31"><vh>linux priv test add safe mon test user</vh></v>
<v t="jonathanhudson.20201006084250.32"><vh>test echo</vh></v>
<v t="jonathanhudson.20201006084250.33"><vh>test file exists</vh></v>
<v t="jonathanhudson.20201006084250.34"><vh>test find configure properties</vh></v>
<v t="jonathanhudson.20201006084250.35"><vh>test hello world</vh></v>
<v t="jonathanhudson.20201006084250.36"><vh>test priv echo dev_all v1</vh></v>
<v t="jonathanhudson.20201006084250.37"><vh>test priv echo dev_d3s v1</vh></v>
<v t="jonathanhudson.20201006084250.38"><vh>test priv echo linux</vh></v>
<v t="jonathanhudson.20201006084250.39"><vh>test priv echo uatdb v2</vh></v>
<v t="jonathanhudson.20201006084250.40"><vh>test priv echo uatdb v3</vh></v>
<v t="jonathanhudson.20201006084250.41"><vh>test priv echo uatdb v4</vh></v>
<v t="jonathanhudson.20201006084250.42"><vh>test priv echo uatdb</vh></v>
<v t="jonathanhudson.20201006084250.43"><vh>test priv echo</vh></v>
<v t="jonathanhudson.20201006084250.44"><vh>test priv who</vh></v>
</v>
<v t="jonathanhudson.20201006084250.45"><vh>code</vh>
<v t="jonathanhudson.20201006084250.46"><vh>cygwin sshpass</vh>
<v t="jonathanhudson.20201006084250.47"><vh>test</vh></v>
<v t="jonathanhudson.20201006084250.48"><vh>sshpass 1.05</vh>
<v t="jonathanhudson.20201006084250.49"><vh>.deps</vh></v>
</v>
</v>
</v>
<v t="jonathanhudson.20201006084250.50"><vh>scripts</vh>
<v t="jonathanhudson.20201006084250.51"><vh>logs</vh></v>
</v>
</v>
</v>
</v>
<v t="jonathanhudson.20201006084250.52"><vh>d3s deployment version override</vh></v>
<v t="jonathanhudson.20201006084250.53"><vh>enable ssh accounts login</vh></v>
<v t="jonathanhudson.20201006084250.54"><vh>filter list and make unique</vh></v>
<v t="jonathanhudson.20201006084250.55"><vh>filtering ansible dicts</vh></v>
<v t="jonathanhudson.20201006084250.56"><vh>fix change home folder for ansible</vh></v>
<v t="jonathanhudson.20201006084250.57"><vh>howto shell with_items show all output</vh></v>
<v t="jonathanhudson.20210301114224.1"><vh>installation</vh></v>
<v t="jonathanhudson.20201006084250.58"><vh>Linux</vh></v>
<v t="jonathanhudson.20201006084250.59"><vh>move files using rsync</vh></v>
<v t="jonathanhudson.20201006084250.60"><vh>pull dev tamper token files</vh></v>
<v t="jonathanhudson.20201006084250.61"><vh>pull prd tamper token files</vh></v>
<v t="jonathanhudson.20201006084250.62"><vh>rename multiple folders cygwin</vh></v>
<v t="jonathanhudson.20210907100423.1"><vh>requesting a new user email</vh></v>
<v t="jonathanhudson.20201006084250.63"><vh>running ansible playbooks</vh></v>
<v t="jonathanhudson.20201006084250.64"><vh>running ansible using sudoers</vh></v>
<v t="jonathanhudson.20201006084250.65"><vh>running ansible using sudoers complete</vh></v>
<v t="jonathanhudson.20201006084250.66"><vh>running shell</vh></v>
<v t="jonathanhudson.20201006084250.67"><vh>smoke tests check result</vh></v>
<v t="jonathanhudson.20201006084250.68"><vh>sql insert postgres</vh></v>
<v t="jonathanhudson.20201006084250.69"><vh>test list filter</vh></v>
<v t="jonathanhudson.20201006084250.70"><vh>vault password</vh></v>
</v>
<v t="jonathanhudson.20210816151808.1"><vh>C Programming</vh>
<v t="jonathanhudson.20210816152317.1"><vh>Debugging</vh></v>
</v>
<v t="jonathanhudson.20201006092251.1"><vh>Chrome</vh></v>
<v t="jonathanhudson.20201006084250.71"><vh>Contacts</vh></v>
<v t="jonathanhudson.20201006084250.72"><vh>Git</vh>
<v t="jonathanhudson.20210301145653.1"><vh>beyond compare</vh></v>
<v t="jonathanhudson.20210611122815.1"><vh>check before merge into release</vh></v>
<v t="jonathanhudson.20201006084250.73"><vh>commands</vh></v>
<v t="jonathanhudson.20210301153511.1"><vh>initial setup</vh></v>
<v t="jonathanhudson.20201006084250.74"><vh>migrate a repo</vh></v>
<v t="jonathanhudson.20210426141656.1"><vh>detach repo remove remote</vh></v>
<v t="jonathanhudson.20201006084250.75"><vh>sorting tags finding commits</vh></v>
<v t="jonathanhudson.20201006084250.76"><vh>submodules</vh></v>
<v t="jonathanhudson.20210621085608.1"><vh>tags listing</vh></v>
</v>
<v t="jonathanhudson.20201006084250.77"><vh>Gluster FS</vh>
<v t="jonathanhudson.20201006084250.78"><vh>Task Overview</vh></v>
<v t="jonathanhudson.20201006084250.79"><vh>Installation</vh></v>
<v t="jonathanhudson.20201006084250.80"><vh>Testing</vh></v>
<v t="jonathanhudson.20201006084250.81"><vh>Commands</vh></v>
<v t="jonathanhudson.20201006084250.82"><vh>Knowledge Base</vh></v>
<v t="jonathanhudson.20201006084250.83"><vh>Heal Investigations</vh>
<v t="jonathanhudson.20210115085519.1"><vh>Check files using find</vh></v>
<v t="jonathanhudson.20201006084250.84"><vh>Uat Dk Outstanding heal entries won't clear</vh></v>
</v>
<v t="jonathanhudson.20201006084250.85"><vh>Performance Stats</vh></v>
<v t="jonathanhudson.20201006084250.86"><vh>Deployment Tasks</vh>
<v t="jonathanhudson.20201006084250.87"><vh>Request firewall opening between FSH and REP / CAP servers ( TCP/UDP 24007, 24008, TCP 49152 - 49156 )</vh></v>
<v t="jonathanhudson.20201006084250.88"><vh>Check servers have valid RedHat subscription FSH, REP, CAP</vh></v>
<v t="jonathanhudson.20201006084250.89"><vh>Install GlusterFS on FSH servers</vh></v>
<v t="jonathanhudson.20201006084250.90"><vh>Install Fuse client on REP / CAP servers</vh></v>
<v t="jonathanhudson.20201006084250.91"><vh>Update the fstab file on REP / CAP servers ( add GFS_DATA_xx mounts )</vh></v>
<v t="jonathanhudson.20201006084250.92"><vh>Create mount folders on REP / CAP servers ( GFS_DATA_xx )</vh></v>
<v t="jonathanhudson.20201006084250.93"><vh>Create zero permission files in mount points for each jurisdiction for protection</vh></v>
<v t="jonathanhudson.20201006084250.94"><vh>Create brick folders with permissions user:d3s 775</vh></v>
<v t="jonathanhudson.20201006084250.95"><vh>Mount GFS volumes on REP / CAP servers</vh></v>
<v t="jonathanhudson.20201006084250.96"><vh>Create jurisdiction folders on GFS_DATA_xx mounts</vh></v>
<v t="jonathanhudson.20201006084250.97"><vh>Create the sudoers files on REP servers</vh></v>
<v t="jonathanhudson.20201006084250.98"><vh>Deploy updated Safe-Mon with GFS monitors</vh></v>
<v t="jonathanhudson.20201006084250.99"><vh>Update configure.properties to add GFS to FileCopy for each jurisdiction in NO_OPERATION mode</vh></v>
<v t="jonathanhudson.20201006084250.100"><vh>Update git version of xx-configuration.xml in git</vh></v>
</v>
<v t="jonathanhudson.20201006084250.101"><vh>Firewalls</vh></v>
<v t="jonathanhudson.20201006084250.102"><vh>Scripts</vh>
<v t="jonathanhudson.20201006084250.103"><vh>glusterrm.sh</vh></v>
<v t="jonathanhudson.20201006084250.104"><vh>disable-gfs-network.sh</vh></v>
<v t="jonathanhudson.20201006084250.105"><vh>enable-gfs-network.sh</vh></v>
<v t="jonathanhudson.20201006084250.106"><vh>delete-gluster-vol.sh</vh></v>
</v>
<v t="jonathanhudson.20201006084250.107"><vh>Tasks</vh>
<v t="jonathanhudson.20201006084250.108"><vh>Safe Monitoring</vh>
<v t="jonathanhudson.20201006084250.109"><vh>Notes</vh></v>
<v t="jonathanhudson.20201006084250.110"><vh>Resync safe data with gfs volumes</vh></v>
<v t="jonathanhudson.20201006084250.111"><vh>Tasks</vh>
<v t="jonathanhudson.20201006084250.112"><vh>Monitor GFS to ensure that errors are detected appropriately</vh></v>
<v t="jonathanhudson.20201006084250.113"><vh>Add monitors to ensure the gfs rsync monitors are running</vh></v>
</v>
</v>
<v t="jonathanhudson.20201006084250.114"><vh>DK XML file copy</vh>
<v t="jonathanhudson.20201006084250.115"><vh>Notes</vh></v>
<v t="jonathanhudson.20201006084250.116"><vh>Rsync Validation Checks XML</vh></v>
<v t="jonathanhudson.20201006084250.117"><vh>Scripts</vh>
<v t="jonathanhudson.20201006084250.118"><vh>rsync-dk-gfs.sh</vh></v>
</v>
<v t="jonathanhudson.20201006084250.119"><vh>Tasks</vh>
<v t="jonathanhudson.20201006084250.120"><vh>See if there is a better way to sync xml files</vh></v>
<v t="jonathanhudson.20201006084250.121"><vh>Add rsync script to copy xml files</vh></v>
<v t="jonathanhudson.20201006084250.122"><vh>Update safe-mon file sync monitor to detect xml files</vh></v>
<v t="jonathanhudson.20201006084250.123"><vh>Work on File Sync Monitor regular expression</vh></v>
<v t="jonathanhudson.20201006084250.124"><vh>Investigate discrepencies with xml files</vh></v>
</v>
</v>
<v t="jonathanhudson.20201006084250.125"><vh>ES Anteriores file copy</vh>
<v t="jonathanhudson.20201006084250.126"><vh>Notes</vh></v>
<v t="jonathanhudson.20201006084250.127"><vh>Rsync Validation Checks Anteriores</vh></v>
<v t="jonathanhudson.20201006084250.128"><vh>Scripts</vh>
<v t="jonathanhudson.20201006084250.129"><vh>rsync-es-gfs.sh</vh></v>
</v>
<v t="jonathanhudson.20201006084250.130"><vh>Tasks</vh>
<v t="jonathanhudson.20201006084250.131"><vh>Add rsync script to copy anteriores files</vh></v>
<v t="jonathanhudson.20201006084250.132"><vh>Update safe-mon file sync monitor to detect xml files</vh></v>
</v>
</v>
</v>
</v>
<v t="jonathanhudson.20201006084250.133"><vh>Java</vh>
<v t="jonathanhudson.20210422110307.1"><vh>change jira cert</vh></v>
<v t="jonathanhudson.20201006084250.134"><vh>compare collections</vh></v>
<v t="jonathanhudson.20201006084250.135"><vh>copy file java 4 ways</vh></v>
<v t="jonathanhudson.20210927153358.1"><vh>debugging SSL errors</vh></v>
<v t="jonathanhudson.20201006084250.136"><vh>generate xml to hash used in spain</vh></v>
<v t="jonathanhudson.20201006084250.137"><vh>heap dump on out of memory error</vh></v>
<v t="jonathanhudson.20201006084250.138"><vh>http download zip on fly</vh></v>
<v t="jonathanhudson.20201006084250.139"><vh>java check heap cmdline</vh></v>
<v t="jonathanhudson.20201006084250.140"><vh>java thread dump</vh></v>
<v t="jonathanhudson.20201006084250.141"><vh>log4j2 programmatic changes</vh></v>
<v t="jonathanhudson.20201006084250.142"><vh>maven build command line</vh></v>
<v t="jonathanhudson.20201006084250.143"><vh>open clover cmd line</vh></v>
<v t="jonathanhudson.20201006084250.144"><vh>open clover cmd secure file transfer</vh></v>
<v t="jonathanhudson.20201006084250.145"><vh>random password generator</vh></v>
<v t="jonathanhudson.20201006084250.146"><vh>rmi connection</vh>
<v t="jonathanhudson.20210906095043.1"><vh>jmx connection permission denied</vh></v>
</v>
<v t="jonathanhudson.20201006084250.147"><vh>scheduled thread pool executor</vh></v>
<v t="jonathanhudson.20201006084250.148"><vh>thread dump types</vh></v>
<v t="jonathanhudson.20201006084250.149"><vh>thread localexample</vh></v>
<v t="jonathanhudson.20201006084250.150"><vh>tomcat web services</vh></v>
<v t="jonathanhudson.20210609084921.1"><vh>tomcat disable wsdl status</vh></v>
</v>
<v t="jonathanhudson.20201006084250.151"><vh>Jursdictions</vh>
<v t="jonathanhudson.20201006084250.152"><vh>Bulgaria</vh>
<v t="jonathanhudson.20201006084250.153"><vh>d3sc single request</vh></v>
</v>
<v t="jonathanhudson.20201218084806.1"><vh>Colombia</vh></v>
<v t="jonathanhudson.20201006084250.154"><vh>Denmark</vh>
<v t="jonathanhudson.20201006084250.155"><vh>tamper token test</vh></v>
</v>
<v t="jonathanhudson.20210309121030.1"><vh>Netherlands</vh>
<v t="jonathanhudson.20210309121037.1"><vh>Extract and decrypt file</vh>
<v t="jonathanhudson.20210518103750.1"><vh>BatchValidator.properties</vh></v>
</v>
</v>
</v>
<v t="jonathanhudson.20201006084250.156"><vh>Kognitio</vh>
<v t="jonathanhudson.20201007075718.1"><vh>AP node db capabilities</vh></v>
<v t="jonathanhudson.20201006132701.1"><vh>Architecture</vh></v>
<v t="jonathanhudson.20201007162331.1"><vh>Backups</vh>
<v t="jonathanhudson.20201007162604.1"><vh>wxbackup</vh></v>
<v t="jonathanhudson.20201008103615.1"><vh>wxrestore</vh></v>
</v>
<v t="jonathanhudson.20201006084250.171"><vh>Build</vh>
<v t="jonathanhudson.20210817154805.1"><vh>Quick build</vh></v>
</v>
<v t="jonathanhudson.20201006084250.157"><vh>Client Tool</vh></v>
<v t="jonathanhudson.20201006084250.158"><vh>Command line</vh>
<v t="jonathanhudson.20201007142818.1"><vh>Stopping and starting</vh></v>
<v t="jonathanhudson.20201007160629.1"><vh>Target filters</vh></v>
<v t="jonathanhudson.20201007142124.1"><vh>wxsubmit</vh>
<v t="jonathanhudson.20201008124905.1"><vh>Predefined wxsuibmit queries</vh></v>
</v>
</v>
<v t="jonathanhudson.20210818130028.1"><vh>Compiler</vh>
<v t="jonathanhudson.20210818130036.1"><vh>Standalone</vh></v>
<v t="jonathanhudson.20210819153406.1"><vh>Reverse Polish</vh></v>
<v t="jonathanhudson.20210819153018.1"><vh>SCode</vh></v>
</v>
<v t="jonathanhudson.20201006084250.159"><vh>Configuration file</vh></v>
<v t="jonathanhudson.20201007111940.1"><vh>Comma Seperated Values</vh></v>
<v t="jonathanhudson.20201006084250.160"><vh>Debug</vh>
<v t="jonathanhudson.20201007073358.1"><vh>Capturing debug information</vh></v>
<v t="jonathanhudson.20201008112647.1"><vh>Capturing debug information on running queries</vh></v>
<v t="jonathanhudson.20201008122547.1"><vh>Capture a replay log for a query</vh></v>
<v t="jonathanhudson.20201008114802.1"><vh>Collection information after a crash</vh></v>
<v t="jonathanhudson.20201007113221.1"><vh>Debugging wx2 crashes</vh></v>
<v t="jonathanhudson.20210824153105.1"><vh>Adding debug lines to the code</vh></v>
</v>
<v t="jonathanhudson.20201006084250.161"><vh>Errors</vh>
<v t="jonathanhudson.20201008125129.1"><vh>Collecting details for specific errors</vh></v>
<v t="jonathanhudson.20201008125511.1"><vh>Hung or crash low memory</vh></v>
</v>
<v t="jonathanhudson.20201009081854.1"><vh>Gdb Debugger</vh></v>
<v t="jonathanhudson.20201007113949.1"><vh>How To</vh>
<v t="jonathanhudson.20201007162947.1"><vh>Reclaim space on logging slab 2</vh></v>
<v t="jonathanhudson.20201007163100.1"><vh>Query header row for table</vh></v>
</v>
<v t="jonathanhudson.20201006084250.162"><vh>Initial notes</vh></v>
<v t="jonathanhudson.20201007160933.1"><vh>Installation</vh>
<v t="jonathanhudson.20201007162111.1"><vh>Changing the management link network interface</vh></v>
<v t="jonathanhudson.20201007160944.1"><vh>Disable swap</vh></v>
</v>
<v t="jonathanhudson.20201006155205.1"><vh>Jargon</vh>
<v t="jonathanhudson.20201007161915.1"><vh>Reconfigure</vh></v>
</v>
<v t="jonathanhudson.20201006150454.1"><vh>Knowledge base</vh>
<v t="jonathanhudson.20201007110752.1"><vh>Altering tables</vh></v>
<v t="jonathanhudson.20201007092220.1"><vh>Checklist</vh></v>
<v t="jonathanhudson.20201007111129.1"><vh>Create table image slow</vh></v>
<v t="jonathanhudson.20201007091424.1"><vh>Fragmented Tables</vh></v>
<v t="jonathanhudson.20201007143028.1"><vh>Find free space</vh></v>
<v t="jonathanhudson.20201007091617.1"><vh>Hanging Queries</vh></v>
<v t="jonathanhudson.20201007085852.1"><vh>Locking issues</vh></v>
<v t="jonathanhudson.20201007091308.1"><vh>Missing join conditions</vh></v>
<v t="jonathanhudson.20201007094724.1"><vh>Out of memory process killer</vh></v>
<v t="jonathanhudson.20201008121853.1"><vh>RPC port clashes</vh></v>
<v t="jonathanhudson.20201007090731.1"><vh>Skewing Queries</vh></v>
<v t="jonathanhudson.20201007104730.1"><vh>Query Results</vh></v>
<v t="jonathanhudson.20201007085952.1"><vh>Queuing</vh></v>
<v t="jonathanhudson.20201007090528.1"><vh>Update Statistics</vh></v>
</v>
<v t="jonathanhudson.20201006084250.163"><vh>License tool build</vh>
<v t="jonathanhudson.20201006084250.164"><vh>Creating new licences for MIS</vh></v>
</v>
<v t="jonathanhudson.20201007111229.1"><vh>Locking</vh>
<v t="jonathanhudson.20201007111714.1"><vh>Investigating locking issues</vh></v>
</v>
<v t="jonathanhudson.20201007095015.1"><vh>Memory</vh>
<v t="jonathanhudson.20201007142238.1"><vh>List temp ram per session</vh></v>
<v t="jonathanhudson.20210401090111.1"><vh>Persistent Memory Area</vh>
<v t="jonathanhudson.20210401090719.1"><vh>Adjust limits of shared memory area</vh></v>
</v>
</v>
<v t="jonathanhudson.20201007092619.1"><vh>Messaging</vh></v>
<v t="jonathanhudson.20201007072520.1"><vh>Monitor</vh>
<v t="jonathanhudson.20201008114136.1"><vh>Check Kognitio is running</vh></v>
<v t="jonathanhudson.20201007074942.1"><vh>Granting user access to  monitoring control tower</vh></v>
<v t="jonathanhudson.20201007105812.1"><vh>Initialising disks</vh></v>
<v t="jonathanhudson.20201007153823.1"><vh>Disk usage</vh></v>
<v t="jonathanhudson.20201007141933.1"><vh>Status of SQL queries</vh></v>
</v>
<v t="jonathanhudson.20201007104419.1"><vh>Networking</vh>
<v t="jonathanhudson.20201006084250.165"><vh>Network bandwidth</vh></v>
<v t="jonathanhudson.20201007104430.1"><vh>Subnets for multiple ethernet devices</vh></v>
</v>
<v t="jonathanhudson.20201008115752.1"><vh>Nodes</vh>
<v t="jonathanhudson.20201008115754.1"><vh>Removing a faulty node for analysis</vh></v>
<v t="jonathanhudson.20201008120118.1"><vh>Replacing a faulty node</vh></v>
</v>
<v t="jonathanhudson.20201006084250.166"><vh>Patches</vh></v>
<v t="jonathanhudson.20201006084250.167"><vh>Performance tests</vh>
<v t="jonathanhudson.20201007162651.1"><vh>Query timing</vh></v>
<v t="jonathanhudson.20201007161028.1"><vh>Test read write performance</vh></v>
<v t="jonathanhudson.20201008120544.1"><vh>Writing efficient queries</vh></v>
</v>
<v t="jonathanhudson.20201006084250.168"><vh>Prerequisite checks</vh></v>
<v t="jonathanhudson.20201007104651.1"><vh>Reclaim</vh></v>
<v t="jonathanhudson.20210824152921.1"><vh>Reverse Polish</vh></v>
<v t="jonathanhudson.20210824152956.1"><vh>S Code</vh></v>
<v t="jonathanhudson.20201007095558.1"><vh>Slabs</vh>
<v t="jonathanhudson.20201007100941.1"><vh>Sequel</vh></v>
</v>
<v t="jonathanhudson.20201007092421.1"><vh>Scripts</vh>
<v t="jonathanhudson.20201007092455.1"><vh>fix-network-device-names</vh></v>
</v>
<v t="jonathanhudson.20201006084250.169"><vh>Servers</vh></v>
<v t="jonathanhudson.20201006150145.1"><vh>SQL command history</vh></v>
<v t="jonathanhudson.20201007105118.1"><vh>Streaming</vh></v>
<v t="jonathanhudson.20201006084250.173"><vh>Toolchain build</vh></v>
<v t="jonathanhudson.20201007110100.1"><vh>Upgrading Versions</vh></v>
<v t="jonathanhudson.20201006084250.174"><vh>QA build</vh></v>
<v t="jonathanhudson.20201006084250.175"><vh>QA tests</vh></v>
<v t="jonathanhudson.20201006160826.1"><vh>Virtual tables</vh>
<v t="jonathanhudson.20201007085021.1"><vh>ipe_allcursessions</vh></v>
<v t="jonathanhudson.20201007094348.1"><vh>ipe_alllocks</vh></v>
<v t="jonathanhudson.20201006161615.1"><vh>ipe_allram_images</vh></v>
<v t="jonathanhudson.20201006161722.1"><vh>ipe_disk_access</vh></v>
<v t="jonathanhudson.20201006161753.1"><vh>ipe_disk_slab</vh></v>
<v t="jonathanhudson.20201006161819.1"><vh>ipe_ftable</vh></v>
<v t="jonathanhudson.20201006161849.1"><vh>ipe_query_queues</vh></v>
<v t="jonathanhudson.20201006161919.1"><vh>ipe_query_queue_stats</vh></v>
<v t="jonathanhudson.20201007163326.1"><vh>ipe_mpk_ tables</vh></v>
</v>
</v>
<v t="jonathanhudson.20211001141004.1"><vh>Kubernites</vh>
<v t="jonathanhudson.20211001144458.1"><vh>creating awx-operator pod</vh>
<v t="jonathanhudson.20211001164416.1"><vh>create-postgres-database</vh></v>
<v t="jonathanhudson.20211001162513.1"><vh>awx-postgres-spec.yml</vh></v>
<v t="jonathanhudson.20211001162457.1"><vh>awx-linux.yml</vh></v>
</v>
<v t="jonathanhudson.20211001152121.1"><vh>external postgres service</vh></v>
<v t="jonathanhudson.20211001155941.1"><vh>access external services</vh></v>
</v>
<v t="jonathanhudson.20201006084250.176"><vh>Linux</vh>
<v t="jonathanhudson.20201006084250.177"><vh>admin</vh>
<v t="jonathanhudson.20201006084250.178"><vh>antivirus sophosav</vh></v>
<v t="jonathanhudson.20201006084250.179"><vh>cron every 2 weeks</vh></v>
<v t="jonathanhudson.20201006084250.180"><vh>date time changes testing</vh></v>
<v t="jonathanhudson.20201006084250.181"><vh>desktop entries centos</vh></v>
<v t="jonathanhudson.20201006084250.182"><vh>enabling services</vh></v>
<v t="jonathanhudson.20201006084250.183"><vh>gnome crashes</vh></v>
<v t="jonathanhudson.20201006084250.184"><vh>https basic authentication</vh></v>
<v t="jonathanhudson.20201006084250.185"><vh>log rotate catalina</vh></v>
<v t="jonathanhudson.20201006084250.189"><vh>move files using tar</vh></v>
<v t="jonathanhudson.20201006084250.190"><vh>nfs setup export</vh></v>
<v t="jonathanhudson.20201006084250.191"><vh>process start time</vh></v>
<v t="jonathanhudson.20201006084250.192"><vh>redhat repos bet365</vh></v>
<v t="jonathanhudson.20201006084250.193"><vh>rename multiple folders</vh></v>
<v t="jonathanhudson.20201006084250.194"><vh>run command remotely</vh></v>
<v t="jonathanhudson.20201006084250.195"><vh>selinux</vh></v>
<v t="jonathanhudson.20201006084250.196"><vh>send messages linux</vh></v>
<v t="jonathanhudson.20201006084250.197"><vh>ssh keys</vh></v>
<v t="jonathanhudson.20201006084250.198"><vh>suspend and kill running script</vh></v>
<v t="jonathanhudson.20201006084250.199"><vh>umask</vh></v>
<v t="jonathanhudson.20201006084250.200"><vh>xhost for su user</vh></v>
<v t="jonathanhudson.20201006084250.201"><vh>yum repo version</vh></v>
</v>
<v t="jonathanhudson.20210916082444.1"><vh>awk</vh></v>
<v t="jonathanhudson.20201006084250.202"><vh>certificates</vh>
<v t="jonathanhudson.20201006084250.203"><vh>cert commands</vh></v>
<v t="jonathanhudson.20201006084250.204"><vh>create a public / private key</vh></v>
<v t="jonathanhudson.20201006084250.205"><vh>create an operator certificate</vh></v>
<v t="jonathanhudson.20201006084250.206"><vh>create an operator csr from the certificate</vh></v>
<v t="jonathanhudson.20201006084250.207"><vh>explode p12</vh></v>
<v t="jonathanhudson.20201006084250.208"><vh>generate the root CA key</vh>
<v t="jonathanhudson.20201006084250.209"><vh>Using a password we can</vh></v>
</v>
<v t="jonathanhudson.20201006084250.210"><vh>get server cert with server name</vh></v>
<v t="jonathanhudson.20201006084250.211"><vh>gitbash</vh></v>
<v t="jonathanhudson.20201006084250.212"><vh>import privatekey to keystore</vh></v>
<v t="jonathanhudson.20201006084250.213"><vh>list remote site certificate sans</vh></v>
<v t="jonathanhudson.20201006084250.214"><vh>replace certs</vh></v>
<v t="jonathanhudson.20201006084250.215"><vh>reverse proxy test</vh></v>
<v t="jonathanhudson.20201006084250.216"><vh>self sign the certificate</vh></v>
<v t="jonathanhudson.20201006084250.217"><vh>sign the operator csr with the root CA certificate</vh></v>
<v t="jonathanhudson.20201006084250.218"><vh>test certs</vh></v>
<v t="jonathanhudson.20201006084250.219"><vh>view the operator signed certificate</vh></v>
</v>
<v t="jonathanhudson.20201006084250.220"><vh>cygwin</vh>
<v t="jonathanhudson.20201006084250.221"><vh>link-cygwin-notepad.txt</vh></v>
<v t="jonathanhudson.20201006084250.222"><vh>cygwin-cygcheck.txt</vh></v>
<v t="jonathanhudson.20201006084250.223"><vh>Configure Maven to run correctly from Cygwin</vh>
<v t="jonathanhudson.20201006084250.224"><vh>Modify the .bashrc</vh></v>
<v t="jonathanhudson.20201006084250.225"><vh>Add the file .exrc</vh></v>
<v t="jonathanhudson.20201006084250.226"><vh>Configure Java unrestricted policy files</vh></v>
</v>
</v>
<v t="jonathanhudson.20201006084250.227"><vh>curl</vh>
<v t="jonathanhudson.20201006084250.228"><vh>test active games using curl</vh></v>
<v t="jonathanhudson.20201006084250.229"><vh>test cap.req</vh></v>
<v t="jonathanhudson.20201006084250.230"><vh>test using curl</vh></v>
<v t="jonathanhudson.20201006084250.231"><vh>test using curl fin add</vh></v>
</v>
<v t="jonathanhudson.20201006084250.232"><vh>disks</vh>
<v t="jonathanhudson.20201006084250.233"><vh>disk full find where used</vh></v>
<v t="jonathanhudson.20201006084250.234"><vh>logical volume display</vh></v>
<v t="jonathanhudson.20201006084250.235"><vh>size of files in folder</vh></v>
</v>
<v t="jonathanhudson.20201006084250.236"><vh>find</vh>
<v t="jonathanhudson.20201006084250.237"><vh>count files older than</vh></v>
<v t="jonathanhudson.20201006084250.238"><vh>delete files older than recursively</vh></v>
<v t="jonathanhudson.20201006084250.239"><vh>diff files on server</vh></v>
<v t="jonathanhudson.20201006084250.240"><vh>find anteriores files</vh></v>
<v t="jonathanhudson.20201006084250.241"><vh>find based on path and name</vh></v>
<v t="jonathanhudson.20201006084250.242"><vh>find based on time</vh></v>
<v t="jonathanhudson.20201006084250.243"><vh>find big files</vh></v>
<v t="jonathanhudson.20201006084250.244"><vh>find corrupt jars maven repo</vh></v>
<v t="jonathanhudson.20201006084250.245"><vh>find files generate script with name</vh></v>
<v t="jonathanhudson.20201006084250.246"><vh>find my external ip</vh></v>
<v t="jonathanhudson.20201006084250.247"><vh>find records in es zip files</vh></v>
<v t="jonathanhudson.20201006084250.248"><vh>find records in gi zip files</vh></v>
<v t="jonathanhudson.20201006084250.249"><vh>find sorted rsync</vh></v>
<v t="jonathanhudson.20201006084250.250"><vh>find with delete</vh></v>
<v t="jonathanhudson.20201006084250.251"><vh>find with regex</vh></v>
<v t="jonathanhudson.20201006084250.252"><vh>move files older than</vh></v>
<v t="jonathanhudson.20210315095504.1"><vh>find and replace recursively</vh></v>
<v t="jonathanhudson.20210422134336.1"><vh>find files and truncate</vh></v>
<v t="jonathanhudson.20210712110051.1"><vh>find zip files and then grep the contents</vh></v>
</v>
<v t="jonathanhudson.20210302081726.1"><vh>gnome</vh></v>
<v t="jonathanhudson.20201006084250.253"><vh>grep</vh>
<v t="jonathanhudson.20201006084250.254"><vh>grep all code</vh></v>
<v t="jonathanhudson.20201006084250.255"><vh>grep using regex</vh></v>
<v t="jonathanhudson.20201006084250.256"><vh>search code for text</vh></v>
</v>
<v t="jonathanhudson.20201006084250.257"><vh>iptables</vh>
<v t="jonathanhudson.20201006084250.258"><vh>iptables block postgres test</vh></v>
<v t="jonathanhudson.20201006084250.259"><vh>iptables general</vh></v>
</v>
<v t="jonathanhudson.20210924095213.1"><vh>limits</vh></v>
<v t="jonathanhudson.20210305132703.1"><vh>mkdocs</vh></v>
<v t="jonathanhudson.20210917073524.1"><vh>mounts</vh>
<v t="jonathanhudson.20201006084250.186"><vh>mount junior</vh></v>
<v t="jonathanhudson.20201006084250.187"><vh>mount tech lead</vh></v>
<v t="jonathanhudson.20210917073624.1"><vh>mount safe_data ro</vh></v>
</v>
<v t="jonathanhudson.20201006084250.260"><vh>network</vh>
<v t="jonathanhudson.20201006084250.261"><vh>ping monitor network</vh></v>
<v t="jonathanhudson.20201006084250.262"><vh>tcp dump general</vh></v>
<v t="jonathanhudson.20201006084250.263"><vh>tcp dump nfs</vh></v>
<v t="jonathanhudson.20201006084250.264"><vh>tcp dump trace</vh></v>
</v>
<v t="jonathanhudson.20211001140846.1"><vh>openssl</vh></v>
<v t="jonathanhudson.20201006084250.265"><vh>performance</vh>
<v t="jonathanhudson.20201006084250.266"><vh>check swap memory usage</vh></v>
<v t="jonathanhudson.20201006084250.267"><vh>create big random file</vh></v>
<v t="jonathanhudson.20201006084250.268"><vh>linux commands performance</vh></v>
<v t="jonathanhudson.20201006084250.269"><vh>linux slow down outbound network</vh></v>
<v t="jonathanhudson.20201006084250.270"><vh>slow file copy tests</vh></v>
<v t="jonathanhudson.20201006084250.271"><vh>slow file io</vh></v>
</v>
<v t="jonathanhudson.20201006084250.272"><vh>programming</vh>
<v t="jonathanhudson.20201006084250.273"><vh>get shared object details</vh></v>
</v>
<v t="jonathanhudson.20201006084250.274"><vh>rsync</vh>
<v t="jonathanhudson.20201006084250.275"><vh>nice cpu limit rsync</vh></v>
<v t="jonathanhudson.20201006084250.276"><vh>rsync with  rolling window</vh></v>
<v t="jonathanhudson.20201006084250.277"><vh>rsync scenarios</vh></v>
</v>
<v t="jonathanhudson.20210310150844.1"><vh>rsyslog</vh></v>
<v t="jonathanhudson.20201006084250.278"><vh>scripting</vh>
<v t="jonathanhudson.20201006084250.279"><vh>column text file</vh></v>
<v t="jonathanhudson.20201006084250.280"><vh>create lots files</vh></v>
<v t="jonathanhudson.20201006084250.281"><vh>diff folders linux</vh></v>
<v t="jonathanhudson.20201006084250.282"><vh>diff folders liunx 2</vh></v>
<v t="jonathanhudson.20201006084250.283"><vh>empty log file</vh></v>
<v t="jonathanhudson.20201006084250.284"><vh>loop things</vh></v>
<v t="jonathanhudson.20201006084250.285"><vh>repeat command</vh></v>
<v t="jonathanhudson.20201006084250.286"><vh>vim commands</vh></v>
</v>
<v t="jonathanhudson.20201006084250.287"><vh>scripts</vh>
<v t="jonathanhudson.20201006084250.288"><vh>check-swap.sh</vh></v>
</v>
<v t="jonathanhudson.20201006084250.289"><vh>samba</vh></v>
<v t="jonathanhudson.20201006084250.290"><vh>sed</vh>
<v t="jonathanhudson.20201006084250.291"><vh>fix script cr lf</vh></v>
<v t="jonathanhudson.20201006084250.292"><vh>sed commands</vh></v>
</v>
<v t="jonathanhudson.20201006084250.293"><vh>setup</vh>
<v t="jonathanhudson.20201006084250.294"><vh>leo install</vh></v>
<v t="jonathanhudson.20201006084250.295"><vh>mounts</vh></v>
<v t="jonathanhudson.20210323152114.1"><vh>ntp time sync</vh></v>
</v>
<v t="jonathanhudson.20201006084250.296"><vh>sftp</vh>
<v t="jonathanhudson.20201006084250.297"><vh>add sftp user local or ad</vh></v>
<v t="jonathanhudson.20201006084250.298"><vh>chroot howto spain</vh></v>
<v t="jonathanhudson.20201006084250.299"><vh>configure secure ftp</vh></v>
</v>
<v t="jonathanhudson.20201006084250.300"><vh>ssh</vh></v>
<v t="jonathanhudson.20210812104855.1"><vh>sort</vh></v>
<v t="jonathanhudson.20201006084250.301"><vh>tmux</vh></v>
<v t="jonathanhudson.20201006084250.302"><vh>tree</vh></v>
<v t="jonathanhudson.20210916162721.1"><vh>uniq</vh></v>
<v t="jonathanhudson.20201006084250.303"><vh>users</vh>
<v t="jonathanhudson.20201006084250.304"><vh>add linux user</vh></v>
<v t="jonathanhudson.20201006084250.305"><vh>create user</vh></v>
</v>
<v t="jonathanhudson.20210204082559.1"><vh>Vim</vh></v>
<v t="jonathanhudson.20210823103451.1"><vh>virtual ip</vh></v>
<v t="jonathanhudson.20210916162241.1"><vh>zip</vh></v>
</v>
<v t="jonathanhudson.20201006084250.306"><vh>Maven</vh>
<v t="jonathanhudson.20201006084250.307"><vh>maven clover cmd line</vh></v>
<v t="jonathanhudson.20210916100148.1"><vh>maven deploy file</vh></v>
<v t="jonathanhudson.20201006084250.308"><vh>run maven on slave setup</vh></v>
<v t="jonathanhudson.20201006084250.309"><vh>run safe test framework on slave</vh></v>
</v>
<v t="jonathanhudson.20201006084250.310"><vh>Nexus</vh></v>
<v t="jonathanhudson.20201006084250.311"><vh>Postgres</vh>
<v t="jonathanhudson.20201006084250.312"><vh>backup restore</vh>
<v t="jonathanhudson.20201006084250.313"><vh>backup data production</vh></v>
<v t="jonathanhudson.20201006084250.314"><vh>pg repack in uat</vh></v>
<v t="jonathanhudson.20201006084250.315"><vh>restore deleted tables</vh></v>
<v t="jonathanhudson.20201006084250.316"><vh>testing pg repack delete restore data</vh></v>
<v t="jonathanhudson.20201006084250.317"><vh>testing pg repack uat 21 06 2018</vh></v>
</v>
<v t="jonathanhudson.20201006084250.318"><vh>backup restore sft dev</vh></v>
<v t="jonathanhudson.20201006084250.319"><vh>cleanup wal folder pg xlog</vh></v>
<v t="jonathanhudson.20201006084250.320"><vh>compile pg_repack linux</vh></v>
<v t="jonathanhudson.20201006084250.321"><vh>compile pg_squeeze linux</vh></v>
<v t="jonathanhudson.20201006084250.322"><vh>create postgres safedb windows</vh></v>
<v t="jonathanhudson.20201006084250.323"><vh>disable synchronous commit</vh></v>
<v t="jonathanhudson.20201006084250.324"><vh>enable postgres safe mon logs</vh></v>
<v t="jonathanhudson.20201006084250.325"><vh>failover testing results</vh></v>
<v t="jonathanhudson.20201006084250.326"><vh>faover commands testing</vh></v>
<v t="jonathanhudson.20201006084250.327"><vh>latest maven driver</vh></v>
<v t="jonathanhudson.20201006084250.328"><vh>normal vacuum and analyze</vh></v>
<v t="jonathanhudson.20201006084250.329"><vh>out of disk space</vh></v>
<v t="jonathanhudson.20201006084250.330"><vh>postgres command line</vh></v>
<v t="jonathanhudson.20201006084250.331"><vh>postgres stop start reload</vh></v>
<v t="jonathanhudson.20201006084250.332"><vh>prod dbas installed progress commands</vh></v>
<v t="jonathanhudson.20201006084250.333"><vh>production hba config.ip addresses</vh></v>
<v t="jonathanhudson.20201006084250.334"><vh>purge analysis lcl</vh></v>
<v t="jonathanhudson.20201006084250.335"><vh>purge analysis rtl</vh></v>
<v t="jonathanhudson.20201006084250.336"><vh>sql</vh>
<v t="jonathanhudson.20201006084250.337"><vh>insert gamedata gr function</vh></v>
<v t="jonathanhudson.20201006084250.338"><vh>replication query reload</vh></v>
<v t="jonathanhudson.20201006084250.339"><vh>delete dk tables</vh></v>
<v t="jonathanhudson.20201006084250.340"><vh>size of database tables</vh></v>
<v t="jonathanhudson.20201006084250.341"><vh>size of database</vh></v>
<v t="jonathanhudson.20201006084250.342"><vh>sft user access fileboxes</vh></v>
<v t="jonathanhudson.20201006084250.343"><vh>sft jpa user not in role</vh></v>
</v>
<v t="jonathanhudson.20201006084250.344"><vh>start postgres lcl linux</vh></v>
<v t="jonathanhudson.20201006084250.345"><vh>uat postgres</vh></v>
<v t="jonathanhudson.20201006084250.346"><vh>vacuum manually with stats</vh></v>
<v t="jonathanhudson.20201006084250.347"><vh>vacuum output log</vh></v>
</v>
<v t="jonathanhudson.20201006084250.348"><vh>Python</vh>
<v t="jonathanhudson.20201006084250.349"><vh>Create lots of random files</vh></v>
</v>
<v t="jonathanhudson.20201006084250.350"><vh>Safe</vh>
<v t="jonathanhudson.20201006084250.351"><vh>checker</vh></v>
<v t="jonathanhudson.20201006084250.352"><vh>d3s</vh>
<v t="jonathanhudson.20201006084250.353"><vh>setup</vh>
<v t="jonathanhudson.20210303153926.1"><vh>linux vm</vh></v>
<v t="jonathanhudson.20201006084250.354"><vh>auto startup</vh>
<v t="jonathanhudson.20201006084250.355"><vh>init.d-safe-mon</vh></v>
<v t="jonathanhudson.20201006084250.356"><vh>systemd-ntp</vh></v>
<v t="jonathanhudson.20201006084250.357"><vh>init.d-d3s-dk</vh></v>
</v>
</v>
<v t="jonathanhudson.20210308150442.1"><vh>startup</vh></v>
</v>
<v t="jonathanhudson.20201006084250.358"><vh>dss</vh>
<v t="jonathanhudson.20201006084250.359"><vh>phasing dss</vh></v>
<v t="jonathanhudson.20201006084250.360"><vh>soap signing request</vh></v>
<v t="jonathanhudson.20210621121255.1"><vh>Using dssc</vh></v>
</v>
<v t="jonathanhudson.20201006084250.361"><vh>House Keeping</vh></v>
<v t="jonathanhudson.20201006084250.362"><vh>safe-mon</vh>
<v t="jonathanhudson.20201006084250.363"><vh>serviceInfo.txt</vh></v>
</v>
<v t="jonathanhudson.20201006084250.364"><vh>scheduler</vh>
<v t="jonathanhudson.20210812105341.1"><vh>Tasks</vh>
<v t="jonathanhudson.20210812105356.1"><vh>anteriores</vh>
<v t="jonathanhudson.20210812105405.1"><vh>Deployment script for LCL</vh></v>
</v>
<v t="jonathanhudson.20210812104814.1"><vh>storage log</vh>
<v t="jonathanhudson.20210916082534.1"><vh>storage-submission-logs</vh></v>
<v t="jonathanhudson.20210812105245.1"><vh>Deployment script for LCL</vh></v>
<v t="jonathanhudson.20210930102844.1"><vh>Local task every 5 minutes</vh></v>
</v>
</v>
</v>
</v>
<v t="jonathanhudson.20201006084250.365"><vh>Safe-Mon</vh>
<v t="jonathanhudson.20201006084250.366"><vh>Monitoring urls</vh></v>
</v>
<v t="jonathanhudson.20201006084250.367"><vh>Scripts</vh>
<v t="jonathanhudson.20201006084250.368"><vh>check-nfs.sh</vh></v>
<v t="jonathanhudson.20201006084250.369"><vh>find-stale-mounts.sh</vh></v>
</v>
<v t="jonathanhudson.20210420100717.1"><vh>Software</vh></v>
<v t="jonathanhudson.20201006084250.370"><vh>Splunk</vh>
<v t="jonathanhudson.20201006084250.371"><vh>Tech Notes</vh></v>
</v>
<v t="jonathanhudson.20201006084250.372"><vh>HSM</vh></v>
<v t="jonathanhudson.20201006084250.373"><vh>Talend</vh></v>
<v t="jonathanhudson.20210811090906.1"><vh>VMWare</vh>
<v t="jonathanhudson.20210811090910.1"><vh>Mount win share in centos vm</vh></v>
</v>
<v t="jonathanhudson.20201006084250.374"><vh>Windows</vh>
<v t="jonathanhudson.20201006084250.375"><vh>Idle timeout lock screen</vh></v>
</v>
</v>
</vnodes>
<tnodes>
<t tx="jonathanhudson.20200131123442.1"></t>
<t tx="jonathanhudson.20200131123459.1"></t>
<t tx="jonathanhudson.20200131124925.1">RULE ^(SAF[a-zA-Z]+-[0-9]+)
REPLACE \1
ICON jira.png
WEIGHT Bold
Font Times

RULE ^(RPS[a-zA-Z]+-[0-9]+)
REPLACE \1
ICON jira.png
WEIGHT Bold
Font Times

RULE ^(RPS-[a-zA-Z]+-[0-9]+)
REPLACE \1
ICON jira.png
WEIGHT Bold
Font Times
</t>
<t tx="jonathanhudson.20200131125049.1"># if the node name starts with 'peacock node DEMO', make a mess of it
RULE ^(peacock node DEMO)
REPLACE LOOK: \1
ICON Tango/16x16/emotes/face-grin.png
ICON Tango/16x16/emotes/face-wink.png
FG @solarized-magenta
BG white
FONT Times
PX 40
ITALIC 1
WEIGHT Bold
</t>
<t tx="jonathanhudson.20200131125633.1"></t>
<t tx="jonathanhudson.20200214080742.1">RULE ( \?)$
REPLACE \1
FG Red
WEIGHT Bold
Font Times
</t>
<t tx="jonathanhudson.20201006084250.1">g.es('Hello World!') # g.es prints all its arguments to the log pane.</t>
<t tx="jonathanhudson.20201006084250.10" __bookmarks="7d7100580700000069735f6475706571014930300a732e">ansible-playbook --syntax-check --list-tasks l-bg-remove-users-and-folders.yml</t>
<t tx="jonathanhudson.20201006084250.100" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.101">@language md
## Block access to GlusterFS
```
iptables -L

iptables -A FORWARD -p tcp --dport 24007 -j DROP
iptables -A INPUT -p tcp --dport 24007 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 24007 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 24008 -j DROP
iptables -A INPUT -p tcp --dport 24008 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 24008 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49152 -j DROP
iptables -A INPUT -p tcp --dport 49152 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49152 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49153 -j DROP
iptables -A INPUT -p tcp --dport 49153 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49153 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49154 -j DROP
iptables -A INPUT -p tcp --dport 49154 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49154 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49155 -j DROP
iptables -A INPUT -p tcp --dport 49155 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49155 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49156 -j DROP
iptables -A INPUT -p tcp --dport 49156 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49156 -m state --state NEW,ESTABLISHED -j DROP

&gt; /dev/tcp/mn2regcap0002u1/24007
```
## Perform Testing

### Reset the Firewall
```
iptables -F
```
### Check Connection 
```
&gt; /dev/tcp/localhost/24007
```</t>
<t tx="jonathanhudson.20201006084250.102"></t>
<t tx="jonathanhudson.20201006084250.103">@language shell
#!/bin/bash
FILE="$1"

hidden_file_path=$(getfattr -m gfid -d -e hex "$FILE" | sed -n '2s/.*0x\(..\)\(..\)\(....\)\(....\)\(....\)\(....\)\(............\)/\1\/\2\/\1\2\3-\4-\5-\6-\7/p')

NOOP=$2

if [ "x$NOOP" = "x" ] ; then
  rm -v .glusterfs/$hidden_file_path "$FILE"
else
  echo "Would remove the following:"
  echo .glusterfs/$hidden_file_path
  echo "$FILE"
fi
</t>
<t tx="jonathanhudson.20201006084250.104">@language shell
iptables -A FORWARD -p tcp --dport 24007 -j DROP
iptables -A INPUT -p tcp --dport 24007 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 24007 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 24008 -j DROP
iptables -A INPUT -p tcp --dport 24008 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 24008 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49152 -j DROP
iptables -A INPUT -p tcp --dport 49152 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49152 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49153 -j DROP
iptables -A INPUT -p tcp --dport 49153 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49153 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49154 -j DROP
iptables -A INPUT -p tcp --dport 49154 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49154 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49155 -j DROP
iptables -A INPUT -p tcp --dport 49155 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49155 -m state --state NEW,ESTABLISHED -j DROP

iptables -A FORWARD -p tcp --dport 49156 -j DROP
iptables -A INPUT -p tcp --dport 49156 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 49156 -m state --state NEW,ESTABLISHED -j DROP

</t>
<t tx="jonathanhudson.20201006084250.105">@language shell
iptables -F</t>
<t tx="jonathanhudson.20201006084250.106">@language shell
gluster volume stop dk-gluster-vol1
gluster volume delete dk-gluster-vol1
rm /data/bricks/dk-brick/* -Rf
</t>
<t tx="jonathanhudson.20201006084250.107"></t>
<t tx="jonathanhudson.20201006084250.108"></t>
<t tx="jonathanhudson.20201006084250.109">@language md
### Notes</t>
<t tx="jonathanhudson.20201006084250.11" __bookmarks="7d7100580700000069735f6475706571014930300a732e">
~/${symlink}/bin/${bin_admin} stop



ansible-playbook l-bg-remove-dss-user.yml
ansible-playbook l-bg-remove-d3s-users-and-groups.yml

ansible-playbook l-bg-install-java.yml
ansible-playbook l-bg-install-tomcat.yml



ansible-playbook l-bg-create-dss-user.yml
ansible-playbook l-bg-create-d3s-user-and-groups.yml

ansible-playbook l-bg-install-d3s-server.yml

ansible-playbook l-bg-start-services.yml



ansible-playbook 
ansible-playbook 
ansible-playbook 
ansible-playbook 
</t>
<t tx="jonathanhudson.20201006084250.110">@language md
### Resync

### Spain
From mn2regcap0001u1
```
rsync -avc --delete /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200131/ /mnt/GFS_DATA_ES/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200131/
```

### Denmark
```
rsync -avcn --delete /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-31/ /mnt/GFS_DATA_DK/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-31/
```</t>
<t tx="jonathanhudson.20201006084250.111"></t>
<t tx="jonathanhudson.20201006084250.112" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.113" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e">,
          {
            "name": "RSync XML GFS",
            "type": "RSYNC",
            "location": "/data/home/d3s-dk/scripts/logs/rsync_dk_gfs.log",
            "text": "",
            "threshold": "120",
            "alertAfter": "RSYNC_ERROR_CODE_23+30,RSYNC_ERROR_CODE_24+30",
            "status": "OK",
            "lastUpdated": "",
            "enabled": "true"
          }
          
,
          {
            "name": "RSync Anteriores GFS",
            "type": "RSYNC",
            "location": "/data/home/d3s-es/scripts/logs/rsync_es_gfs.log",
            "checkAfter": "08:00",
            "threshold": "1500",
            "text": "",
            "alertAfter": "RSYNC_ERROR_CODE_23+30,RSYNC_ERROR_CODE_24+30",
            "status": "OK",
            "lastUpdated": "",
            "enabled": "true"
          }          </t>
<t tx="jonathanhudson.20201006084250.114"></t>
<t tx="jonathanhudson.20201006084250.115" annotate="7d71002858080000007072696f7269747971014d0f27580a000000707269736574646174657102580a000000323032302d30312d32387103752e">@language md
### Notes
Branches

tk/SAFDK-151/gfs-copy-xml</t>
<t tx="jonathanhudson.20201006084250.116">@language md
## Rsync Checks
```
watch -n 5 "find /mnt/GFS_DATA_DK/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29 -name \"*.xml\" -type f | wc -l"

watch -n 5 "find /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29 -name \"*.xml\" -type f | wc -l"

watch -n 10 "rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29/ mn2regrep0001d1:/data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29/"
```
### From mn2regrep0001d1
```
rsync -avnc /data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/ mn2regcap0001d1:/mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/
```
### From mn2regcap0001d1
```
rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29/ mn2regrep0001d1:/data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-29/
```

### From mn2regrep0001u1
```
rsync -avnc /data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/ mn2regcap0001u1:/mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/
```
### From mn2regcap0001u1
```
rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/ mn2regrep0001u1:/data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/ | grep '.zip'
```

#### Force update due to d3s running without GFS
From mn2regcap0001d1
```
rsync -avc --delete /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/ /mnt/GFS_DATA_DK/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-30/
```
</t>
<t tx="jonathanhudson.20201006084250.117"></t>
<t tx="jonathanhudson.20201006084250.118">@language shell
#!/bin/bash
# */5 * * * * flock -n /tmp/rsync_dk-gfs.lock /data/home/d3s-dk/scripts/rsync-dk-gfs.sh

XML_OPTS="-av --stats --ignore-existing --delete --include="*.xml" --include="*/"  --exclude="*""

dates=( `date -dyesterday +%Y-%m-%d` `date -dtoday +%Y-%m-%d` )
tenants=( 'b365' 'b365sports' 'Dictao2' )

umask 002

sync_to_gfs()
{
    for i in "${tenants[@]}"
    do
        for n in "${dates[@]}"
        do
            printf "RSync %s on %s\n" "$i" "$n"
    
            SRC="/mnt/SAFE_DATA/d3s-dk/storage/$i/folderstruktur-spilsystem/Zip/$n/"
            DEST="/mnt/GFS_DATA_DK/d3s-dk/storage/$i/folderstruktur-spilsystem/Zip/$n/"
    
            if [ -d $SRC ];
            then
                if [ ! -d $DEST ];
                then
                    mkdir -p $DEST
                fi
    
                echo "Copy to "$DEST
                rsync_suppress_errors ${XML_OPTS} ${SRC} ${DEST} 2&gt;&amp;1 | tee ~/scripts/logs/rsync_dk_gfs.log
            fi
    
        done
    done
}

rsync_suppress_errors()
{
    (rsync "$@" ; if [ $? == 24 -o $? == 23 -o $? == 20 ]; then exit 0; else exit $?; fi) 2&gt;&amp;1 | grep -v "vanished" | grep -v "No such file" | grep -v "Not a directory"
}

sync_to_gfs</t>
<t tx="jonathanhudson.20201006084250.119"></t>
<t tx="jonathanhudson.20201006084250.12" __bookmarks="7d7100580700000069735f6475706571014930300a732e"></t>
<t tx="jonathanhudson.20201006084250.120" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e">@language md

### Summary

Decided that sufficient to rsync every 5 mins but we need to monitor that the rsync is working

```
# /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/Dictao2-459229914/
# /mnt/GFS_DATA_DK/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/Dictao2-155681749/

rsync --info=progress2 -auvz ~/Music/ /data/music/

XML_OPTS="-a --stats --ignore-existing --delete --include="*.xml" --include="*/"  --exclude="*""

SRC1=""
while inotifywait -r -e modify,create,delete,move /source-xml; do
    rsync -avz --delete /source-xml/ /target-xml/
done

inotifywait -m -q -r -e create --format '%w%f' "$watchpath" |
    while read -r path; do
        : # do something with path
    done

inotifywait -m -q -r -e create --format '%w%f' "$watchpath" |
    while read -r path; do
        : # do something with path
    done
```
</t>
<t tx="jonathanhudson.20201006084250.121" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.122" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e">@language md
### Updates
Add multiple file types currently only supporting file type detected in expression

</t>
<t tx="jonathanhudson.20201006084250.123" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e">@language md
### RSync
```
rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/ mn2regrep0001d1:/data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/
```

### RegEx
```
(\w+)-(\w+)-(\d+).(xml|zip)
```


### Files
```
Every 10.0s: rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/ mn2regrep0001d1:/data/bricks/dk-brick/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2020-01-28/       Tue Jan 28 12:54:41 2020

sending incremental file list
./
Dictao2-446041687.zip.tmp
Dictao2-446041687/EndOfDay/2020-01-28/Dictao2-446041687-1206.xml
Dictao2-446041687/EndOfDay/2020-01-28/Dictao2-446041687-1213.xml
Dictao2-446041687/EndOfDay/2020-01-28/Dictao2-446041687-1223.xml
Dictao2-446041687/EndOfDay/2020-01-28/Dictao2-446041687-1226.xml
Dictao2-446041687/EndOfDay/2020-01-28/Dictao2-446041687-1241.xml
```</t>
<t tx="jonathanhudson.20201006084250.124" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e">These were down to the time delay between running the rsync copy job

Every 5 sec the job syncs and in between syncs you will get a discrepency
</t>
<t tx="jonathanhudson.20201006084250.125"></t>
<t tx="jonathanhudson.20201006084250.126">@language md
### Notes

### Branches

tk/SAFES-304/gfs-copy-anteriores

### Check

find /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/ -type f -name '*.zip.*'

find /mnt/GFS_DATA_ES/d3s-es/storage/Dictao2/CNJ/Dictao2/ -type f -name '*.zip.*'

find /mnt/SAFE_DATA/d3s-es/storage -type f -regextype sed -regex ".*/\.zip\.[[:digit:]]\{3\}" 

```
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190117.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190118.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190122.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190123.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190128.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190129.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190130.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190131.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190201.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190204.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190211.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190212.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190213.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190218.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190219.zip.001
/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/Anteriores/Dictao2_Dictao2_JU_DIARIO_20190227.zip.001
```

### Output
</t>
<t tx="jonathanhudson.20201006084250.127">@language md
## Rsync Checks
```
watch -n 5 "find /mnt/GFS_DATA_ES/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ -name \"*.zip\" -type f | wc -l"

watch -n 5 "find /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ -name \"*.zip\" -type f | wc -l"

watch -n 10 "rsync -avnc /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ mn2regrep0001u1:/data/bricks/es-brick/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/"
```
### From mn2regrep0001d1
```
rsync -avnc /data/bricks/es-brick/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ mn2regcap0001d1:/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/
```
### From mn2regcap0001d1
```
rsync -avnc /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ mn2regrep0001d1:/data/bricks/es-brick/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/
```

### From mn2regrep0001u1
```
rsync -avnc /data/bricks/es-brick/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ mn2regcap0001u1:/mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/
```
### From mn2regcap0001u1
```
rsync -avnc /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ mn2regrep0001u1:/data/bricks/es-brick/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/
```

#### Force update due to d3s running without GFS

Sync between SAFE_DATA and GFS_DATA when get out of step because no FileCopy or Recon Test

From mn2regcap0001u1
```
rsync -avcn --delete /mnt/SAFE_DATA/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ /mnt/GFS_DATA_ES/d3s-es/storage/Dictao2/CNJ/Dictao2/JU/20200130/ 
```
</t>
<t tx="jonathanhudson.20201006084250.128"></t>
<t tx="jonathanhudson.20201006084250.129">@language shell
#!/bin/bash
# 30 5 * * * flock -n /tmp/rsync_es-gfs.lock /data/home/d3s-es/scripts/rsync-es-gfs.sh
SRC="/mnt/SAFE_DATA/d3s-es/storage/"
DEST="/mnt/GFS_DATA_ES/d3s-es/storage"

declare -A tenants
tenants[Dictao1]="Dictao"
tenants[Dictao2]="Dictao2"
tenants[b365]="1013"
tenants[b365sports]="1007"

sync_to_gfs()
{
    RUN_SUFFIX=`date +%s`
    FILE_LIST="/tmp/d3s-es-rsync-gfs.${RUN_SUFFIX}.txt"
    cat /dev/null &gt; ${FILE_LIST}
    
    ZIP_OPTS="-av --stats --include-from=${FILE_LIST} --exclude=/*"
    
    echo -n &gt; ~/scripts/logs/rsync_es_gfs.log
    
    previous_month=`date --date="1 month ago" +%Y%m`
    current_month=`date +%Y%m`
    months="\(${previous_month}\|${current_month}\)"
    pushd ${SRC} &gt; /dev/null
    
    for t in "${!tenants[@]}"
    do
        printf "RSync GFS Anteriores : %s\n" "$t"
        echo "Months : ${previous_month} | ${current_month}"
        find "${t}/CNJ/${tenants[$t]}" -type f  -regextype sed -regex ".*${months}[0-9]\{2\}\.zip\.[[:digit:]]\{3\}" &gt;&gt; "${FILE_LIST}"
    done
    
    popd &gt; /dev/null

    rsync_suppress_errors ${ZIP_OPTS} ${SRC} ${DEST} 2&gt;&amp;1 | tee -a ~/scripts/logs/rsync_es_gfs.log 2&gt;&amp;1

    cat ${FILE_LIST}

    /bin/rm ${FILE_LIST}
}

rsync_suppress_errors()
{
    (rsync "$@" ; if [ $? == 24 -o $? == 23 -o $? == 20 ]; then exit 0; else exit $?; fi) 2&gt;&amp;1 | grep -v "vanished" | grep -v "No such file" | grep -v "Not a directory"
}

sync_to_gfs
</t>
<t tx="jonathanhudson.20201006084250.13" __bookmarks="7d7100580700000069735f6475706571014930300a732e">jonathanhudson ALL=(ALL) ALL

dzdo -u safe-mon-test /bin/sh




Think of it like this:
•ansible_ssh_user is the user to ssh to the host as
•ansible_sudo_user is the user to sudo on the host

In other words, using your users and commands as the example, the equivalent commands that ansible will run are:

ssh foo@server1 sudo -u bar "echo test &gt; testfile"

Therefore the foo user's password needs to be provided, not the bar user. The foo user will need privileges to sudo as bar. Something like this in sudoers:
foo    ALL=(bar) NOPASSWD: ALL


#### sudoers

# /etc/sudoers
#
# This file MUST be edited with the 'visudo' command as root.
#
# See the man page for details on how to write a sudoers file.
#

Defaults        env_reset

# Host alias specification

# User alias specification

# Cmnd alias specification

# User privilege specification
root    ALL=(ALL) ALL







Update centrify

adflush
adreload</t>
<t tx="jonathanhudson.20201006084250.130"></t>
<t tx="jonathanhudson.20201006084250.131" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.132" annotate="7d71002858080000007072696f7269747971014b64580a000000707269736574646174657102580a000000323032302d30312d33307103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358310000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626c6b2e706e677104580700000072656c506174687105580f000000636c656f2f63686b626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.133"></t>
<t tx="jonathanhudson.20201006084250.134">@language md

#### Imports
```
import org.apache.commons.collections4.CollectionUtils;
import org.apache.commons.collections4.Equator;
import java.util.stream.Collectors;
```

#### Using Equator
```
    Equator&lt;Group&gt; groupEquator = new GroupDaoTest.GroupEquator();

    boolean isEqual = CollectionUtils.isEqualCollection( result1, result4, groupEquator );

    Assertions.assertThat( isEqual ).isTrue();
```

#### Equator
```
    private class GroupEquator implements Equator&lt;Group&gt;
    {
        public boolean equate(Group c1, Group c2)
        {
            boolean isEqual = true;

            isEqual = c1.getGroupName().equals( c2.getGroupName() );

            if ( isEqual )
                isEqual = c1.getDescription().equals( c2.getDescription() );

            if ( isEqual )
                isEqual = c1.getCreatedOn().equals( c2.getCreatedOn() );

            if ( isEqual )
                isEqual = c1.getUpdatedOn().equals( c2.getUpdatedOn() );

            if ( isEqual )
                isEqual = c1.getHasXPermission().equals( c2.getHasXPermission() );

            return isEqual;
        }

        @Override
        public int hash( Group group )
        {
            return group.hashCode();
        }
    }
```</t>
<t tx="jonathanhudson.20201006084250.135">@language java
private static void copyFileUsingStream(File source, File dest) throws IOException 
{
    InputStream is = null;
    OutputStream os = null;
    try {
        is = new FileInputStream(source);
        os = new FileOutputStream(dest);
        byte[] buffer = new byte[1024];
        int length;
        while ((length = is.read(buffer)) &gt; 0) {
            os.write(buffer, 0, length);
        }
    } finally {
        is.close();
        os.close();
    }
}

private static void copyFileUsingChannel(File source, File dest) throws IOException 
{
    FileChannel sourceChannel = null;
    FileChannel destChannel = null;
    try {
        sourceChannel = new FileInputStream(source).getChannel();
        destChannel = new FileOutputStream(dest).getChannel();
        destChannel.transferFrom(sourceChannel, 0, sourceChannel.size());
       }finally{
           sourceChannel.close();
           destChannel.close();
   }
}

private static void copyFileUsingApacheCommonsIO(File source, File dest) throws IOException 
{
    FileUtils.copyFile(source, dest);
}



private static void copyFileUsingJava7Files(File source, File dest) throws IOException {
    Files.copy(source.toPath(), dest.toPath());
}





package com.journaldev.files;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.nio.channels.FileChannel;
import java.nio.file.Files;

import org.apache.commons.io.FileUtils;

public class JavaCopyFile {

    public static void main(String[] args) throws InterruptedException, IOException {
        File source = new File("/Users/pankaj/tmp/source.avi");
        File dest = new File("/Users/pankaj/tmp/dest.avi");

        //copy file conventional way using Stream
        long start = System.nanoTime();
        copyFileUsingStream(source, dest);
        System.out.println("Time taken by Stream Copy = "+(System.nanoTime()-start));
        
        //copy files using java.nio FileChannel
        source = new File("/Users/pankaj/tmp/sourceChannel.avi");
        dest = new File("/Users/pankaj/tmp/destChannel.avi");
        start = System.nanoTime();
        copyFileUsingChannel(source, dest);
        System.out.println("Time taken by Channel Copy = "+(System.nanoTime()-start));
        
        //copy files using apache commons io
        source = new File("/Users/pankaj/tmp/sourceApache.avi");
        dest = new File("/Users/pankaj/tmp/destApache.avi");
        start = System.nanoTime();
        copyFileUsingApacheCommonsIO(source, dest);
        System.out.println("Time taken by Apache Commons IO Copy = "+(System.nanoTime()-start));
        
        //using Java 7 Files class
        source = new File("/Users/pankaj/tmp/sourceJava7.avi");
        dest = new File("/Users/pankaj/tmp/destJava7.avi");
        start = System.nanoTime();
        copyFileUsingJava7Files(source, dest);
        System.out.println("Time taken by Java7 Files Copy = "+(System.nanoTime()-start));        
    }
}



Time taken by Stream Copy = 44582575000
Time taken by Channel Copy = 104138195000
Time taken by Apache Commons IO Copy = 108396714000
Time taken by Java7 Files Copy = 89061578000
</t>
<t tx="jonathanhudson.20201006084250.136">@language java
   public Map&lt;String, String&gt; addAllToCache( String batchId, BatchEs batch )
    {
        if (recIdempCache == null)
        {
            return null;
        }

        Map&lt;String, String&gt; remainingDigests = new HashMap&lt;&gt;();
        Map&lt;String, String&gt; digestToRefs     = new HashMap&lt;&gt;();

        boolean         allAdded = false;
        SHA3.DigestSHA3 md       = new SHA3.DigestSHA3( 512 );
        int             index    = 0;

        BatchInfoEs batchInfoEs = batch.getBatchInfo();
        byte[]      batchData   = batch.getData();
        String      xmlData     = new String( batchData, StandardCharsets.UTF_8 );

        try
        {
            LOG.debug( "Reading the record id from batch" );

            InputStream in = new ByteArrayInputStream( batchData );

            xmlData = xmlData.replaceAll( "\n", "" );

            DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();

            dbf.setNamespaceAware( true );
            dbf.setIgnoringElementContentWhitespace( true );
            dbf.setIgnoringComments( true );

            Document doc = dbf.newDocumentBuilder().parse( new InputSource( new StringReader( xmlData ) ) );

            Element docElemet = doc.getDocumentElement();

            String tagName = docElemet.getTagName();

            LOG.debug( "TagName : " + tagName );

            NodeList records = docElemet.getElementsByTagName( "Registro" );

            for ( int i=0; i&lt; records.getLength(); i++ )
            {
                Node     registro     = records.item( i );
                Node     cabecera     = registro.getChildNodes().item( 0 );
                NodeList cabeceraList = cabecera.getChildNodes();

                Node operadorId       = cabeceraList.item( 0 );
                Node almacenId        = cabeceraList.item( 1 );
                Node registroId       = cabeceraList.item( 2 );
                Node subregistroId    = cabeceraList.item( 3 );
                Node SubregistroTotal = cabeceraList.item( 4 );
                Node fecha            = cabeceraList.item( 5 );

                String RegistroIdText       = registroId.getTextContent();
                String subregistroIdText    = subregistroId.getTextContent();
                String SubregistroTotalText = SubregistroTotal.getTextContent();

                int subregistroIdTextInt    = Integer.valueOf( subregistroIdText );
                int SubregistroTotalTextInt = Integer.valueOf( SubregistroTotalText );

                String recordData = nodeToString( registro );

                md.reset();

                String digestText = Base64.encodeBase64StringUnChunked( md.digest( recordData.getBytes( StandardCharsets.UTF_8 ) ) );

                LOG.debug( "Record Md : " + digestText );

                String localRef  = getEntryLocalRef( batchId, index++ );
                //String recordRef = String.format( Locale.ROOT, "%s[%d/%d] %s", callId, subregistroIdTextInt + 1, SubregistroTotalTextInt, RegistroIdText );
                String recordRef = String.format( Locale.ROOT, "[%d/%d] %s", subregistroIdTextInt + 1, SubregistroTotalTextInt, RegistroIdText );

                remainingDigests.put( recordRef, digestText );
                digestToRefs.put( digestText, localRef );
            }

            /*
            for ( RegistroBase entry : entries )
            {
                md.reset();

                String digestText = Base64.encodeBase64StringUnChunked(md.digest(entry.getData()));

                String localRef = getEntryLocalRef(batchId, index++);

                remainingDigests.put(entry.getRecordInfo().getRecordFullRef(), digestText);

                digestToRefs.put(digestText, localRef);
            }
            */

            // Check that there was no dup before checking the db
            if ( digestToRefs.size() == remainingDigests.size() )
            {
                allAdded = recIdempCache.addAllReferences( digestToRefs );
            }
        }
        catch( Exception e )
        {
            LOG.error( "Error %s Unable to deserialise record %s for %s", e.getCause().getMessage(), tenantId, batch.getBatchInfo().getBatchFileNameWithoutExtension() );
        }

        return allAdded ? null : remainingDigests;
    }

    private String getEntryLocalRef(String batchId, int i) {
        return batchId + " " + i;
    }

    private String nodeToString( Node node )
        throws TransformerException
    {
        StringWriter buf   = new StringWriter();
        Transformer  xform = TransformerFactory.newInstance().newTransformer();

        xform.setOutputProperty( OutputKeys.OMIT_XML_DECLARATION, "yes" );

        xform.transform( new DOMSource( node ), new StreamResult( buf ) );

        return (buf.toString());
    }
</t>
<t tx="jonathanhudson.20201006084250.137">@nocolor

The -XX:HeapDumpOnOutOfMemoryError Option

This option tells the Java HotSpot VM to generate a heap dump when an allocation from the Java heap or the permanent generation cannot be satisfied. There is no overhead in running with this option, so it can be useful for production systems where the OutOfMemoryError exception takes a long time to surface.

You can also specify this option at runtime with the MBeans tab in the JConsole utility.

The heap dump is in HPROF binary format, and so it can be analyzed using any tools that can import this format. For example, the jhat tool can be used to do rudimentary analysis of the dump. For more information on the jhat tool, see The jhat Utility.

Example D-1 shows the result of running out of memory with this flag set.

Example D-1 Sample Code for Running Out of Memory

$ java -XX:+HeapDumpOnOutOfMemoryError -mn256m -mx512m ConsumeHeap
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid2262.hprof ...
Heap dump file created [531535128 bytes in 14.691 secs]
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
        at ConsumeHeap$BigObject.(ConsumeHeap.java:22)
        at ConsumeHeap.main(ConsumeHeap.java:32)

The ConsumeHeap fills up the Java heap and runs out of memory. When the java.lang.OutOfMemoryError exception is thrown, a heap dump file is created. In this case the file is 507 MB and is created with the name java_pid2262.hprof in the current directory.

By default the heap dump is created in a file called java_pidpid.hprof in the working directory of the VM, as in the example above. You can specify an alternative file name or directory with the -XX:HeapDumpPath= option. For example -XX:HeapDumpPath=/disk2/dumps will cause the heap dump to be generated in the /disk2/dumps directory.
</t>
<t tx="jonathanhudson.20201006084250.138">@nocolor
Simply stream the zip file to the response's output stream. Start like this: 
?
1
2
OutputStream responseStream = response.getOutputStream();
ZipOutputStream zos = new ZipOutputStream(responseStream);


@RequestMapping(value="/zip", produces="application/zip")
public void zipFiles(HttpServletResponse response) throws IOException {

    //setting headers  
    response.setStatus(HttpServletResponse.SC_OK);
    response.addHeader("Content-Disposition", "attachment; filename=\"test.zip\"");

    ZipOutputStream zipOutputStream = new ZipOutputStream(response.getOutputStream());

    // create a list to add files to be zipped
    ArrayList&lt;File&gt; files = new ArrayList&lt;&gt;(2);
    files.add(new File("README.md"));

    // package files
    for (File file : files) {
        //new zip entry and copying inputstream with file to zipOutputStream, after all closing streams
        zipOutputStream.putNextEntry(new ZipEntry(file.getName()));
        FileInputStream fileInputStream = new FileInputStream(file);

        IOUtils.copy(fileInputStream, zipOutputStream);

        fileInputStream.close();
        zipOutputStream.closeEntry();
    }    

    zipOutputStream.close();
}
</t>
<t tx="jonathanhudson.20201006084250.139">@nocolor
[root@mn2regrep0003d0 ~]# export PATH=/usr/java/jdk1.8.0_111/bin:$PATH
[root@mn2regrep0003d0 ~]# java -version
java version "1.8.0_111"
Java(TM) SE Runtime Environment (build 1.8.0_111-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode)
[root@mn2regrep0003d0 ~]#
[root@mn2regrep0003d0 ~]#
[root@mn2regrep0003d0 ~]# jstat -gccapacity 24466
 NGCMN    NGCMX     NGC     S0C   S1C       EC      OGCMN      OGCMX       OGC         OC       MCMN     MCMX      MC     CCSMN    CCSMX     CCSC    YGC    FGC
 41984.0 672256.0 356864.0 1536.0 1536.0  49664.0    84992.0  1345024.0   157184.0   157184.0      0.0 1118208.0  78464.0      0.0 1048576.0   9088.0    123    11
[root@mn2regrep0003d0 ~]# jps -l
24466 org.codehaus.plexus.classworlds.launcher.Launcher
12100 slave.jar
26309 sun.tools.jps.Jps
24742 /data/code/safe-test-framework/safe-test-es/target/surefire/surefirebooter5306963849607506070.jar
24571 org.apache.catalina.startup.Bootstrap
24686 org.apache.catalina.startup.Bootstrap
[root@mn2regrep0003d0 ~]# jmap -histo:live 24466 | head

 num     #instances         #bytes  class name
----------------------------------------------
   1:        203884       14689480  [C
   2:        203363        4880712  java.lang.String
   3:          2574        3471704  [B
   4:         57910        2779680  com.sun.tools.javac.file.ZipFileIndex$Entry
   5:         61529        1968928  java.util.HashMap$Node
   6:         31532        1585648  [Ljava.lang.Object;
   7:         30246        1451808  java.util.HashMap

 jmap -heap 24466

 Attaching to process ID 24466, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 25.111-b14

using thread-local object allocation.
Parallel GC with 4 thread(s)

Heap Configuration:
   MinHeapFreeRatio         = 0
   MaxHeapFreeRatio         = 100
   MaxHeapSize              = 2065694720 (1970.0MB)
   NewSize                  = 42991616 (41.0MB)
   MaxNewSize               = 688390144 (656.5MB)
   OldSize                  = 87031808 (83.0MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 0 (0.0MB)
</t>
<t tx="jonathanhudson.20201006084250.14" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># Userful Commands

ansible-playbook hello-world.yml

ansible-playbook -i "localhost," -c local hello-world.yml

#ansible all -m shell -a 'echo hello world'

ansible -m ping test -k
</t>
<t tx="jonathanhudson.20201006084250.140">@language shell
export JAVA_HOME=/usr/java/jdk1.8.0_111/
export PATH="$JAVA_HOME/bin:$PATH"

jcmd -l

jstack -F -l 12417 &gt; jstack.out

kill -3 12418
</t>
<t tx="jonathanhudson.20201006084250.141">@nocolor
        /*
        logger = ( Logger ) LogManager.getLogger( ScanDispatcher.class );
        logger.addAppender( mockAppender );
        logger.setLevel( Level.DEBUG );
        */

        /*
        LoggerContext                                      ctx          = ( LoggerContext ) LogManager.getContext( true );
        org.apache.logging.log4j.core.config.Configuration config       = ctx.getConfiguration();
        LoggerConfig                                       loggerConfig = config.getLoggerConfig( LogManager.ROOT_LOGGER_NAME );
        loggerConfig.setLevel( Level.DEBUG );
        ctx.updateLoggers();

        Configurator.setLevel("com.bet365", Level.DEBUG );
        Configurator.setRootLevel( Level.DEBUG );
        Configurator.setAllLevels("com.bet365", Level.DEBUG);
        */

        //updateAppender( "com.bet365" );
        
	private void updateAppender( String loggerName )
    {
        LoggerContext ctx = (LoggerContext) LogManager.getContext(false);

        org.apache.logging.log4j.core.config.Configuration config = ctx.getConfiguration();

        LoggerConfig loggerToUpdate = config.getLoggers().get( loggerName );

        loggerToUpdate.removeAppender("STANDARD" );
        loggerToUpdate.addAppender( config.getAppender("CONSOLE"), Level.DEBUG, null );

        ctx.updateLoggers();

        LOGGER.info( "Updated logging appender!" );
    }        
</t>
<t tx="jonathanhudson.20201006084250.142">@language md

Maven Linux SFT

```
mvn clean install -P sftpostgres,tomee-embedded
```
Single Test
```
mvn clean install -P sftpostgres,tomee-embedded -Dtest=ConcurrentActivateFileTest#testConcurrentActivations
```
</t>
<t tx="jonathanhudson.20201006084250.143">@nocolor
cd /data/code/safe-spain

mvn clean clover:setup verify clover:log clover:aggregate clover:clover -Dmaven.clover.includeFailedTestCoverage=true -Dclover.logging.level=debug -Dclover.logging.adapter=log4j -f pom.xml -P sftpostgres,tomee-embedded
</t>
<t tx="jonathanhudson.20201006084250.144">@nocolor


mvn clean clover:setup verify clover:log clover:aggregate clover:clover -Dmaven.clover.includeFailedTestCoverage=true -Dclover.logging.level=debug -Dclover.logging.adapter=log4j -f pom.xml -Pnexus,sftpostgres,tomee-embedded

mvn clean clover:setup install clover:log clover:aggregate clover:clover tomee:stop -DskipITs=true -Dmaven.clover.includeFailedTestCoverage=true -Dclover.logging.level=debug -Dclover.logging.adapter=log4j -f pom.xml  -Pnexus,sftpostgres,tomee-embedded
</t>
<t tx="jonathanhudson.20201006084250.145">@nocolor
----------------------

•	Accounts with access to sensitive information must have passwords at least 9 characters in length
•	Passwords must contain at least one lowercase letter
•	Passwords must contain at least one uppercase letter
•	Passwords must contain at least one non-alphanumeric character (e.g. ~!@#%^&amp;*_-+=|\(){}[]:;'&lt;&gt;,.?/)
•	Passwords must contain at least one number

----------------------

Java

http://www.passay.org/

or 

String regex = "[ab]{4,6}c";
Xeger generator = new Xeger(regex);
String result = generator.generate();
assert result.matches(regex);

NOTE** Automation Jar

----------------------

(/^(?=.*\d)(?=.*[a-z])(?=.*[A-Z])[0-9a-zA-Z]{8,}$/)

-----------------------


openssl rand -base64 37 | sed -e ‘s/^\(.\{37\}\).*/\1/g’


openssl rand -base64 37 | sed -e ‘s/^\(.\{9\}\).*/\1/g’


openssl rand -base64 37 | awk 'BEGIN{FS=""} { for ( i=1; i&lt;=37; i++ ) printf( "%s2", $i ); } { printf "\n" }'


</t>
<t tx="jonathanhudson.20201006084250.146">@nocolor
service:jmx:rmi:///jndi/rmi://localhost:1099/jmxrmi

service:jmx:rmi:///jndi/rmi://localhost:8004/jmxrmi
service:jmx:rmi:///jndi/rmi://localhost:8184/jmxrmi
service:jmx:rmi:///jndi/rmi://localhost:8174/jmxrmi
service:jmx:rmi:///jndi/rmi://localhost:8274/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2rstats0001u0:25004/jmxrmi

lsof -i

service:jmx:rmi:///jndi/rmi://ob1-001122:29004/jmxrmi
service:jmx:rmi:///jndi/rmi://ob1-001122:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://ob1-001122:23004/jmxrmi
service:jmx:rmi:///jndi/rmi://ob1-001122:36004/jmxrmi
service:jmx:rmi:///jndi/rmi://ob1-001122:37004/jmxrmi


service:jmx:rmi:///jndi/rmi://mn2regcap0001u1:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001u2:25004/jmxrmi

service:jmx:rmi:///jndi/rmi://ir1regcap0002r0:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://ir1regcap0003r0:23004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0001d1:23004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001d2:23004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0001d1:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001d2:25004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0001d1:29004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001d2:29004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0001u1:29004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001u2:29004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0001u1:23004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001u2:23004/jmxrmi


service:jmx:rmi:///jndi/rmi://mn2regcap0001u1:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0001u2:25004/jmxrmi

service:jmx:rmi:///jndi/rmi://IR1REGCAP0002P1:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://IR1REGCAP0002P2:25004/jmxrmi

service:jmx:rmi:///jndi/rmi://IR1REGCAP0002P1:8004/jmxrmi
service:jmx:rmi:///jndi/rmi://IR1REGCAP0002P2:8004/jmxrmi

service:jmx:rmi:///jndi/rmi://IR1REGCAP0001P1:29004/jmxrmi
service:jmx:rmi:///jndi/rmi://IR1REGCAP0001P2:29004/jmxrmi

service:jmx:rmi:///jndi/rmi://IR1REGCAP0005P1:37004/jmxrmi
service:jmx:rmi:///jndi/rmi://IR1REGCAP0005P2:37004/jmxrmi
service:jmx:rmi:///jndi/rmi://IR1REGCAP0005P3:37004/jmxrmi


service:jmx:rmi:///jndi/rmi://BG2RRPUSH0001P1:30004/jmxrmi
service:jmx:rmi:///jndi/rmi://BG2RRPUSH0001P1:31004/jmxrmi

service:jmx:rmi:///jndi/rmi://mn2regcap0002u1:25004/jmxrmi
service:jmx:rmi:///jndi/rmi://mn2regcap0002u2:25004/jmxrmi


service:jmx:rmi:///jndi/rmi://mn2sftapp0001d1:60004/jmxrmi
</t>
<t tx="jonathanhudson.20201006084250.147">F@language md

Basic Example

```
# Setup
ScheduledThreadPoolExecutor stpe = new ScheduledThreadPoolExecutor( 1, new NamedThreadFactory( "**Thread Name**" ) );

stpe.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);
stpe.setRemoveOnCancelPolicy(true);
	
# Create	
ScheduledFuture currentRefreshtask = stpe.schedule(new BatchHandlerBase.CloseBatchTask( this, this.currentBatchWriter), this.batchMaxDuration, this.batchMaxDurationUnit);
ScheduledFuture currentRefreshtask = stpe.scheduleWithFixedDelay(new BatchHandlerBase.CloseBatchTask( this, this.currentBatchWriter), this.batchMaxDuration, this.batchMaxDuration, this.batchMaxDurationUnit);

# To Cancel
currentRefreshtask.cancel(false);

# Runnable or Callable
protected static class CloseBatchTask&lt;B extends Batch&gt; implements Runnable
{
	private final BatchHandlerBase&lt;B&gt; bh;
	private final BatchWriter&lt;B&gt;      bw;

	public CloseBatchTask( BatchHandlerBase&lt;B&gt; bh, BatchWriter&lt;B&gt; bw )
	{
		this.bw = bw;
		this.bh = bh;
	}

	@Override
	public void run()
	{
	}
}
```</t>
<t tx="jonathanhudson.20201006084250.148">@nocolor
NEW
A thread that has not yet started is in this state.

RUNNABLE
A thread executing in the Java virtual machine is in this state.

BLOCKED
A thread that is blocked waiting for a monitor lock is in this state.

WAITING
A thread that is waiting indefinitely for another thread to perform a particular action is in this state.

TIMED_WAITING
A thread that is waiting for another thread to perform an action for up to a specified waiting time is in this state.

TERMINATED
A thread that has exited is in this state.
</t>
<t tx="jonathanhudson.20201006084250.149">@nocolor
ThreadLocal&lt;UserContext&gt; threadLocal = new ThreadLocal&lt;&gt;();

userContext setup ....

threadLocalValue.set( userContext );
UserContext userContext = threadLocalValue.get();

ThreadLocal&lt;UserContext&gt; threadLocal = ThreadLocal.withInitial( () -&gt; userContext );

threadLocal.remove();
</t>
<t tx="jonathanhudson.20201006084250.15" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: dev_d3s
  vars:
    env: 'dev'
    sourceDirectory: '/data/code'
    configFolder: '{{sourceDrive}}{{sourceDirectory}}/safe-configs/Denmark/DEV'
    commonTaskLocation: '{{sourceDirectory}}/safe-deployments/ansible/common/tasks'   
    executionDate: "{{ lookup('pipe', 'date +%Y%m%d-%H%M') }}"
    tamperTokenBackup: '~/backup/backup-tamper-token-safdk-232-{{ansible_date_time.date}}.zip'
    serviceState: []
    users:
#      - { name: 'd3s-dk', oldTamperTokenFolder: '/mnt/emergency/d3s-dk/tampertokens/', newTamperTokenFolder: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/', component: 'd3s', port: '25080', context: 'dk', target: 'mn2regcap0001d1' }
      - { name: 'd3s-dk', oldTamperTokenFolder: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/', newTamperTokenFolder: '/mnt/emergency/d3s-dk/tampertokens/', component: 'd3s', port: '25080', context: 'dk', target: 'mn2regcap0001d1' }
    configs:
      - { name: 'configure.properties', user: 'd3s-dk', type: '', prefix: 'dk-d3s-', suffix: '-1', dstLocation: '~/d3s', srcLocation: '{{configFolder}}', target: 'mn2regcap0001d1' }      
  tasks:      

    # Backup tamper token files
  - name: create tamper token backup folder
    file: path="~/backup" state=directory mode=0770
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: backup tamper token source files
    archive: path={{item.oldTamperTokenFolder}} dest={{tamperTokenBackup}} remove=false format=zip
    register: zipTamperTokenFiles
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: check backup ok
    debug: msg="Backup tamper tokens successfull"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and zipTamperTokenFiles.results[0] is defined and zipTamperTokenFiles.results[0].state == 'file'

    #Move tamper tokens to new location using rsync
  - name: move tamper token source files to new location
    shell: "/usr/bin/rsync -a --remove-source-files /{{item.0.oldTamperTokenFolder}}/ /{{item.0.newTamperTokenFolder}}/"
    no_log: True
    register: moveTamperTokenFiles
    become: yes
    become_user: '{{ item.0.name }}'
    become_method: dzdo
    with_together:
      - "{{users}}"
      - "{{zipTamperTokenFiles.results }}"
    when: item.0.target in ansible_hostname and (item.1.state is defined and item.1.state == 'file')
    delegate_to: "{{item.0.target}}"

  - name: check move ok
    debug: msg="Move tamper tokens successfull"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and moveTamperTokenFiles.results[0] is defined and moveTamperTokenFiles.results[0].rc == 0
    
    #Reconcile and move old tamper tokens
  - name: count tampertoken files in old location
    find: paths={{item.oldTamperTokenFolder}} recurse=yes file_type=file
    register: sourceTamperTokenCount
    no_log: True
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: count tampertoken files in new location
    find: paths={{item.newTamperTokenFolder}} recurse=yes file_type=file
    register: targetTamperTokenCount
    no_log: True
    with_items: '{{users}}'
    when: item.target in ansible_hostname

#  - name: count tampertoken files in new location async action
#    find: paths={{item.newTamperTokenFolder}} recurse=yes file_type=file
#    register: targetTamperTokenCount
#    no_log: True
#    async: 1000
#    poll: 0
#    with_items: '{{users}}'
#    when: item.target in ansible_hostname

#  - name: debug var
#    debug: var=targetTamperTokenCount
#    with_items: '{{users}}'
#    when: item.target in ansible_hostname

#  - name: count tampertoken files in new location async wait
#    async_status: jid={{ targetTamperTokenCount.results[0].ansible_job_id }}
#    register: job_result
#    until: job_result.finished
#    retries: 100
#    delay: 10

  - name: set move ok
    set_fact: copiedOk=True
    with_items: '{{users}}'
    when: item.target in ansible_hostname and sourceTamperTokenCount.results[0] is defined and sourceTamperTokenCount.results[0].matched == targetTamperTokenCount.results[0].matched

  - name: reconcile source files against target files success
    debug: msg="All tamper token files copied from old to new location - old '{{sourceTamperTokenCount.results[0].matched}}' - new - '{{targetTamperTokenCount.results[0].matched}}'"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and copiedOk is defined and copiedOk == True

  - name: reconcile source files against target files failed
    debug: msg="Missing files copied from old to new location - old '{{sourceTamperTokenCount.results[0].matched}}' - new - '{{targetTamperTokenCount.results[0].matched}}'"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and copiedOk is not defined

#  - name: mv old tamper token folder to backup location
#    debug: msg="Would Move"
#    become: yes
#    become_user: "{{ item.name }}"
#    become_method: dzdo
#    with_items: "{{users}}"
#    when: item.target in ansible_hostname and copiedOk is defined and copiedOk == True

  - name: remove empty tamper token folders
    shell: "find /{{item.0.oldTamperTokenFolder}}/ -type d -empty -delete"
    no_log: True
    become: yes
    become_user: '{{ item.0.name }}'
    become_method: dzdo
    with_together:
      - "{{users}}"
      - "{{moveTamperTokenFiles.results}}"
    when: item.0.target in ansible_hostname and copiedOk is defined and copiedOk == True
    delegate_to: "{{item.0.target}}"

</t>
<t tx="jonathanhudson.20201006084250.150">@language md
# Deploy JAX-WS web services on Tomcat
Here’s a guide to show you how to deploy JAX-WS web services on Tomcat servlet container. 
See following summary steps of a web service deployment.

1. Create a web service (of course).
2. Create a ```sun-jaxws.xml```, defines web service implementation class.
3. Create a standard ```web.xml```, defines WSServletContextListener, WSServlet and structure of a web project.
4. Build tool to generate WAR file.
5. Copy JAX-WS dependencies to “```${Tomcat}/lib```” folder.
6. Copy WAR to “```${Tomcat}/webapp```” folder.
7. Start It.

Directory structure of this example, so that you know where to put your files.

![Image](tomcat-web-services/jaxws-deploy-tomcat-folder.png?raw=true)

## 1. WebServices
A simple JAX-WS hello world example.

File : ```HelloWorld.java```
```java
package com.mkyong.ws;

import javax.jws.WebMethod;
import javax.jws.WebService;
import javax.jws.soap.SOAPBinding;
import javax.jws.soap.SOAPBinding.Style;

//Service Endpoint Interface
@WebService
@SOAPBinding(style = Style.RPC)
public interface HelloWorld{

	@WebMethod String getHelloWorldAsString();

}
```
File : ```HelloWorldImpl.java```
```java
package com.mkyong.ws;

import javax.jws.WebService;

//Service Implementation Bean

@WebService(endpointInterface = "com.mkyong.ws.HelloWorld")
public class HelloWorldImpl implements HelloWorld{

	@Override
	public String getHelloWorldAsString() {
		return "Hello World JAX-WS";
	}
}
```
Later, you will deploy this hello world web service on Tomcat.

## 2. sun-jaxws.xml
Create a web service deployment descriptor, which is also known as JAX-WS RI deployment descriptor – sun-jaxws.xml.

File : ```sun-jaxws.xml```
```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;endpoints
  xmlns="http://java.sun.com/xml/ns/jax-ws/ri/runtime"
  version="2.0"&gt;
  &lt;endpoint
      name="HelloWorld"
      implementation="com.mkyong.ws.HelloWorldImpl"
      url-pattern="/hello"/&gt;
&lt;/endpoints&gt;
```
When user access /hello/ URL path, it will fire the declared web service, which is HelloWorldImpl.java.
## 3. web.xml
Create a standard web.xml deployment descriptor for the deployment. Defines WSServletContextListener as listener class, WSServlet as your hello servlet.

File : ```web.xml```
```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE web-app PUBLIC "-//Sun Microsystems,
Inc.//DTD Web Application 2.3//EN"
"http://java.sun.com/j2ee/dtds/web-app_2_3.dtd"&gt;

&lt;web-app&gt;
    &lt;listener&gt;
        &lt;listener-class&gt;
                com.sun.xml.ws.transport.http.servlet.WSServletContextListener
        &lt;/listener-class&gt;
    &lt;/listener&gt;
    &lt;servlet&gt;
        &lt;servlet-name&gt;hello&lt;/servlet-name&gt;
        &lt;servlet-class&gt;
        	com.sun.xml.ws.transport.http.servlet.WSServlet
        &lt;/servlet-class&gt;
        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;
    &lt;/servlet&gt;
    &lt;servlet-mapping&gt;
        &lt;servlet-name&gt;hello&lt;/servlet-name&gt;
        &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;
    &lt;/servlet-mapping&gt;
    &lt;session-config&gt;
        &lt;session-timeout&gt;120&lt;/session-timeout&gt;
    &lt;/session-config&gt;
&lt;/web-app&gt;
```

## 4. WAR Content
Use Ant, Maven or JAR command to build a WAR file to include everything inside. The WAR content should look like this :
```
WEB-INF/classes/com/mkyong/ws/HelloWorld.class
WEB-INF/classes/com/mkyong/ws/HelloWorldImpl.class
WEB-INF/web.xml
WEB-INF/sun-jaxws.xml
```
**Note**
For those who are interested, here’s the Ant file to build this project and generate the WAR file.

File : ```build.xml```
```xml
&lt;project name="HelloWorldWS" default="dist" basedir="."&gt;
    &lt;description&gt;
        Web Services build file
    &lt;/description&gt;
  &lt;!-- set global properties for this build --&gt;
  &lt;property name="src" location="src"/&gt;
  &lt;property name="build" location="build"/&gt;
  &lt;property name="dist"  location="dist"/&gt;
  &lt;property name="webcontent"  location="WebContent"/&gt;

  &lt;target name="init"&gt;
        &lt;!-- Create the time stamp --&gt;
        &lt;tstamp/&gt;
        &lt;!-- Create the build directory structure used by compile --&gt;
        &lt;mkdir dir="${build}"/&gt;
  &lt;/target&gt;

  &lt;target name="compile" depends="init"
  	description="compile the source " &gt;
        &lt;!-- Compile the java code from ${src} into ${build} --&gt;
        &lt;javac srcdir="${src}" destdir="${build}"/&gt;
  &lt;/target&gt;

  &lt;target name="war" depends="compile"
  	description="generate the distribution war" &gt;

	&lt;!-- Create the war distribution directory --&gt;
  	&lt;mkdir dir="${dist}/war"/&gt;

  	&lt;!-- Follow standard WAR structure --&gt;
  	&lt;copydir dest="${dist}/war/build/WEB-INF/" src="${webcontent}/WEB-INF/" /&gt;
  	&lt;copydir dest="${dist}/war/build/WEB-INF/classes/" src="${build}" /&gt;

	&lt;jar jarfile="${dist}/war/HelloWorld-${DSTAMP}.war" basedir="${dist}/war/build/"/&gt;
  &lt;/target&gt;

&lt;/project&gt;
```

## 5. JAX-WS Dependencies

By default, Tomcat does not comes with any JAX-WS dependencies, So, you have to include it manually.

1. Go here ```http://jax-ws.java.net/.```
2. Download JAX-WS RI distribution.
3. Unzip it and copy following JAX-WS dependencies to Tomcat library folder “`````{$TOMCAT}/lib`````“.
```
jaxb-impl.jar
jaxws-api.jar
jaxws-rt.jar
gmbal-api-only.jar
management-api.jar
stax-ex.jar
streambuffer.jar
policy.jar
```

## 6. Deployment

Copy the generated WAR file to `````{$TOMCAT}/webapps/````` folder and start the Tomcat server.

For testing, you can access this URL : ```http://localhost:8080/HelloWorld/hello```, if you see following page, it means web services are deploy successfully.

![Image](tomcat-web-services/jaxws-deploy-tomcat-example.png?raw=true)

[Source Code](tomcat-web-services/JAX-WS-Deploy-To-Tomcat-Example.zip)</t>
<t tx="jonathanhudson.20201006084250.151"></t>
<t tx="jonathanhudson.20201006084250.152"></t>
<t tx="jonathanhudson.20201006084250.153">@nocolor
bin/d3sc gaming_bg --in lib/smoke-test/bg/xml/input/gaming/01-gaming_RECORD_Dictao.xml -sysprop service:Dictao1</t>
<t tx="jonathanhudson.20201006084250.154"></t>
<t tx="jonathanhudson.20201006084250.155">@nocolor
# PROD 

curl https://rofusweb.spillemyndigheden.dk/TamperTokenAnvend/TamperTokenAnvendService
&gt; /dev/tcp/rofusweb.spillemyndigheden.dk/443

tcptraceroute rofusweb.spillemyndigheden.dk 443

openssl s_client -connect rofusweb.spillemyndigheden.dk:443

# UAT 

&gt; /dev/tcp/rofusdemo.spillemyndigheden.dk/443

tcptraceroute rofusdemo.spillemyndigheden.dk 443

https://rofusdemo.spillemyndigheden.dk/TamperTokenAnvend/TamperTokenAnvendService</t>
<t tx="jonathanhudson.20201006084250.156">@language md

#### Commands

#### Inititialise the server
wxroot
```
wxadmin server+newsys albatros
```
#### Change the server
wxroot
```
wxadmin
````
#### Update server config

```
wxviconf
```

#### Running QA Tests

```
export WCS=/kognitio

/kognitio/tools/clsql -s mn2regblm0001d0 -u sys -p albatros

/kognitio/tools/clsql -s mn2regblm0001d0 -u sys -p albatros -f /path/to/script

./qarunner --server mn2regblm0001d0 --sys-password albatros
```</t>
<t tx="jonathanhudson.20201006084250.157">@language md

#### Notes

Connect to the worker node on port 6550

Username in client is : sys

</t>
<t tx="jonathanhudson.20201006084250.158">@language md

#### Temp2

Loader : ab1kgltmp0001p0
Worker : ab1kgwtmp0001p0

#### create license

```
source setenv
create_system --company=Bet365 --name=new_system  --descr="My New System"
create_license --system new_system --license_type=base_and_ram_anydisk_noexpiry --count=1000
create_license --system new_system --license_type=super_escrow --count=1000
```

#### server restart
```
wxserver smd all restart
wxserver start
```

#### change current version
wxadmin
```
wxserver install &lt;pathtow.wxpkg&gt; force
ls -al /opt/kognitio/wx2/installs/
wxserver set current_version &lt;version&gt;
wxserver smd all restart force
wxserver start
wxtop
 
wxserver install /tmp/kognitio/wx2-80200rel200221i-rps-2.wxpkg force
wxserver set current_version ver80200rel200221i-rps-2
wxserver smd all restart force
wxserver start
```

```
wxserver syn                            # List all the valid options for command
wxserver info versions                  # To see a list of installed versions
```

#### change config for qa
wxadmin
```
wxconftool -s 'boot options' -a 'external_tables=yes' -WS
wxconftool -s 'boot options' -a 'external_scripts=yes' -WS
wxserver smd all restart
wxserver start
```

#### update config
wxadmin
```
wxviconf
```

#### verify custom build running

```
wxsubmit -s ab1kgwtmp0001p0 -u sys -p albatros -x "select version, patch_level from sys.ipe_system"
```

#### new sys
from wxroot
```
wxadmin server+newsys albatros
```

#### bring in additional disks

wxadmin front-end, choosing options 'server' followed by 'reconfigure'
Note that reconfigure 'quickadd' option should always be used now, rather than the deprecated 'fulladd' option

#### clsql

```
export WCS=/kognitio   (so it knows where to look)
./qarunner --server mn2regblm0001d0 --sys-password albatros

/kognitio/tools/clsql -s mn2regblm0001d0 -u sys -p albatros

/kognitio/tools/clsql -s mn2regblm0001d0 -u sys -p albatros -f /path/to/script
```

#### qarunner
```
./qarunner --server mn2regblm0001d0 --sys-password albatros

./qarunner --server mn2regblm0001d0 --sys-password albatros
```

#### wxsubmit

```
/opt/kognitio/wx2/current/bin/wxsubmit -s ab1kgwtmp0001p0 -u sys -p albatros


Kognitio WX2 SQL Submission Tool v8.02.00-rel200221-rps-1
(c)Copyright Kognitio Ltd 1992-2019.

Connected to ab1kgwtmp0001p0 ODBC Version 8.02.00-rel200221-rps-1 Server Version 08.02.0000
&gt;TotalQueries    0         Session Time   0:45.4     ----     ----     ----
```

### wxprobe

from wxadmin

```
wxprobe -s 
wxprobe -H                                      # (or -HN) Show the node reporting RAM
wxprobe -HNF                                    # To confirm that the new nodes are identical
wxprobe -a '{can DB}' -wF | egrep '(ID|link)'   # List out the links for each database node
wxprobe -a '{can DB}' -HN                       # Check the number of network links being used
wxprobe -a '{can DB}' -HNF                      # To see the different types of WX2 nodes
wxprobe -a '{can DB}' -wF | egrep '(bay|CPU)';  # Check cores on each node
```

### wxtool

Check current pointer ( as in symlink )
```
wxtool -S 'ls -ltr /opt/kognitio/wx2' 
wxtool -S date                                                      # See current time on all nodes
wxtool –E ‘&lt;command&gt;’                                               # Run command on all nodes
wxtool -S '&lt;shell command&gt;'                                         # Run shell on all nodes
wxtool -a '{can DB}' -R | sort                                      # List the DB nodes
wxtool -a '{can DB}' -S "cat /opt/kognitio/wx2/etc/local_config"    # Confirm all nodes have been updated
```

### wxsync

```
wxsync -c                                       # To see the clock skew across all nodes
wxsync -C                                       # To synchronize clocks ( although should use ntp ) 
wxsync -S &lt;filename&gt;                            # Copy a file to all nodes
```

### logs folder

```
cd `wxlogd smd`
```
</t>
<t tx="jonathanhudson.20201006084250.159">@language md

WX2 configuration files are stored in the ‘/opt/kognitio/wx2/etc’ 

```
FAQ: What can be put in the configuration file?

Solution 00000325 by David Wild at 2012-09-03T15:11:16.000+0000

This is not a complete list of WX2 config. file settings. Note that in general you should not need to make these settings as the default behaviour should be sufficient. The only setting that MUST be made is [general] system_id.


[general]

1. system_id=mysystem

2. min_free_kbytes=50000

 

1. System id. Must match with system license name. This is what allows the smds to synchronise with each other to determine which nodes comprise a system.

2. Make the kernel swap daemon get in earlier, rather than waiting until there is almost no free memory. Should not normally be set.

 

[system]

1. external_net=eth0

2. memsize=16000000000

3. max_ram_per_proc=2139095040 

4. default_net=eth0

 

1. SF Solution 97: "To enable connection bouncing where ‘eth?’ is the interface that is being used by the clients to connect to the WX2 server". Note for this to work all clients should be able to see the relevant interface on all nodes, otherwise they can be bounced to a node that they cannot connect to.

2. Make this system behave as if the nodes were 16G. This will affect the output to wxprobe -HN

3. Controls how much RAM a WX2 process can use - a way of restricting how big RS nodes can grow.

4. Tells WX2 what network devices to use for internal traffic.  This isn't

always present though -- on appliance mode systems (LMG, POC10) this is

detected automatically from the Linux image.

 

[runtime parameters]

1. ci_systab_solo=1

2. ds_gsr_percent=80

3. lkti=30 

4. leave_embedded=1

5. rcus=10      

6. rsec=160

7. lockcheck_crash=0

8. q10_maxrun=1

9. q10_pause=1

 

1. Enable new "less intrusive, finer grain" locking code. The default setting from 6.1.7 onwards.

2. Reclaim a slab once we encounter a chunk (typically a 100KB buffer) with reclaimable space &gt; 80%. Then revert to reclaiming everything possible for that slab. Used to avoid worst case reclaim where a single record is reclaimable early in the slab, then hardly anything after that. 80 is the default now, so should be no need to set this parameter in normal use.

3. Lock Timeout setting. Time in seconds to wait for a pending lock. Default is one day.

4. Record embedded comments in SQL code in ipe_command. Default is to remove embedded comments. The only reason for setting this is to allow applications to pass audit information into the command log (as used on BT).

5. Number of pages to reserve per slab between the data growing up the slab, and data at the top of the slab. If the slab fills up, this setting can be reduced to allow a small number of writes to take place, which can be enough to allow reclaim to run. Default is 10, and if this is ever reduced on a machine, need to reset to the default afterwards otherwise the next time disk fills up you may not be able to recover.

6. Equivalent of (5) in sectors, but now obsolete as calculated from the setting in (5).

7. There is some code in the server which crashes if an internal lock is held for more than 10 minutes (not the lock manager locks - internal locks are grabbed to protect access to a particular resource inside the server, rather than a system/table/column lock). If the lockcheck_crash parameter is set to 0 this does not happen. Normally we want this switched on as it is a good way of detecting things like a node hanging - usually there will be attempts to send messages to that node, they will hold internal locks, and so after 10 minutes of the node not responding the server will tend to crash. 

8. Specify the maximum number of concurrent queries for queue 10 to be 1.

9. Specify that queue 10 is paused - i.e. no queries which would queue can be run.


[boot options]

1. raid_cluster_size=4   

2. virtual_diskstores=1    

3. enable_hugelb=no    

4. sessions_per_cp=30 

5. num_int_nodes=60

6. max_fixed_pool=4800000000

7. ds_ramsize=0x3400000

8. se_ramsize=300000000

9. fixed_pool_size=20

10. tm_ramsize=100000000

11. num_comp_nodes=20

 

1. RAID cluster size. Used to enable SW RAID. Newsys required to activate new settings.

2. Used to allow the system to boot even if disk resources are missing, assuming SW RAID is in use. WX2 will place a disk handling process for each missing disk resource somewhere in the system, and this will handle IO for that disk resource via SW RAID.

3. Enable huge pages, which was implemented as a possible aid for dealing with Linux resource exhaustion problems. No reason to use this.

4. Number of Association Managers (AMs or sessions) per I/O process (per WX node). Number of connections per node.

5. Number of interpreter processes (default is the maximum of the number of nodes in the system, and 20). If a system is likely to have more concurrent queries running, it is possible to exhaust the interpreter pool, so increasing this parameter and restarting WX2 is an expedient solution.
Note that to reduce the parameter you will probably need to set topup_intnodes=0 in the [boot options] section of the config file.
6. The maximum amount of memory allocated to non-WX2 usage. Before 6.1.7 this typically was the value used as the estimate from configuration tended to be too high. From 6.1.7 onwards, the estimates are better so this is simply a cap on the memory allowed for non-WX2 usage given all other settings.

7. Memory allocated to each Diskstore process. This amount of RAM used by each disk store is dependent on the size of the disk resources, and the number of ram stores in the system.

8. Sets the amount of RAM available for SQL compilers in the server. If set without the most significant bit being on, the compilers will be given dedicated RAM, whereas the default setting is for them to share with other processes on each node such as interpreters, misc and IO nodes (but not disk stores and ram stores).

9. Allocate 20% of memory for non-wx2usage - note this will be capped by max_fixed_pool. Currently used on mig_bupa2, which has a mixture of nodes with various RAM sizes, as the default sizing algorithm does not deal well with this situation. 

10. Amount of RAM available for each interpreter process. 

11. Number of compiler processes (default is 10).   


[disks]

1. direct_io=yes

2. sparse_file=yes

 

1. CASE 1108: Direct write to disk without going via Linux. To help eliminate Linux grabbing too much RAM and invoking OOM killer on WX2 processes. Now the default setting, so no need to use in config files.


2. Set to yes if you don't want the disks to be initialised during a newsys. Useful for quick recommisioning of test systems.

 

[wxsmd]

1. tolerate_changes=missing_node, missing_link, missing_disk

 

1. This allows WX2 to tolerate a missing disk resource when the system is started as well as broken links and missing nodes - otherwise the default is to fail to start with an error reporting which disk resources are missing. For the system to work in such a case, SW RAID must be on, and virtual_diskstores enabled.

 

[mpk]

1. queuelimit=10000

 

1. Prevents the Message Passing Kernel (MPK) from throttling messages at the default of 1000 outstanding messages. Not required.

 

[capabilities]

1. not_master_nodes=poc3-ap2


1. This is a way to prevent certain nodes being the master smd - the nodes in the system choose a master, and that has special requirements (e.g. if you have a recovery script which works off smd events, you'd want dumping to take place on a node which has access to lots of storage).

```</t>
<t tx="jonathanhudson.20201006084250.16" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: dev_d3s
  vars:
    env: 'dev'
    sourceDirectory: '/data/code'
    configFolder: '{{sourceDrive}}{{sourceDirectory}}/safe-configs/Denmark/DEV'
    commonTaskLocation: '{{sourceDirectory}}/safe-deployments/ansible/common/tasks'   
    executionDate: "{{ lookup('pipe', 'date +%Y%m%d-%H%M') }}"
    tamperTokenBackup: '~/backup/backup-tamper-token-safdk-232-{{ansible_date_time.date}}.zip'
    serviceState: []
    users:
      - { name: 'd3s-dk', oldTamperTokenFolder: '/mnt/emergency/d3s-dk/tampertokens/', newTamperTokenFolder: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/', component: 'd3s', port: '25080', context: 'dk', target: 'mn2regcap0001d1' }
    configs:
      - { name: 'configure.properties', user: 'd3s-dk', type: '', prefix: 'dk-d3s-', suffix: '-1', dstLocation: '~/d3s', srcLocation: '{{configFolder}}', target: 'mn2regcap0001d1' }      

  tasks:  
    
    # Stop the running service 
  - name: check service is running on {{inventory_hostname}}   
    uri: url=http://{{inventory_hostname}}:{{ item.port }}/d3s/jel/{{ item.context }}/monitoring.xml return_content=yes
    register: result
    ignore_errors: yes
    with_items: "{{users}}"
    when: item.target in ansible_hostname

  - name: set service status
    set_fact: serviceState="{{ serviceState + [item.status|default(0)] }}"
    with_items: "{{ result.results }}"
    
  - name: disable activator for running service {{inventory_hostname}}   
    uri: url=http://{{inventory_hostname}}:{{ item.0.port }}/activator/state?force=down return_content=yes
    register: result
    ignore_errors: yes
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) and item.0.component == 'd3s' )
  
  - name: wait for queues to empty before proceeding d3s
    uri: url=http://{{inventory_hostname}}:{{ item.0.port }}/d3s/jel/{{ item.0.context }}/monitoring.xml return_content=yes
    register: result
    ignore_errors: yes
    until: ( result.content.find( "Level1QueueSize&gt;0&lt;" ) != -1 and result.content.find( "Level2QueueSize&gt;0&lt;" ) != -1 )
    retries: 10
    delay: 120   
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) and item.0.component == 'd3s' )
  
  - name: check queues are empty before proceeding
    fail: msg="Queues are not empty unable to proceed"
    with_items: "{{ result.results }}"
    when: item.failed is defined and item.attempts == 10
   
  - name: run the d3sadmin stop command
    shell: ". /data/home/{{ item.0.name }}/.bashrc &amp;&amp; ~/d3s/bin/d3sadmin stop"
    become: yes
    become_user: "{{ item.0.name }}"
    become_method: dzdo
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) )
    
    # Backup tamper token files
  - name: create tamper token backup folder
    file: path="~/backup" state=directory mode=0770
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: backup tamper token source files
    archive: path={{item.oldTamperTokenFolder}} dest={{tamperTokenBackup}} remove=false format=zip
    register: zipTamperTokenFiles
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: check backup ok
    debug: msg="Backup tamper tokens successful"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and zipTamperTokenFiles.results[0] is defined and zipTamperTokenFiles.results[0].state == 'file'

    # Move tamper token files to shared storage
  - name: create new tamper token folder
    file: path={{item.newTamperTokenFolder}} state=directory mode=0770
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: unzip tamper tokens into new location
    unarchive: src={{tamperTokenBackup}} dest={{item.newTamperTokenFolder}} remote_src=yes
    register: moveTamperTokenFiles
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname

  - name: check move ok
    debug: msg="Move tamper tokens successful"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and moveTamperTokenFiles.results[0] is defined and moveTamperTokenFiles.results[0].state == 'directory'

    #Reconcile and move old tamper tokens
  - name: count tampertoken files in old location
    find: paths={{item.oldTamperTokenFolder}} recurse=yes file_type=file
    register: sourceTamperTokenCount
    no_log: True
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: count tampertoken files in new location
    find: paths={{item.newTamperTokenFolder}} recurse=yes file_type=file
    register: targetTamperTokenCount
    no_log: True
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: set move ok
    set_fact: copiedOk=True
    with_items: '{{users}}'
    when: item.target in ansible_hostname and sourceTamperTokenCount.results[0] is defined and sourceTamperTokenCount.results[0].matched == targetTamperTokenCount.results[0].matched

  - name: reconcile source files against target files success
    debug: msg="All tamper token files copied from old to new location - old '{{sourceTamperTokenCount.results[0].matched}}' - new - '{{targetTamperTokenCount.results[0].matched}}'"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and copiedOk is defined and copiedOk == True

  - name: reconcile source files against target files failed
    debug: msg="Missing files copied from old to new location - old '{{sourceTamperTokenCount.results[0].matched}}' - new - '{{targetTamperTokenCount.results[0].matched}}'"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and copiedOk is not defined

  - name: mv old tamper token folder to backup location
    command: mv {{item.oldTamperTokenFolder}} ~/backup/
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname and copiedOk is defined and copiedOk == True
    
    # Backup and update configure.properties
  - name: backup the configure.properties file
    copy: remote_src=true src=~/d3s/configure.properties dest=~/d3s/configure.properties.tt.bkp mode=775
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname  
  
  - name: push configs to remote folder
    copy: src={{ item.srcLocation }}/{{ item.type }}/{{ item.prefix }}{{ item.name }}{{ item.suffix }} dest={{ item.dstLocation }}/{{ item.name }} force=yes
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{ configs }}"
    when: item.target in ansible_hostname
    
    # Configure to use new tamper token location
  - name: run the d3sadmin configure script
    shell: ". /data/home/{{ item.user }}/.bashrc &amp;&amp; ~/d3s/bin/d3sadmin configure -Dforce=true"
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{configs}}"    
    when: item.target in ansible_hostname and item.name == 'configure.properties' 

    # Start the service
  - name: start d3s managed services
    service: name={{ item.name }} state=started enabled=yes
    become: yes
    become_user: root
    become_method: dzdo  
    with_items: "{{users}}"
    when: item.target in ansible_hostname

  - name: reset service variables
    set_fact:
      serviceState: []
    
  - file: path=/var/log/ansible state=touch
    become: yes
    become_user: root
    become_method: dzdo
    
  - name: log execution
    lineinfile: dest=/var/log/ansible line='{{ ansible_date_time.iso8601 }} relocate tamper token files' insertafter='EOF'
    become: yes
    become_user: root
    become_method: dzdo
</t>
<t tx="jonathanhudson.20201006084250.160">@language md


$ wxprobe -s

$ wxdgdump -m125 -C/data/tmp/rs125.core

#### Listing SQL from history file

wxsqlhist -f history.T_2020-09-14_18:11:47_BST

#### Extracting core dump from dumps

```
wxdumpx -t dumps.T_2020-09-14_18:11:52_BST.wxd | more

Core dump info: pid 255613, node mn2kgwork0001p0, length 0.
                type Ramstore, name WXDB(0): Ramstore
                mpid 0, crashed 1
                stopstring AS(MM_VALID_CHUNK(res)) rs.c:2577
                
wxdumpx -m0 -o rs0-jono.core 

wxdumpx -X -m0 -o rs0-jono.core dumps.T_2020-09-14_18:11:52_BST.wxd
```

$ gdb /kognitio/wx2/out/GPLinux/bin/wxdb ci104.core

thread apply all bt

t 4

bt

f 12
f 13

x /6d m

p *rootentry
p *mpkmessage
p /x *mpkmessage
p *a
p 0x68


# SQL History

# su - wxadmin

$ wxsqlhist -t 281431 -re
  

$ wxdhdiagnose -t 281431 -n 1 -D


</t>
<t tx="jonathanhudson.20201006084250.161">@language md

```
All Plans Complete: 806, Bad: 2595, Passes: 37623, Fails 653, Duration 1979s




T_2020-06-11_14:19:31_GMT: Stdout: CRASH FOR PID 1383 NODE 'mn2regblm0001d0' TMPID 0xffffffff TNAME '*** CRASHED ***: AM listener server' STOPSTRING 'ERROR: Signal SEGV at EIP 0x0862F1B5 data address 0xffffffff: address not mapped to object'
T


T_2020-06-11_13:27:04_GMT: Monitor started.
T_2020-06-11_13:27:04_GMT: Monitor stopped.
T_2020-06-11_13:27:05_GMT: Monitor started.
T_2020-06-11_14:13:00_GMT: Detected a crash on node mn2regblm0001d0:
T_2020-06-11_14:13:00_GMT: Process 1262: Name WXDB(WATCHDOG), nthreads 1, state T, size 16105472.
T_2020-06-11_14:13:00_GMT:             : ppid 893, pgid 1262, tracerpid 0, time 1(0,1)+3805(2504,1301).
T_2020-06-11_14:13:00_GMT:             : status Crashed(ERROR: Child pid 1362 exited on signal 9).
T_2020-06-11_14:13:00_GMT:             : mpid -1, type WATCHDOG

T_2020-06-11_14:13:00_GMT: SERVER CRASH!
T_2020-06-11_14:20:05_GMT: Detected a crash on node mn2regblm0001d0:
T_2020-06-11_14:20:05_GMT: Process 1383: Name WXDB(21): IO Node, nthreads 323, state T, size 1279262720.
T_2020-06-11_14:20:05_GMT:             : ppid 1262, pgid 1262, tracerpid 0, time 38238(16144,22094)+36265(15262,21003).
T_2020-06-11_14:20:05_GMT:             : status Crashed(ERROR: Signal SEGV at EIP 0x0862F1B5 data address 0xffffffff: address not mapped to object).
T_2020-06-11_14:20:05_GMT:             : mpid 21, type IO Node



```</t>
<t tx="jonathanhudson.20201006084250.162">@language md

#### Notes

User /data but had to create sym link to /kognitio

#### Runningon Build master

#### Kognitio

export WCS=/kognitio

#### From wxroot
```
$ wxadmin server+newsys albatros

$ wxviconf

[boot options]
external_tables=yes
external_scripts=yes

$ wxserver smd all restart
```
#### From wxadmin

```
$ wxserver install &lt;pathtow.wxpkg&gt;
$ wxserver set current_version &lt;version&gt;
$ wxserver smd all restart
$ wxserver start
$ wxtop
```
#### Kognitio qatools

./qarunner --server mn2regblm0001d0 --sys-password albatros

grep -r -i 'mn2regblm0001d0' --include \*.log | grep 'Driver' | more

/kognitio/tool/bin/clsql -s &lt;worker-node&gt; -u tester99 -p &lt;pass&gt; -f &lt;path-to-script.scr&gt;

/kognitio/tool/bin/clsql -s &lt;worker-node&gt; -u tester99 -p &lt;pass&gt; -f &lt;path-to-script.scr&gt; -qotf    &lt;quit on test fail&gt;

# wxsubmit into server
```
$ wxsubmit 

diagnose &lt;query&gt;

select count(*) from sys.ipe_process where type = 'rs';   // ram stores

/opt/kognitio/wx2/current/bin/wxsubmit -s &lt;worker-node&gt; -u tester99 -p &lt;pass&gt; 

explain fragtest;

^*^wx_create_tno(),wx_update_tno(),*
```
</t>
<t tx="jonathanhudson.20201006084250.163">@language md

```
-bash-4.2$ make -r out/GPLinux/bin/lkgen
  PATCHSTRING     (none: version.h not correctly filtered)
reloading makefile ...
  GCC OBJECT      GPLinux       lkgen.apps.o         liscram.c
  GCC OBJECT      GPLinux       lkgen.apps.o         lkgen.c
  AR              GPLinux       liblkgen.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/liblkgen.a
  RANLIB          GPLinux       liblkgen.a
  GCC OBJECT      GPLinux       licode.libs.o        ligen.c
  GCC OBJECT      GPLinux       licode.libs.o        litos.c
  GCC OBJECT      GPLinux       licode.libs.o        lifetch.c
  GCC OBJECT      GPLinux       licode.libs.o        licomp.c
  GCC OBJECT      GPLinux       licode.libs.o        lichk.c
  GCC OBJECT      GPLinux       licode.libs.o        liload.c
  AR              GPLinux       liblicode.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/liblicode.a
  RANLIB          GPLinux       liblicode.a
  GCC OBJECT      GPLinux       sconfig.libs.o       sctime.c
  GCC OBJECT      GPLinux       sconfig.libs.o       sconfig.c
  GCC OBJECT      GPLinux       sconfig.libs.o       sclayout.c
  GCC OBJECT      GPLinux       sconfig.libs.o       scparse.c
  GCC OBJECT      GPLinux       sconfig.libs.o       scperms.c
  GCC OBJECT      GPLinux       sconfig.libs.o       cfparse.c
  GCC OBJECT      GPLinux       sconfig.libs.o       scmerge.c
  GCC OBJECT      GPLinux       sconfig.libs.o       scmaster.c
  GCC OBJECT      GPLinux       sconfig.libs.o       ncdisks.c
  GCC OBJECT      GPLinux       sconfig.libs.o       ncnetwork.c
  GCC OBJECT      GPLinux       sconfig.libs.o       ncnodeinfo.c
  GCC OBJECT      GPLinux       sconfig.libs.o       ncnodecfg.c
  AR              GPLinux       libsconfig.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/libsconfig.a
  RANLIB          GPLinux       libsconfig.a
  GCC OBJECT      GPLinux       hdfsdslib.libs.o     hdfsdslib.c
  AR              GPLinux       libhdfsdslib.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/libhdfsdslib.a
  RANLIB          GPLinux       libhdfsdslib.a
  LD STATIC-APP   GPLinux       lkgen
-bash-4.2$ cp out/GPLinux/bin/lkgen /kognitio/lkgen/bin/linux
```

New license key

```
kognitio on mn2regblm0001d0 at ~/lkgen $create_system --company=Bet365 --name=rps-kog --descr="RPS System 20200612"
System rps-kog exists!
kognitio on mn2regblm0001d0 at ~/lkgen $create_system --company=Bet365 --name=rps-kog-1 --descr="RPS System 20200612"
kognitio on mn2regblm0001d0 at ~/lkgen $create_license --system rps-kog-1 --license_type=base_and_ram_anydisk_noexpiry --count=1000
Kognitio WX2 Licence Key Generator v8.02.00-rel200221-rpsbuild-1
(c)Copyright Kognitio Ltd 2012-2019.

Generating key with command:
lkgen_V1 -s rps-kog-1 -c 1000 -t /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry -n 50000004
Gave key: gx0d_q6wBGYXs0EtY6_S05L7qHXPGZ8ALMOkoRR3MJcR_JZ353_VlUFx9qeaa5GmFleRBdu6Jmd0!
GENERATED LICENSE:
License 50000004:
system : rps-kog-1
status : current
gen_command : lkgen_V1 -s rps-kog-1 -c 1000 -t /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry -n 50000004
license_number : 50000004
key : gx0d_q6wBGYXs0EtY6_S05L7qHXPGZ8ALMOkoRR3MJcR_JZ353_VlUFx9qeaa5GmFleRBdu6Jmd0
typearg : /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry
company : Bet365
count : 1000
version : 1
license_type : base_and_ram_anydisk_noexpiry
system_descr : RPS System 20200612
company_descr : Bet365.com

LICENCE KEY gx0d_q6wBGYXs0EtY6_S05L7qHXPGZ8ALMOkoRR3MJcR_JZ353_VlUFx9qeaa5GmFleRBdu6Jmd0
Lic 50000004:  Num licences 1000.
           generated Fri Jun 12 10:57:09 2020.
           Type BASE.
   Licence allows:
           external_scripts yes
           external_tables yes
           new_system yes
           ram_node_gb 1000 (1 * 1000)
   Licence valid if:
           base_license yes
           license_version 1
           system_id rps-kog-1

COMPLETE.  LICENSE KEY IS:
gx0d_q6wBGYXs0EtY6_S05L7qHXPGZ8ALMOkoRR3MJcR_JZ353_VlUFx9qeaa5GmFleRBdu6Jmd0
```</t>
<t tx="jonathanhudson.20201006084250.164">@language md

```

dzdo su - kognitio

cd /kognitio/

cd lkgen/

source setenv

create_system --company=Bet365 --name=rps_system --desc="RPS Test System"
create_system --company=Bet365 --name=prod_extra --desc="prod_extra"
create_system --company=Bet365 --name=dr_extra --desc="dr_extra"
create_system --company=Bet365 --name=rtl_extra --desc="rtl_extra"
create_system --company=Bet365 --name=bet365_22 --desc="bet365_22"

create_license --system prod_extra --license_type=base_and_ram_anydisk_noexpiry --count=1000
create_license --system dr_extra --license_type=base_and_ram_anydisk_noexpiry --count=1000
```

created a new template for MIS with ram limit of 15360

/data/home/kognitio/lkgen/templates/base_and_ram_15360_anydisk_noexpiry
/data/home/kognitio/lkgen/templates/base_and_ram_12288_anydisk_noexpiry

Used the license template

```
create_license --system dr_extra --license_type=base_and_ram_15360_anydisk_noexpiry --count=1000
create_license --system bet365_22 --license_type=base_and_ram_15360_anydisk_noexpiry --count=1000
create_license --system bet365_23 --license_type=base_and_ram_15360_anydisk_noexpiry --count=1000

create_license --system rtl_extra --license_type=base_and_ram_12288_anydisk_noexpiry --count=1000
create_license --system bet365_21 --license_type=base_and_ram_12288_anydisk_noexpiry --count=1000

```</t>
<t tx="jonathanhudson.20201006084250.165">@language md

The default network MTU ( maximum transmission unit ) of 1500 can be too small for the Kognitio cluster

This should be increased by using whats known as "jumbo frames" up to 9000

This is something that would be configured as supported in the network switch and then the DHCP server config would
need to be changed so that it created ip addresses with an MTU size of 9000

You can see the current MTU on the interface by calling

```
ifconfig

eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.48.88.16  netmask 255.255.255.0  broadcast 10.48.88.255
        inet6 fe80::725a:fff:fe3e:1643  prefixlen 64  scopeid 0x20&lt;link&gt;
        ether 70:5a:0f:3e:16:43  txqueuelen 1000  (Ethernet)
        RX packets 20041127  bytes 14082512766 (13.1 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 21266008  bytes 14349537521 (13.3 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        device interrupt 16  memory 0xe3100000-e3120000  

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;
        loop  txqueuelen 0  (Local Loopback)
        RX packets 4234642  bytes 2950494761 (2.7 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 4234642  bytes 2950494761 (2.7 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
        inet 192.168.122.1  netmask 255.255.255.0  broadcast 192.168.122.255
        ether 52:54:00:29:34:24  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

```
</t>
<t tx="jonathanhudson.20201006084250.166">@language md

#### Applying Patches

```
rm wx2 -Rf
git clone repo/wx2-8.2.0.git wx2
cd wx2
git am /tmp/kognitio/2020-06-17/wx2-8.2.0.patch/00*
git am /tmp/kognitio/2020-06-19/wx2-8.2.0.patch/00*
```


#### Apply Patch Release from Kognitio

Periodically Kognitio
generate patch releases containing bug fixes, these releases will not be
required to be applied by all users. 

A patch release can be applied to a system
if the release number (e.g. 7.2.1) remains the same, but the label after the release
number has changed. 

Patch builds are standalone, complete releases and do not
depend on the previous version installed on the machine. Patch builds are
applied with the following syntax:

```
wxserver install &lt;pathname to patch file *.wxpkg&gt;
wxserver set current_version &lt;patch version - e.g. ver70201rel120803&gt;
wxserver smd all restart
wxserver start
```
</t>
<t tx="jonathanhudson.20201006084250.167">@language md

#### Notes

```
export PATH=$PATH:/opt/kognitio/wx2/ver80200rel200617-rps-3/bin

make clean
make all

./wxtpcds -s ab1kgwtmp0001p0 -u sys -p albatros -sf 1 -gd
```

#### Performance figures

#### Scanning

With 2.6GHz AMD CPUs, 32 cores per node, we can scan at up to 1.5 billion rows/second/node with 7.2.1 code as of April 2012. With some internal experiments we have easily got to 3.9 billion rows/second/node (on very short rows, of course, to maximumse the rows/second figure). With a more representative row size of 64 bytes (rather than 4 used for the above), the original 1.5 billion rows/second/node becomes 600 million rows/second/node.

From disk, back in November 2011 with 7.1.2 code we were scanning at 92MB/s/disk, which was pretty much as fast as hdparm could read from disk on the hardware concerned.

From figures back in 2010, it did not make any difference to disk scan performance whether a raw partition or large file was used for the disk resource.

#### Loading

In November 2011 we used wxloader to load a 32 node server using 2 APs and got a rate of 13.4TB/hour.

#### Defragmentation

Defragmenting a 68GB image (140 million rows) to reclaim half the space (back to 34GB, 70 million rows) took 10-15 seconds back in early 2011.

#### Simple queries

With the work already done for simple queries, we are seeing them running 3 times as fast in pre-releases of version 8 as they were in 7.2.1.
</t>
<t tx="jonathanhudson.20201006084250.168">@language md

#### Notes

Old notes on prereq checks provided by MIS

```
#!/bin/bash

#####################################################################################
#RHEL 6.5 additional packages
echo -e "Checking Additional RHEL 6.5 Packages...\n"

#By default RHEL 6.5 does not install 32 bit packages on 64 bit environments.
#Ensure that the following non-default packages (and dependencies) are installed on all Kognitio nodes:
#glibc.i686
#libgcc.i686
#zlib.i686
#openssl.i686
#perl

#Example, to determine glibc packages that are installed:
# yum list installed | grep glibc
#glibc.i686                            2.5-118.el5_10.2                 installed
#glibc.x86_64                          2.5-118.el5_10.2                 installed
#glibc-common.x86_64                   2.5-118.el5_10.2                 installed
#glibc-devel.i386                      2.5-118.el5_10.2                 installed
#glibc-devel.x86_64                    2.5-118.el5_10.2                 installed
#glibc-headers.x86_64                  2.5-118.el5_10.2                 installed

echo "Package glibc.i686 should be installed, problem if it is not listed below (running yum list installed | grep glibc)"
yum list installed | grep glibc
echo "Package libgcc.i686 should be installed, problem if it is not listed below (running yum list installed | grep libgcc)"
yum list installed | grep libgcc
echo "Package zlib.i686 should be installed, problem if it is not listed below (running yum list installed | grep zlib)"
yum list installed | grep zlib
echo "Package openssl.i686 should be installed, problem if it is not listed below (running yum list installed | grep openssl)"
yum list installed | grep openssl
echo "Package perl should be installed, problem if it is not listed below (running yum list installed | grep perl)"
yum list installed | grep perl

#####################################################################################
#Symbolic links
echo -e "Checking Symbolic links to ssl libraries...\n"

#The following libraries or links need to exist for correct funciton of ssl:
#/usr/lib/libcrypto.so
#/usr/lib/libssl.so

# libcrypto libraries

echo "/usr/lib/libcrypto.so and /usr/lib/libssl.so should exist"
ls -l /usr/lib/libcrypto.so*
ls -l /usr/lib/libssl.so*

# different versions of ssl may install the libraries in /lib but not create the symbolic links at /usr/lib 
ls -l /lib/libcrypto.so*
ls -l /lib/libssl.so*

#####################################################################################
# Ensure Firewall is disabled. REQUIRED
echo -e "Checking Status of local Firewall...\n"

/sbin/service iptables status

# To turn off the firewall:
# /sbin/service iptables save 
# /sbin/service iptables stop
# /sbin/chkconfig iptables off

#####################################################################################
#Ensure SELinux is disabled. REQUIRED
echo -e "Checking status of SELinux...\n"

#SELinux can interfere with the internode operations of Kognitio, so this needs to be disabled on all of the Kognitio nodes.

/usr/sbin/sestatus

#SELinux status:                 disabled

#If enabled, SELinux can be disabled immediately with
# /usr/sbin/setenforce 0

#To permamenently disable (i.e. after next reboot), add the following line to /etc/sysconfig/selinux
#SELINUX=disabled

#Or to check that this line already exists:
## grep '^SELINUX=' /etc/sysconfig/selinux

#SELINUX=disabled

#####################################################################################
#Ensure kdump is disabled on DB nodes. REQUIRED
echo -e "Checking status of kdump service...\n"

#Make sure that kdump is disabled:

/sbin/service kdump status

#To permanently disable kdump
## /sbin/service kdump stop

#####################################################################################
#Ensure swap is disabled. RECOMMENDED
echo -e "Checking status of swap...\n"

#This is a recommendation rather than a requirement (swap space doesn't really do any harm, it just doesn't get used effectively).
# swap
free | grep -i '^swap:' | awk '{print $2;}'

grep -i swap /etc/fstab

# 
#swap can permanently disabled with: 
#Turnoff all active swap spaces:
## swapoff -a 
#To make this permanent:
#comment out (with a #) or remove the 'swap' line or lines from /etc/fstab 
# 
#Check swap space is not being used:
#Run 'free' and check that swap is set to 0 
## free
#             total       used       free     shared    buffers     cached
#Mem:      66001704   65831796     169908          0     596840   64347704
#-/+ buffers/cache:     887252   65114452
#Swap:            0          0          0
#
# 
######################################################################################
#Ensure IO scheduler on DB nodes is set to noop. REQUIRED
echo -e "Checking IO scheduler (should be [noop] on DB nodes...\n"

# io scheduler
ls -1 /sys/block/cciss*/queue/scheduler
ls -1 /sys/block/sd*/queue/scheduler

cat /sys/block/cciss*/queue/scheduler
cat /sys/block/sd*/queue/scheduler

# 
#For DB nodes, the IO scheduler for the disk devices must not re-order IO messages.
#
#The default IO scheduler is shown in square brackets ([]) and if, for example, cfq is enabled then this can be checked by using the command cat /sys/block/cciss*/queue/scheduler which would show:
#          noop anticipatory deadline [cfq]
# 
#In order to change the IO scheduler to noop copy noop to each scheduler “file”. I.e.:
#          # wxtool –a '{can DB}' –S 'echo "noop" &gt; /sys/block/cciss\!c0d0/queue/scheduler'
#          etc.
# 
#To identify the devices:
#          # wxtool –a '{can DB}' –S 'ls -1 /sys/block/cciss*/queue/scheduler'
# 
#To check all DB nodes:
#          # wxtool –a '{can DB}' –S 'cat /sys/block/cciss*/queue/scheduler'
## wxtool –a '{can DB}' –S 'cat /sys/block/sd*/queue/scheduler'
# 
#The output should now look like:
#      [noop] anticipatory deadline cfq
# 
#Note: the MOP target string '{can DB}' means “all nodes that have DB capability”.
# 
######################################################################################
#Ensure connection tracker is not running. REQUIRED
echo -e "Checking that connection tracker is not running...\n"

#Linux kernel module 'onnection tracker' matches messages with reqponses, and is used by NAT routing and the firewall. 
#To find it:
# connection tracker
lsmod | grep conntrack

## lsmod | grep conntrack
# 
#The loaded module can be unloaded with
## rmmod ip_conntrack
# 
#The module may be re-loaded by services that load it on start-up. These services need to be identified and prevented from running or loading the connection tracker module.
# 
######################################################################################
#Existence of users and groups. REQUIRED
echo -e "Checking required users and groups...\n"

#The bet365 Linux build standards requires that the groups wxadmin and wxetern and users wxadmin wxroot and wxextern are created before installing Kognitio.
# users and groups
grep -e '^wxadmin:' /etc/passwd
grep -e '^wxroot:' /etc/passwd
grep -e '^wxadmin:' /etc/group

# 
######################################################################################
#Ensure necessary libraries are installed. REQUIRED
echo -e "Checking 32 bit libraries...\n"

#libgcc and libz 
# 32 bit libraries
ls -l /lib/libgcc_s.so.1
ls -l /lib/libz.so.1

# 
######################################################################################
#Other (obvious?) checks 
# 
######################################################################################
#Ensure required Ethernet devices come up on boot. REQUIRED
echo -e "Checking that Ethernet devices come up on boot...\n"

# ethernet devices onboot
grep -i onboot /etc/sysconfig/network-scripts/ifcfg-*

## grep -i onboot /etc/sysconfig/network-scripts/ifctg-*
#/etc/sysconfig/network-scripts/ifcfg-eth0:ONBOOT=yes
#/etc/sysconfig/network-scripts/ifcfg-eth3:ONBOOT=no
#/etc/sysconfig/network-scripts/ifcfg-lo:ONBOOT=yes
#
######################################################################################
#Ensure Network interfaces enumerate correctly. REQUIRED
echo -e "Checking Network interfaces enumerate correctly...\n"

#Ports can be assigned different names after a reboot, and so it's strongly advisable to create a renaming script that runs at boot time to ensure they are named in a consistent way.
# 
######################################################################################
#Ensure IP addresses are unique for Kognitio nodes. REQUIRED
echo -e "Checking IP addresses are unique for nodes...\n"

#Ensure that the IP address assigned to the Kognitio nodes do not exist anywhere else on the network.
# 
######################################################################################
#Ensure NIC adapter software, firmware and drivers are correct and up-to-date. RECOMMENDED 
echo -e "Ensure NIC adapter software, firmware and drivers are correct and up-to-date...\n"

# check NIC adapter 
/sbin/lspci | grep -i ether

#09:00.0 Ethernet controller: Emulex Corporation OneConnect 10Gb NIC (be3) (rev 01)
#09:00.1 Ethernet controller: Emulex Corporation OneConnect 10Gb NIC (be3) (rev 01)

ethtool -i eth4
ethtool -i eth5

######################################################################################
#Ensure Firmware and BIOS is up-to-date. RECOMMENDED
echo -e "Ensure Firmware and BIOS is up-to-date...\n"

# 
######################################################################################
#Check Network link speeds post install. RECOMMENDED
echo -e "Checking Network link speeds...\n"

ethtool eth4  | grep -i speed
ethtool eth5  | grep -i speed


AND

#Other packages required. ksh, inotify-tools &amp; postfix (sendmail) is only for loaders
yum list installed | egrep "^bzip2|^coreutils|^diffutils|^findutils|^gawk|^grep|^gzip|^hp-health|^inotify-tools|^ksh|^mailx|^net-tools|^openssh-clients|^procps|^psmisc|^redhat-lsb|^rsync|^sed|^postfix|^tar|^time|^unzip|^util-linux"


The ethtool checks are no longer valid (eth4 / eth5 are no longer the standard naming for ports)
There may be other checks which have been updated. If you come across any, drop me a message and I’ll verify.

```</t>
<t tx="jonathanhudson.20201006084250.169">@language md

#### Notes

Servers

#### Loader
ab1kgltmp0001p0

#### Workers

ab1kgwtmp0001p0
ab1kgwtmp0002p0
</t>
<t tx="jonathanhudson.20201006084250.17" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># Install cygwin pre-requisites
binutils
curl
gmp
libgmp-devel
make
python (2.7.x)
python-crypto
python-openssl
python-setuptools
python2-devel
git (2.5.x)
nano
openssh
openssl
openssl-devel

# Install Ansible
easy_install-2.7 pip
CFLAGS="-g -O2 -D_BSD_SOURCE" pip install -U pycrypto
pip install ansible
exit
nano ansible.cfg

ansible
ansible-galaxy install mrlesmithjr.base
ansible-galaxy install mrlesmithjr.bootstrap
   
# Create ssh keys for login
ssh-keygen -t rsa -f .ssh/id_jonathanhudson

# Copy ssh public key to remote "authorized_keys"

# Edit Ansible hosts file 

localhost ansible_connection=local

[webservers]
jonohudson.co.uk   ansible_user=root

# Create ansible config file in ansible (current) folder ansible.cfg
[ssh_connection]
ssh_args = -o ControlMaster=no
remote_user = root
private_key_file = /home/jonoh_000/.ssh/id_jonohudson

# Create an exampel ansible yaml file (don't use tabs)
---
- hosts: all
  tasks:
    - shell: echo "hello world"
- hosts: webservers
  tasks:
    - script: ~/ansible/change-www-owner.sh
	
# Ensure the remote folder for the script exists
~/ansible/

# Ensure file has linux eol endings

# Finally run the playbook
ansible-playbook change-owner.yml</t>
<t tx="jonathanhudson.20201006084250.171">@language md

```
Instructions for compiling Kognitio 8.2.0

Source files:

kognitio-wx2-8.2.0.git.tar.gz -- source code bundle
kognitio-compenv.git.tar.gz -- the Kognitio build environment

Below is a walkthrough of the process required to build Kognitio WX2 for standalone deployment from source.  This is intended to be illustrative and to be used as a 'quickstart' for the initial installation.  Once you have this working you can modify the steps to suit your environment.  

These instructions are simply a guide to build the software and are not an authorisation to do so.  All Kognitio software contained in these distributables is (c) Kognitio and all rights are reserved.  You need to have an appropriate license or agreement with Kognitio before making use of our source code.


== Linux setup ==

Start with a CentOS 7.8 system (older releases should work and other Linuxes are likely to work but these instructions were written on a CentOS 7.8 system and may need to be adapted for other environments).  Full package list from a working CentOS machine is in the centos-package-list file accompanying this.  The system needs to have the 32 bit runtime environment installed allong with some other helper packages:

$ sudo yum install make perl glibc.i686 zlib.i686 zip git m4 ctags-etags libstdc++.i686 

Now, create a kognitio user to build the software:

$ sudo useradd -c "Kognitio build user" -d /kognitio -m kognitio

And switch to the new user with sudo.  The remainder of these steps will be done as the Kognitio user.  Note that this user has a home directory of /kognitio.  This is because the kognitio build tool chain currently relies on absolute paths (this is a limitation of the gnu compiler toolchain) and needs to be installed at a specific location.  Most of the things we create below can be placed anywhere you like but the /kognitio/tools/compenv tree needs to have the correct full path in order for things to work.

$ sudo su - kognitio


== set up the build tree ==

The kognitio deliverables are supplied as bare git repositories (to make them small and to make it easy to update them later if needed).  You will need to unpack these into a local directory and use git to create working trees from these repositories:

$ cd /kognitio
$ mkdir repo
$ mkdir tools
$ tar -C repo -xzf /tmp/kognitio-compenv.git.tar.gz 
$ tar -C repo -xzf /tmp/kognitio-wx2-8.2.0.git.tar.gz 
$ ls repo
kognitio-compenv.git  wx2-8.2.0.git

$ git clone repo/kognitio-compenv.git tools/compenv
$ git clone repo/wx2-8.2.0.git wx2

Now you should have a directory called 'tools' which contains the build tools, compilers, etc used to build Kognitio and one called wx2 which contains the source.  (Do not use dots in the name of the wx2 folder, the Kognitio build scripts will not work if you do!)
Note again that the compenv repo is cloned into /kognitio/tools/compenv.  You have to use this path, THE COMPENV PACKAGE DOES NOT WORK FROM ANY OTHER PATH.  The kognitio-compenv-build repository (also supplied with these materials) can be used to build a new compile environment which works from a different path but this is a long process and is outside the scope of these instructions.


== Setting up the source tree ==

The git repository contains the source for all official Kognitio releases and git tags can be used to select a release to build.  If you just want to build the latest Kognitio release then the tree will be ready to do this.  For previous releases you can checkout a particular git tag to get the source for that release.  For 8.2.0-rel190606, for example, you can run:

$ git checkout sys80200rel190606

You can see the full list of available releases with this command:

$ git tag --list |grep rel


Before building you need to set a version string for your build (you can leave this alone but it will be difficult for everyone to tell your builds apart from Kognitio's if you don't change it).  Usually you will leave the version number alone and change the patch string, which typically reads 'relNNNNNN'.  You can do this by editing wx2/include/version.h.  You need to change the WCS_PATCHSTRING line to something appropriate, for example:

#define WCS_PATCHSTRING "rel200221-mybuild"

== Compiling the software ==

cd into the wx2 directory to run the rest of these commands.  First you need to generate makefiles with this command:

$ make -r makefiles
  DMAKE           dmakefile
  DMAKE           cmakefile
  DMAKE           dmakefile
  DMAKE           cmakefile
  PATCHSTRING     (none: version.h not correctly filtered)
reloading makefile ...
  GCC OBJECT      GPLinux       fsbuilder.apps.o     fsdesc.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     fsdep.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     wxpkgfs.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     vfs.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     dbgfs.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     fsdprint.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     fsbuilder.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     minix.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     tar.c
  GCC OBJECT      GPLinux       fsbuilder.apps.o     fsdreader.c
  AR              GPLinux       libfsbuilder.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/libfsbuilder.a
  RANLIB          GPLinux       libfsbuilder.a
Building buildroot.h
  GCC OBJECT      GPLinux       gputils.libs.o       gpflatten.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpqa.c
  GCC OBJECT      GPLinux       gputils.libs.o       threadutils.c
  GCC OBJECT      GPLinux       gputils.libs.o       gphash.c
  GCC OBJECT      GPLinux       gputils.libs.o       gputils.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpstring.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpdaemon.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpstreams.c
  GCC OBJECT      GPLinux       gputils.libs.o       gplists.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpunicode.c
  GCC OBJECT      GPLinux       gputils.libs.o       gphashfunc.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpwcb.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpfile.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpsmbios.c
  GCC OBJECT      GPLinux       gputils.libs.o       gperrors.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpuids.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpfork.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpxm.c
  GCC OBJECT      GPLinux       gputils.libs.o       sockutils.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpzip.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpsi.c
  GCC OBJECT      GPLinux       gputils.libs.o       gpproc.c
  AR              GPLinux       libgputils.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/libgputils.a
  RANLIB          GPLinux       libgputils.a
  LD STATIC-APP   GPLinux       fsbuilder
  GCC OBJECT      GPLinux       getversion.apps.o    getversion.c
  AR              GPLinux       libgetversion.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/libgetversion.a
  RANLIB          GPLinux       libgetversion.a
  LD STATIC-APP   GPLinux       getversion
  FSBUILDER DEP   -             wx2-linux-clients.tar.d
  FSBUILDER DEP   -             wx2-kodoop.tar.d
  FSBUILDER DEP   -             wx2-windows64-clients.tar.d
  FSBUILDER DEP   -             wx2-server.tar.d
  FSBUILDER DEP   -             wx2-windows-clients.tar.d
  FSBUILDER DEP   -             wx2-linux64-clients.tar.d
  FSBUILDER DEP   -             wx2-cygwin-clients.tar.d
  FSBUILDER DEP   -             wx2-windows64-gui-clients.tar.d
  FSBUILDER DEP   -             wx2-windows-gui-clients-tng.tar.d
  FSBUILDER DEP   -             wx2-koglul-linux.tar.d
  FSBUILDER DEP   -             kodoop.tar.d
  FSBUILDER DEP   -             wx2-windows-gui-clients.tar.d
Building extversion.h
  PATCHSTRING     (none: version.h not correctly filtered)
reloading makefile ...
  FSBUILDER DEP   -             wx2-80200rel200221-mybuild.wxpkg.d
  PATCHSTRING     (none: version.h not correctly filtered)
reloading makefile ...
  DMAKE           dmakefile
  DMAKE           cmakefile

Once makefiles are ready you can compile the Kognitio software.  If you want to make a binary as close to an existing Kognitio release as possible you can use this command:

make -r packages

This is the command Kognitio uses to build the official software releases and it will result in as repeatable a build process as possible (so if you build a second time you are likely to get the same binary again.  If the build fails partway through you will need to rm -rf the 'out' subdirectory and re-run the makefile build step above to have the build process remain repeatable.

To make a faster, but less repeatable build (which is functionally the same but whose executables may link objects in a different order, etc so you can't debug against a core made from a previous build attempt) you can use this command:

make -rj12 wx2

This will just build the .wxpkg file which you use to deploy the software and will build using 12 parallel streams.  This is the way we build the software while doing product development.


==  Installing ==

Your build process puts all generated files into the 'out' subdirectory.  You can 'rm -rf' this to reset the build back to the way it was before you compiled anything (this is the same as a 'make clean').  The build creates deliverables in the 'out/packages' subdirectory:

$ ls -l out/packages/
total 618020
-rwxrwxr-x. 1 kognitio kognitio 30861408 May  3 15:04 install-wx2-80200rel200221-mybuild
-rwxrwxr-x. 1 kognitio kognitio 68663698 May  3 14:59 jdbc_odbc_bridge_installer_80200rel200221-mybuild.sh
-rw-rw-r--. 1 kognitio kognitio 86711201 May  3 15:04 kodoop.tar.gz
-rw-rw-r--. 1 kognitio kognitio   200087 May  3 15:03 KognitioJDBC.jar
-rwxrwxr-x. 1 kognitio kognitio 97100200 May  3 15:03 linux-install-80200rel200221-mybuild.sfx
-rw-rw-r--. 1 kognitio kognitio 96719436 May  3 15:03 wx2-80200rel200221-mybuild.wxpkg
-rw-rw-r--. 1 kognitio kognitio 22605122 May  3 15:05 wx2clients-32-80200rel200221-mybuild.exe
-rw-rw-r--. 1 kognitio kognitio 22718705 May  3 15:05 wx2clients-64-80200rel200221-mybuild.exe
-rw-rw-r--. 1 kognitio kognitio 19460672 May  3 15:04 wx2console-32-80200rel200221-mybuild.exe
-rw-rw-r--. 1 kognitio kognitio 19504699 May  3 15:05 wx2console-64-80200rel200221-mybuild.exe
-rw-rw-r--. 1 kognitio kognitio  2478148 May  3 15:03 wx2-cygwin-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 30864232 May  3 15:04 wx2-kodoop.tar.gz
-rw-rw-r--. 1 kognitio kognitio  3678621 May  3 15:03 wx2-koglul-linux.tar.gz
-rw-rw-r--. 1 kognitio kognitio 12286205 May  3 15:04 wx2-linux64-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 29902556 May  3 15:04 wx2-linux-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 30860345 May  3 15:04 wx2-server.tar.gz
-rw-rw-r--. 1 kognitio kognitio  3484870 May  3 15:03 wx2-windows64-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 19341618 May  3 15:04 wx2-windows64-gui-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio  3409796 May  3 15:03 wx2-windows-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 19298231 May  3 15:04 wx2-windows-gui-clients.tar.gz
-rw-rw-r--. 1 kognitio kognitio 10225145 May  3 15:03 wx2-windows-gui-clients-tng.tar.gz

This contains the self extracting installers (install-wx2-VVVV and linux-install-VVV.sfx), the upgrade package for deployment onto an existing system (the .wxpkg file) and various client tools and utilities.

Users already familiar with the WX2 software should be able to take these and turn them into a running system either by doing a fresh install or by upgrading an existing system.  If you have a running sytstem and want to switch version, you would usually do so like this:

$ wxserver install /full/path/to/wx2-80200rel200221-mybuild.wxpkg force
$ wxserver set current_version ver80200rel200221-mybuild
$ wxserver smd all restart force
$ wxserver start

The 'force' in the install command above allows you to overwrite an existing package with a new one (instead of getting the package installed error).  This is useful if you need to rebuild your source several times while developing code and replace the package each time without changing the version.

The second 'force' in the restart command is used to abandon whatever the SMD is doing and force a new one to start immediately.  This is useful for cases when you broke the boot process with a software change and now can't do a normal install or SMD restart because the broken boot process has not terminated yet.

Once the SMD restart command has been done you will see your new version string in the banner when running commands.  After the server restart completes you can run 'select version, patch_level from sys.ipe_system' to verify your custom build is running.  For example:

$ wxsubmit -s localhost sys -p syspassword -x "select version, patch_level from sys.ipe_system"
Kognitio WX2 SQL Submission Tool v8.02.00-rel200221-custom
(c)Copyright Kognitio Ltd 1992-2019.

Connected to localhost ODBC Version 8.02.00-rel200221-custom Server Version 08.02.0000
select version, patch_level from sys.ipe_system
    VERSION|PATCH_LEVEL
      80200|rel200221-mybuild
Query           1                1 row     ----   0:00.0   0:00.0   0:00.0
TotalQueries    1         Session Time   0:00.1     ----     ----     ----

To install from scratch on a fresh set of Kognitio nodes, transfer 'linux-install-80200rel200221-mybuild.sfx' onto each system and follow the standard Kognitio installation process.  This usually involves running the installer on one node without arguments and going theough the interactive install process.  This installs on one node and outputs a command line to run to get the same install done on the other nodes.  Then the stage 2 install process is started on any node.

Install instructions for this type of Kognitio install (we call this type of installation 'kognitio standalone' as oposed to kognitio on Hadoop, AWS or containerised Kognitio) can be found here:  https://kognitio.com/forum/Kognitio%20Software%20Installation.pdf


===== License key generation =====

By default Kognitio will operate without a license on systems with up to 512G of RAM.  For larger systems you will need to generate a license code before you install.  To do this you will first need to unpack the license key generator repository:

$ cd /kognitio
$ tar -C repo -xzf /tmp/bet365-lkgen.git.tar.gz 
$ git clone repo/bet365-lkgen.git lkgen

Then cd into lkgen and you can create keys.  We have pre-populated the license key database with all existing valid keys issued to Bet365.  There is a README and a .docx file in there to explain the full operation of the key generator and management tools, but the short version for creating a new key is to pick a system ID then do the following:

$ source setenv
$ create_system --company=Bet365 --name=rps-kog --descr="RPS System 20200610"
$ create_license --system rps-kog --license_type=base_and_ram_anydisk_noexpiry --count=1000
lkgen_V1 -s rps-kog -c 1000 -t /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry -n 50000003
Gave key: KIFM00yjPxpLAzIf4BSkUCApi8TxOs5hxkTynOFjT8M0LOGTWVgeYyv87hujy74qhYg6E7XXnM30!
GENERATED LICENSE:
License 50000003:
system : rps-kog
status : current
gen_command : lkgen_V1 -s rps-kog -c 1000 -t /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry -n 50000003
license_number : 50000003
key : KIFM00yjPxpLAzIf4BSkUCApi8TxOs5hxkTynOFjT8M0LOGTWVgeYyv87hujy74qhYg6E7XXnM30
typearg : /kognitio/lkgen/templates/base_and_ram_anydisk_noexpiry
company : Bet365
count : 1000
version : 1
license_type : base_and_ram_anydisk_noexpiry
system_descr : RPS System 20200610
company_descr : Bet365.com

LICENCE KEY KIFM00yjPxpLAzIf4BSkUCApi8TxOs5hxkTynOFjT8M0LOGTWVgeYyv87hujy74qhYg6E7XXnM30
Lic 50000003:  Num licences 1000.
           generated Wed Jun 10 10:13:14 2020.
           Type BASE.
   Licence allows:
           external_scripts yes
           external_tables yes
           new_system yes
           ram_node_gb 1000 (1 * 1000)
   Licence valid if:
           base_license yes
           license_version 1
           system_id rps-kog



COMPLETE.  LICENSE KEY IS:
KIFM00yjPxpLAzIf4BSkUCApi8TxOs5hxkTynOFjT8M0LOGTWVgeYyv87hujy74qhYg6E7XXnM30


Note that this package uses a pre-built version of the 'lkgen' command.  You can replace this with the one from the source tree if you do:

$ cd /kognitio/wx2
$ make -r out/GPLinux/bin/lkgen
  GCC OBJECT      GPLinux       lkgen.apps.o         liscram.c
  GCC OBJECT      GPLinux       lkgen.apps.o         lkgen.c
  AR              GPLinux       liblkgen.a
/kognitio/tools/compenv/V6p2/GPLinux/bin/i686-kognitio-linux-ar: creating out/GPLinux/obj/liblkgen.a
  RANLIB          GPLinux       liblkgen.a
  LD STATIC-APP   GPLinux       lkgen
$ cp out/GPLinux/bin/lkgen /kognitio/lkgen/bin/linux

The next time you generate a license key you will see the version string from your build in the banner.
```</t>
<t tx="jonathanhudson.20201006084250.173">@language md

```
These instructions enable you to build the Kognitio compile environment from source packages.  The compile environment largely consists of a set of GNU compiler tools and the associated toolchain and some runtime libraries.  This is the standard environment we compile Kognitio against to produce a single binary which works across a variety of Linux versions and distributions.

These instructions can be carried out in any location as any user.

== Setting up Linux ==

Start with a Centos 7.8 machine (which is what we tested these instructions on, other Linuxes should work too if you modify the instructions appropriately).  Add the packages required to build Kognitio:

$ sudo yum install make perl glibc.i686 zlib.i686 zip git m4 ctags-etags libstdc++.i686 


Now add the packages required to build the GNU toolchain:

$ sudo yum groups install 'Development Tools'


== Preparing the source ==

First, unpack the bare reposotry:

$ mkdir repo
$ tar C repo -xzf /tmp/kognitio-compenv-build.git.tar.gz

And create a working repository from it:

$ git clone repo/kognitio-compenv-build.git compenv-build


== building a compile environment ==

Now you have a directory called 'compenv-build' which contains the compile environment components.  There are two compile environments present, v6p2 and V6p3.  v6p2 is for building 8.2.0 while v6p3 is for building the more recent versions of the software including the 64 bit version.  

You will need to decide where you want to install the compile environment before you can build it.  In this example I am installing it into /kognitio/newtools but it can be installed into any path.

To build v6p2, cd into compenv-build/build-v6p2 and run this command from that directory:

BASE_DIR=/kognitio/newtools ./bin/make_cenv 

(where the BASE_DIR value is the location you want to install the tools into).  
This will build the GNU toolchain for multiple platforms inside that directory and create the whole compile environment.  Building the GNU tools is a long process with a lot of intermediate files.  By default these will also go into the target directory but you might want to have the intermediate files reside somewhere else (if the target is an NFS volume, for example).  In this case you can also specify TEMPDIR after BASE_DIR on the command line to put the intermediate files somewhere else, for example:


BASE_DIR=/kognitio/newtools TEMPDIR=/tmp ./bin/make_cenv 

These scripts were never really designed to be used externally and do not give a lot of feedback.  It will seem like nothing is happening, use 'top' to monitor progress.  if you look in the subdirectories of $BASE_DIR/tmp-Linux (or $TEMPDIR if you gave one) you will see the work in progress.  The 'log' subdirectory has the progress files.


== Setting the version ==

The compile environment build command above will create a new compile environment in a directory called /kognitio/newtools/v6p2 where v6p2 is the version being built.  If you want to make a compile environment with a different version you need to edit the build script being used.  There will be a line in the script which sets COMPENV_NAME and changing that will set the compile environment to a different version.

== Adding the extras ==

The kognitio compile environment also contains an 'extras' folder which contains additional assets that get shipped with the product and some other utilities used as part of the build process.  Most of these are only required for version 8.2.1 and onwards (this includes the 64 bit port).  They are generally just freely available runtime libraries which have been downloaded from the internet.  Examples include the Java JDK 1.8.0, Amazon.s aws Java API libraries, Google's JDK libraries and the standard Parquet library.  These have been installed in the extras directory by hand either by standard product installations or by using Maven to build components.  Unlike the full Kognitio build environment, these do not need modification to work in different locations.  One only needs to copy the files to another location.

To make a working Kognitio build the '/kognitio/tools/compenv/extras' folder needs to be copied into the root of the compile environment.  Using the above example, it would need to be copied into /kognitio/newtools.


== Using the new compile environment ==

Kognitio's build environment uses the 'scripts/find_compenv' script in order to locate the compile environment being used.  To make it pick up your environment from a different place, edit this script and wherever you see /kognitio/tools/compenv you need to change that to the location of the environment you are using (so in the above example, /kognitio/tools/compenv will change into /kognitio/newtools.   If you changed the version string of the compile environment, you will also need to edit the 'COMPENV:=' line in the Makefile in the root of the build to contain the new version string.


== Testing the new compile environment ==

The best way to test a new compile environment is to rebuild everything from scratch.  We do not recommend mixing object code made by different compile environments.  To remove all existing objects, run this in the WX2 build tree:

$ rm -rf out

Then, re-run the build with VERBOSE_BUILD turned on like this:

$ VERBOSE_BUILD=1 make -r makefiles
 (or whatever make command you want to use)

With verbose build turned on you will be able to see the full command issued to build each object.  You can use this to verify that the compilers being run do, in fact, come from the newly built compile environment.

```</t>
<t tx="jonathanhudson.20201006084250.174">@language md

```
Instructions for building Kognitio's QA tools and using them to run the QA test extract against a server

You will need a node similar to the build machine used for building the Kognitio software earlier.  Ideally this will be able to connect directly to a Kognitio server available for running the tests.  We recommend running the QA tests on a node with at least 4G RAM.

You will also need a test machine to run the tests against.  We recommend using a 2 node system where each node has at least 32G RAM, 4 CPUs, a 50G type-60 partition for Kognitio to use and a 10G network interface.  In their default configuration the QA tests will run over 200,000 individual transactions and at times there will be up to 20 sessions going in parallel.  Tests take around 3 hours to complete on our system with the above configuration.  

It is possible to run the tests against a smaller Kognitio instance but you will need to reduce the concurrency using the --max-streams option to qarunner (recommend 10 for a system 1/2 as big) and this will increase the runtime.  Some tests will fail unless there are 2 disk resources so you would need a second type 60 partition on the node.

It is also possible to run the tests directly on the Kognitio instance but this requires extra configuration of Kognitio to leave some memory free for the tests to run in.  Use the '[boot options]' fixed_pool_size setting to reduce memory used by Kognitio so there is an extra 4G of memory free for the tests.  fixed_pool_size of 25 works well on a lot of systems.


== Setting up Linux ==

These instructions assume the same build machine which was used to compile WX2 (see the 'wx2-build-instructions' file).  You will need to add the 'Development Tools' package group:

$ sudo yum groups install 'Development Tools'

And you will also need to install python 3.6 (any version of python3 will do) for the qarunner tool used to actually run tests.

$ sudo yum install python3

And build dependencies for clsql:

$ sudo yum install readline-devel unixODBC-devel


======= Building the QA tools =======

Use the 'kognitio' user created in the wx2-build-instructions file to build the kognitio QA tools.  First, transfer and unpack the bare repository

$ sudo su - kognitio
$ cd /kognitio
$ tar -C repo -xzf /tmp/kognitio-clsql.git.tar.gz 
$ git clone repo/kognitio-clsql.git qatools
$ cd qatools

Now you need to build the wxscript component and the binaries:

$ make -C wxscript -f wxsmake
$ make -C clsql -f clmake
 (note that the lib64 directory in this repo contains a binary kognitio ODBC driver.  This can be replaced with one built from the Kognitio WX2 tree if required).
$ make -C planrunner

And copy clsql and planrunner into /kognitio/tools/bin so the qarunner can find it:

$ mkdir -p /kognitio/tools/bin
$ cp clsql/clsql planrunner/planrunner /kognitio/tools/bin/

(planrunner is not actually used at the moment to run the test extract but you might need it for version 9 tests later or for other testing)

===========Using CLSQL============

clsql is a command line SQL client with testing extensions added to the SQL language it supports.  It is capable of running queries against a server then checking the results which come back,  To run clsql you would do something like this:

/kognitio/tools/clsql -s &lt;server&gt; -u &lt;user&gt; -p &lt;password&gt; -f /path/to/script

Where &lt;server&gt; is usually the DNS name or IP address of your running WX2 server.  use name:port or IP:port for servers running on a non-standard port.

clsql -h shows an option summary


==========Running the QA Extracts=============

First, extract the QA test repository and check it out:

$ cd /kognitio
$ tar -C repo -xzf /tmp/kognitio-qatests.git.tar.gz
$ git clone repo/kognitio-qatests.git qatests

Now, cd into qatests and you can run the tests from that directory using the qarunner command.  qarunner -h will show the options:

$ ./qarunner -h
usage: qarunner [-h] [--server SERVER] [--sys-password SYS_PASSWORD]
                [--clsql CLSQL] [--max-streams MAX_STREAMS]
                [--script-timeout SCRIPT_TIMEOUT] [--masterplan MASTERPLAN]
                [--test-user TEST_USER] [--test-password TEST_PASSWORD]
                [--without-setup] [--plans PLANS [PLANS ...]] [--list-plans]
                [--result-cache RESULT_CACHE] [--verbose] [--detail]
                [--script-root SCRIPT_ROOT] [--result-dir RESULT_DIR]
                [--cleanup-script CLEANUP_SCRIPT]

Toplevel QA test runner

optional arguments:
  -h, --help            show this help message and exit
  --server SERVER       server to contact
  --sys-password SYS_PASSWORD
                        Password for sys user
  --clsql CLSQL         Path to the clsql command
  --max-streams MAX_STREAMS
                        Max number of concurrent scripts to run
  --script-timeout SCRIPT_TIMEOUT
                        Timeout for running a script
  --masterplan MASTERPLAN
                        Name of the masterplan file
  --test-user TEST_USER
                        User to connect with for non-sys scripts
  --test-password TEST_PASSWORD
                        Password for test user
  --without-setup       Skip the initial setup script
  --plans PLANS [PLANS ...]
                        Select one or more plans to run
  --list-plans          Show a list of plans available and exit
  --result-cache RESULT_CACHE
                        Cache results to allow resumption
  --verbose, -v         Show all test activity on stdout
  --detail              Show test script start/end on stdout
  --script-root SCRIPT_ROOT
                        Directory containing the script tree
  --result-dir RESULT_DIR
                        Output Directory
  --cleanup-script CLEANUP_SCRIPT
                        Location of script to cleanup a user's schema

If you just want to do a full test run on a suitable server, a command like this will do that:

$ ./qarunner --server &lt;servername&gt; --sys-password &lt;syspassword&gt; 

Where &lt;servername&gt; and &lt;syspassword&gt; are for the machine the tests will run on.  The tests assume full use of the machine and that the machine has just been initialised with a 'new system'.  Re-running the tests without resetting the machine with another new system might result in test errors.  The tests will create a number of users which, by default, will use the sys password.  You can specify a different one with --test-password.

A test run will look something like this (the deliberate mistake is inserted to illustrate what a fail would look like!):

$ ./qarunner --server andybqa.dv --test-password foobar --sys-password syspassword
Creating output directory results-andybqa.dv
Running Setup scripts
scripts to run: 
Complete: 1, Bad: 0, Passes: 212, Fails 0

Running SysQueries scripts
Complete: 304, Bad: 0, Passes: 7056, Fails 0

Running GeneralTests scripts
Complete: 2028, Bad: 0, Passes: 101027, Fails 0
FAILED: {'Script': 'server/nonTopLevelOrderBy/nonTopLevelOrderBy_002.wxs', 'ngood': 6, 'nbad': 4, 'good': False, 'bad': True, 'timeout': False, 'error': False, 'sno': 14}
Complete: 2497, Bad: 1, Passes: 291323, Fails 4

Running CDMTests scripts
Complete: 470, Bad: 0, Passes: 17088, Fails 0

Running Systable scripts
Complete: 4, Bad: 0, Passes: 70, Fails 0

Running MergeInto scripts
Complete: 9, Bad: 0, Passes: 10926, Fails 0

Running AlterTable scripts
Complete: 8, Bad: 0, Passes: 192, Fails 0

Running LongTests scripts
Complete: 101, Bad: 0, Passes: 31717, Fails 0

Running VeryLongTests scripts
Complete: 6, Bad: 0, Passes: 7266, Fails 0
All Plans Complete: 3400, Bad: 1, Passes: 365850, Fails 4, Duration 9401s

The test run will create a results directory called 'results-&lt;server.name&gt;' (results-andybqa.dv in my case).  The 'results' file contains a result summary similar to the above and the 'log' file has a more detailed description.  There are also session logs for each test and a detailed script log for each script run in 'logs/&lt;script&gt;.log'.  So for the above error you could open 'results-andybqa.dv/logs/server/nonTopLevelOrderBy/nonTopLevelOrderBy_002.wxs.log' and search for 'FAIL' to see what went wrong.  
```</t>
<t tx="jonathanhudson.20201006084250.175">@language md

#### Notes

```
./qarunner --server ab1kgwtmp0001p0 --sys-password albatros

./qarunner --server ab1kgwtmp0001p0 --sys-password albatros --max-streams 1
```</t>
<t tx="jonathanhudson.20201006084250.176"></t>
<t tx="jonathanhudson.20201006084250.177"></t>
<t tx="jonathanhudson.20201006084250.178">cd /opt/sophos-av/bin

./savdctl disable

./savdstatus

</t>
<t tx="jonathanhudson.20201006084250.179">0 8 * * 6 test $((10#$(date +\%W)\%2)) -eq 1 &amp;&amp; yourCommand

date +%W: week number of year with Monday as first day of week, today week 39

10#$(date +%W): conver the date +W to decimal number and avoid shell base parsing confusion

$((39%2)): modulo operation: result is 0 (even week number) or 1 (odd week number), this week result is 1, next week 0

test 1 -eq 1: arithmetic test (equal), in this case result is boolean true

&amp;&amp; yourCommand: Boolean AND: run yourCommand only if result of previous command was boolean true

Note that the year can get two odd weeks: 53 (this year) and 1 (next year)







0 4 * * 6
</t>
<t tx="jonathanhudson.20201006084250.18" __bookmarks="7d7100580700000069735f6475706571014930300a732e"></t>
<t tx="jonathanhudson.20201006084250.180">

timedatectl status
timedatectl set-ntp true

service ntpd stop
date -s "Aug 11 2018"
.....
service ntpd start


grep ^server /etc/ntp.conf


ntpdate -s 10.4.254.250


service ntpd stop
ntpd -gq
service ntpd start



# Heck to reset date
</t>
<t tx="jonathanhudson.20201006084250.181">In GNOME and other freedesktop.org-compliant desktop environments, such as KDE and Unity, applications are added to the desktop's menus or desktop shell via desktop entries, defined in text files with the .desktop extension (referred to as desktop files). The desktop environments construct menus for a user from the combined information extracted from available desktop entries.

Desktop files may be created in either of two places:

/usr/share/applications/ for desktop entries available to every user in the system
~/.local/share/applications/ for desktop entries available to a single user
You might need to restart GNOME for the new added applications to work.

Per convention, desktop files should not include spaces or international characters in their name.

Each desktop file is split into groups, each starting with the group header in square brackets ([]). Each section contains a number of key, value pairs, separated by an equal sign (=).

Below is a sample of desktop file:

[Desktop Entry]
Type=Application
Encoding=UTF-8
Name=Application Name
Comment=Application description
Icon=/path/to/icon.xpm
Exec=/path/to/application/executable
Terminal=false
Categories=Tags;Describing;Application
Explanation

[Desktop Entry] the Desktop Entry group header identifies the file as a desktop entry
Type the type of the entry, valid values are Application, Link and Directory
Encoding the character encoding of the desktop file
Name the application name visible in menus or launchers
Comment a description of the application used in tooltips
Icon the icon shown for the application in menus or launchers
Exec the command that is used to start the application from a shell.
Terminal whether the application should be run in a terminal, valid values are true or false
Categories semi-colon (;) separated list of menu categories in which the entry should be shown
Command line arguments in the Exec key can be signified with the following variables:

%f a single filename.
%F multiple filenames.
%u a single URL.
%U multiple URLs.
%d a single directory. Used in conjunction with %f to locate a file.
%D multiple directories. Used in conjunction with %F to locate files.
%n a single filename without a path.
%N multiple filenames without paths.
%k a URI or local filename of the location of the desktop file.
%v the name of the Device entry.
Note that ~ or environmental variables like $HOME are not expanded within desktop files, so any executables referenced must either be in the $PATH or referenced via their absolute path.

A full Desktop Entry Specification is available at the GNOME Dev Center.

Launch Scripts

If the application to be launched requires certain steps to be done prior to be invoked, you can create a shell script which launches the application, and point the desktop entry to the shell script. Suppose that an application requires to be run from a certain current working directory. Create a launch script in a suitable to location (~/bin/ for instance). The script might look something like the following:

#!/bin/bash
pushd "/path/to/application/directory"
./application "$@"
popd
Set the executable bit for the script:

$ chmod +x ~/bin/launch-application
Then point the Exec key in the desktop entry to the launch script:

Exec=/home/user/bin/launch-application
</t>
<t tx="jonathanhudson.20201006084250.182">Enable Services

systemd
=======

systemctl is-enabled httpd; echo $?
systemctl is-enabled nc_hardserver; echo $?

systemctl enable httpd

initd
=====

chkconfig --list
chkconfig nfs on

</t>
<t tx="jonathanhudson.20201006084250.183">Gnome crashes

killall -3 gnome-shell
killall -3 gnome-putty
</t>
<t tx="jonathanhudson.20201006084250.184">

htpasswd -b -p /etc/httpd/ro-test test $(openssl passwd -1  )



htpasswd -b -p /tmp test $(openssl passwd -1  )


N@pYUvWFih03



htpasswd -b -p ./test.txt prd-bg-test $(openssl passwd -1 )


prd-bg-test:$1$dHIOPwEI$y6Ru.CAyQWOI2S.xgIZLW.</t>
<t tx="jonathanhudson.20201006084250.185">su - d3s-statsrv
mkdir /data/home/d3s-statsrv/logs/old

exit

cd /etc/logrotate.d/
vi d3s-catalina

/data/home/d3s-statsrv/logs/catalina.out {
    su d3s-statsrv d3s
    weekly
    copytruncate
    compress
    dateext
    rotate 365
    olddir /data/home/d3s-statsrv/logs/old
    notifempty
    missingok
}


logrotate -f /etc/logrotate.d/d3s-catalina



check 
cd /etc/cron.daily/



logrotate -f /etc/logrotate.d/tomcat
</t>
<t tx="jonathanhudson.20201006084250.186">find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -i "&lt;RegistroId" | wc -l' \; -print

# Show record ids
find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print


# regular expression ( 3 characters before and 4 chars after 
-P '.{0,3}string.{0,4}'

id -u jonathanhudson
247320266

id -g jonathanhudson
247200513


mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Junior\ Regulatory\ Platform\ Services\ Developer /mnt/junior
</t>
<t tx="jonathanhudson.20201006084250.187">id -u jonathanhudson
247320266

id -g jonathanhudson
247200513


mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Regulatory\ Platform\ Services\ Technical\ Lead /mnt/windows
</t>
<t tx="jonathanhudson.20201006084250.189">

tar cf - . | (cd /dst; tar xvf -)


cd /mnt/emergency/d3s-dk/
tar cf - tampertokens/. | (cd /tmp/tar; tar xvf -)

</t>
<t tx="jonathanhudson.20201006084250.19"></t>
<t tx="jonathanhudson.20201006084250.190"># Export NFS Folder

# Server

yum install nfs-utils nfs-utils-lib rpcbind -y

chkconfig --levels 235 nfs on 
chkconfig --level 35 rpcbind on

service rpcbind start
service nfs start

rpcinfo -p

vi /etc/exports

/data/home/d3s-dk/d3s/catalina/logs     10.15.69.183/255.255.255.0(ro,sync,no_root_squash,no_subtree_check)

service nfs restart

exportfs -a

showmount -e

# Client

yum install nfs-utils nfs-utils-lib -y

mkdir -p /mnt/NFS/d3s-cap-1-logs
mkdir -p /mnt/NFS/d3s-cap-2-logs

showmount -e 10.15.69.183

nfs_server="10.15.69.183"
mount ${nfs_server}:/data/home/d3s-dk/rd_deliveries/dictao-d3s-jel-setup-server-5.0-35.1-SNAPSHOT/catalina/logs  /mnt/NFS/d3s-cap-1-logs

nfs_server="10.15.69.184"
mount ${nfs_server}:/data/home/d3s-dk/rd_deliveries/dictao-d3s-jel-setup-server-5.0-35.1-SNAPSHOT/catalina/logs  /mnt/NFS/d3s-cap-2-logs
</t>
<t tx="jonathanhudson.20201006084250.191">
Process start time

ls -ld /proc/xyx

dr-xr-xr-x 9 d3s-bg d3s 0 Oct  5 06:26 /proc/30130</t>
<t tx="jonathanhudson.20201006084250.192">subscription-manager register --org="bet365" --activationkey="ls-2017-10" </t>
<t tx="jonathanhudson.20201006084250.193">rename u-bg- p-bg- *

rename 1007_10071 1013_5510131 *.zip

# Recursively 

find /mnt/SAFE_DATA/d3s-es/storage/b365/CNJ/1013/ -type f -name '*.zip' -execdir rename 20210107 20210106 {} \;</t>
<t tx="jonathanhudson.20201006084250.194">@language md
```
ssh user@remote "find type -f /xxx/yyy" | diff - /path/to/local/file
```
### Read Password
```
read -p "Password: " -s SSHPASS # *MUST* be SSHPASS
export SSHPASS

sshpass -e ssh jonathanhudson@MN2REGCAP0001D1 free -m | grep "Mem:" | awk '{ print "Total memory (used+free): " $3 " + " $4 " = " $2 }'
```


### Create a file
```
date
hostname
cat /etc/resolv.conf
```
### then -t ignores error
```
cat remote-box-commands.bash | sshpass -e ssh -t jonathanhudson@MN2REGCAP0001D1
```
###
```
ssh vivek@www1.cyberciti.biz 'free -m' &gt; /tmp/memory.status
ssh root@www1.cyberciti.biz '/etc/init.d/mysql restart'
ssh root@www1.cyberciti.biz reboot 
ssh vivek@www1.cyberciti.biz df -h


ssh operator@oracle1  '/scripts/backup.sh'
```
### sudo syntax ##
```
ssh -t user@hostname sudo command
ssh -t user@hostname 'sudo command1 arg1 arg2'
```
### su syntax ##
```
ssh user@nas01 su -c "/path/to/command1 arg1 arg2"
```
### RHEL/CentOS specific
```
ssh user@nas01 su --session-command="/path/to/command1 arg1 arg2"

ssh vivek@nixcraft.home.server su --session-command="/sbin/service httpd restart"
```</t>
<t tx="jonathanhudson.20201006084250.195">[root@OB1-001122 log]# less secure
[root@OB1-001122 log]# less messages
[root@OB1-001122 log]# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      28
[root@OB1-001122 log]# setenforce 0
[root@OB1-001122 log]#
[root@OB1-001122 log]# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   permissive
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      28



</t>
<t tx="jonathanhudson.20201006084250.196">@nocolor
write inf_gdougan pts/0
Thanks looks ok now. We have the same problem on bg2rrpush001p2 and so it also needs a reboot
^D
</t>
<t tx="jonathanhudson.20201006084250.197">@language md
## SSH Keys

### Create private public key

From mn2regcap0002u1

```
ssh-keygen -t rsa

ssh-copy-id -i $HOME/.ssh/id_rsa.pub jonathanhudson@mn2regcap0002u2
```

### In case no .ssh on remote

First create .ssh directory on server
cat local id.rsa.pub file and pipe over ssh to append the public key in remote server

```
ssh jonathanhudson@mn2regcap0002u2 "umask 077; test -d .ssh || mkdir .ssh"
 

cat $HOME/.ssh/id_rsa.pub | ssh jonathanhudson@mn2regcap0002u2 "cat &gt;&gt; .ssh/authorized_keys"
```

### Permisions

home folder should have 755
.ssh folder should have 600


authorized_keys
config
	Host mn2regrep0001d2
		IdentityFile ~/.ssh/id_mn2regrep0001d1
known_hosts
id_mn2regrep0001d1
id_mn2regrep0001d1.pub

### Example d3s rep to push

#### On UAT Rep 1
```
file=~/.ssh/id_mn2regrep0001u1
ssh-keygen -t rsa -f ${file}
```
#### Copy public key to authorized_keys folder on UAT Push Cap 1
```
authorized_keys ( 600 )
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDNOEUEKiOgoGj96AlRaX8USJwWSKzu9UvBZX3EAXuoCgfaqTM7CDNwUZzivmfRnsvQefndrZGSYKdBqkV3dzMzs2pTHObTV6TqJuV5dCjLuopN/l+pK5ZnK5DkMFg4b5u+AG10VQWpXEbxC9KJZdRzi5I0AV3V7SFzXnY0DekF72eay+gVGAc9fSaY34D3jlcU4iaRYvk0PvEsf9j9wKekoMx4z55gJ6qr0Eu2gIkq0yq7D5atpDuVevTFsf/dLCIQ04y91skEyBfvTfcYSkocoTtbImH2zFO6dheeDf2lhjizwJ+jUbkP5eS47M253pKLbOem8BzTJkThqHazRDYJ d3s-dk@mn2regcap0001u1.uk365office.co.uk

config
Host mn2regcap0001d1
   IdentityFile ~/.ssh/id_mn2regcap0001u1
```
#### Ensure permissions on home folder
```
chmod 755 /data/home/d3s-dk   
```
#### On UAT Cap 2
```
file=~/.ssh/id_mn2regcap0001u2
ssh-keygen -t rsa -f ${file}
```
#### Copy public key to authorized_keys folder on PRD Cap 1
```
authorized_keys ( 600 )
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDfOpLiLJaZFQh4tRFGPflV5IKTvuwTbqtC9SYPs1eIwsdG8r9MUMmAj1NphMkRZ8Rpfz+ZjHSVq3dInKqaLKgu3DDpYWcoIYTEWOrfWTcytrk7l8A3hPIPS5w2FgPRcZPX4O1bKszU3HZgMt5whV2L4WPIce54w/XU2es6FwG1o8n9pkLIwAn5H/nSZETxoELBYzhj7BDcTopYwfsbdv2VuioUVGEo0/D1l+D1C16WTvbxoWpr4JfN08x14ERafaEATAoTBLXT2kNj9flksqLRyfMABZb5Aaoi6VOLsAD9mqbX7IbKrIj4lH7BDWgFbcBPWQXFKQVo0zGF4S92f9kB d3s-dk@mn2regcap0001u2.uk365office.co.uk

config ( 600 )
Host mn2regcap0001d2
   IdentityFile ~/.ssh/id_mn2regcap0001u2
```
#### Ensure permissions on home folder
```
chmod 755 /data/home/d3s-dk   
```

</t>
<t tx="jonathanhudson.20201006084250.198">press Ctrl-Z to suspend the script

kill %%

The %% tells the bash built-in kill that you want to send a signal (SIGTERM by default) to the most recently suspended background job in the current shell, not to a process-id.

You can also specify jobs by number or by name. e.g. when you suspend a job with ^Z, bash will tell you what its job number is with something like [n]+  Stopped, where the n inside the square brackets is the job number.

For more info on job control and on killing jobs, run help jobs, help fg, help bg, and help kill in bash, and search for JOB CONTROL (all caps) or jobspec in the bash man page.
</t>
<t tx="jonathanhudson.20201006084250.199">Mask	Files
(requested permissions 666)	Directories
(requested permissions 777)
000	666 (rw-rw-rw-)	777 (rwxrwxrwx)
002	664 (rw-rw-r--)	775 (rwxrwxr-x)
007	660 (rw-rw----)	770 (rwxrwx---)
022	644 (rw-r--r--)	755 (rwxr-xr-x)
027	640 (rw-r-----)	750 (rwxr-x---)
077	600 (rw-------)	700 (rwx------)
277	400 (r--------)	500 (r-x------)
</t>
<t tx="jonathanhudson.20201006084250.2">@language md
#### Docks

To initialise the Docks
Add the `--init-docks` startup parameter which corrects the "Render" tab location
</t>
<t tx="jonathanhudson.20201006084250.20"></t>
<t tx="jonathanhudson.20201006084250.200"># Setting Display for jvisualvm running from d3s-es

From root

xhost +

( That's it )

uname -a
env | grep -e DISPLAY -e AUTHORITY
loginctl show-session $XDG_SESSION_ID

su - d3s-es

export HOST=`hostname -s`
export DISPLAY=${HOST}:0
export DISPLAY=10.224.36.38:0

XAUTHORITY=/run/gdm/auth-for-jonathanhudson-PTTVTZ/database

touch ~/.Xauthority
xauth generate :0 . trusted
xauth add ${HOST}:0 . $(xxd -l 16 -p /dev/urandom)
xauth add 10.224.36.38:0 . $(xxd -l 16 -p /dev/urandom)
xauth add :0.0 . $(xxd -l 16 -p /dev/urandom)
xauth add ":0" . $(xxd -l 16 -p /dev/urandom)
xauth list

xauth list|grep `uname -n`

xauth add $DISPLAY . hexkey

xauth add $DISPLAY . 961da15e9a3f50c332ce9b6d2cf0f2e1
xauth add $DISPLAY . 3d9192a06af1225fba8e7a9ab1d3f243
</t>
<t tx="jonathanhudson.20201006084250.201">echo 7Server &gt; /etc/yum/vars/releasever

yum clean all 
rm -rf /var/cache/yum/* 
yum repolist 
yum update
</t>
<t tx="jonathanhudson.20201006084250.202"></t>
<t tx="jonathanhudson.20201006084250.203">
# Change password
keytool -importkeystore -srckeystore prd-d3s-client-test.uk365office.co.uk.p12 -srcstoretype PKCS12 -srcstorepass p29548tn01ygq -destkeystore prd-d3s-client-test.p12 -deststoretype PKCS12 -deststorepass password -destkeypass password

# Explode certificate
keystore="d2s-client-keystore"
passfile=${keystore}.p12.password
 
openssl pkcs12 -in ${keystore}.p12 -out ${keystore}.pem -clcerts -nokeys -passin file:${passfile} -passout pass:TemporaryPassword
openssl pkcs12 -in ${keystore}.p12 -out ${keystore}.key -nocerts  -passin file:${passfile} -passout pass:TemporaryPassword
openssl pkcs12 -export -in ${keystore}.crt -name ${keystore} -out ${keystore}.p12 -inkey ${keystore}.key  -passin file:${passfile} -passout pass:TemporaryPassword
 
# To create a decrypted key (without passphrase)
openssl rsa -in ${keystore}.key -out ${keystore}_decrypted.key -passin pass:TemporaryPassword

# Export cert from truststore
keytool -export -alias dss-signature -keystore dev-client-truststore.jks -rfc -file dss-trust.cer

</t>
<t tx="jonathanhudson.20201006084250.204">```
openssl genpkey -algorithm RSA -out private_key.pem -pkeyopt rsa_keygen_bits:2048
```
Now extract the **public** key from the **private key**

```
openssl rsa -pubout -in private_key.pem -out public_key.pem
```
And to view the key components
```
openssl rsa -text -in private_key.pem
```

</t>
<t tx="jonathanhudson.20201006084250.205">```
$ openssl genrsa -out operator.key 2048
Generating RSA private key, 2048 bit long modulus
.................................................................................................+++
......+++
e is 65537 (0x10001)
```
</t>
<t tx="jonathanhudson.20201006084250.206">```
$ openssl req -new -key operator.key -out operator.csr

You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:UK
State or Province Name (full name) [Some-State]:Staffs
Locality Name (eg, city) []:Stoke
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Bets'R'Us
Organizational Unit Name (eg, section) []:Regulator
Common Name (e.g. server FQDN or YOUR name) []:Jono Hudson
Email Address []:jonathan.hudson@bet365.com

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:secretpassword
An optional company name []:
```
</t>
<t tx="jonathanhudson.20201006084250.207">@language shell
keystore="d2s-client-keystore"
 
passfile=${keystore}.p12.password
 
#openssl pkcs12 -in ${keystore}.p12 -out ${keystore}.ca -cacerts -nokeys 
openssl pkcs12 -in ${keystore}.p12 -out ${keystore}.pem -clcerts -nokeys -passin file:${passfile} -passout pass:TemporaryPassword
openssl pkcs12 -in ${keystore}.p12 -out ${keystore}.key -nocerts  -passin file:${passfile} -passout pass:TemporaryPassword
#openssl pkcs12 -export -in ${keystore}.crt -name ${keystore} -out ${keystore}.p12 -inkey ${keystore}.key  -passin file:${passfile} -passout pass:TemporaryPassword
 
# To create a decrypted key (without passphrase)
openssl rsa -in ${keystore}.key -out ${keystore}_decrypted.key -passin pass:TemporaryPassword</t>
<t tx="jonathanhudson.20201006084250.208">```
$ openssl genrsa -out rootCA.key 2048
Generating RSA private key, 2048 bit long modulus
..........+++
...............................................................................+++
e is 65537 (0x10001)
```
</t>
<t tx="jonathanhudson.20201006084250.209">```
$ openssl genrsa -des3 -out rootCA.key 2048
```
You will then be prompted for a password

</t>
<t tx="jonathanhudson.20201006084250.21"></t>
<t tx="jonathanhudson.20201006084250.210">@nocolor

 openssl s_client -connect Datatransfer.dkh.minfin.bg:443 -servername Datatransfer.dkh.minfin.bg
 
 1 s:/street=2 P. Volov Str./CN=StampIT Qualified CA/O=Information Services Plc./L=Sofia/ST=B:831641791/C=BG
   i:/street=2 P. Volov Str./CN=StampIT Primary Root CA/O=Information Services Plc./L=Sofia/ST=B:831641791/C=BG
 
 
 
  Issuer: street=2 P. Volov Str., CN=StampIT Primary Root CA, O=Information Services Plc., L=Sofia, ST=B:831641791, C=BG
  
  Issuer: C=BG, ST=B:831641791, L=Sofia, O=Information Services Plc., CN=StampIT Primary Root CA, STREET=2 P. Volov Str.</t>
<t tx="jonathanhudson.20201006084250.211">@nocolor
$ openssl genrsa -out rootCA.key 2048
Generating RSA private key, 2048 bit long modulus
..........+++
...............................................................................+++
e is 65537 (0x10001)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

$ openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem

You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:UK
State or Province Name (full name) [Some-State]:Staffordshire
Locality Name (eg, city) []:Stoke-On-Trent
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Bet365
Organizational Unit Name (eg, section) []:RPS
Common Name (e.g. server FQDN or YOUR name) []:Jonathan Hudson
Email Address []:jonathan.hudson@bet365.com

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

$ openssl req -new -key operator.key -out operator.csr

You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:UK
State or Province Name (full name) [Some-State]:Staffs
Locality Name (eg, city) []:Stoke
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Bets'R'Us
Organizational Unit Name (eg, section) []:Regulator
Common Name (e.g. server FQDN or YOUR name) []:Jono Hudson
Email Address []:jonathan.hudson@bet365.com

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:secretpassword
An optional company name []:


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------





openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 500 -sha256


</t>
<t tx="jonathanhudson.20201006084250.212">@language shell
openssl rsa -noout -modulus -in sig-policy-1
openssl req -noout -modulus -in sig-policy-1_req
openssl x509 -noout -modulus -in sig-policy-1.cer


Importing private keys into a Java keystore using keytool

openssl verify -verbose -CAfile ca.crt.pem  sig-policy-1.cer


openssl pkcs12 -export -in sig-policy-1.cer -inkey sig-policy-1 -out uat-hsm-keystore.p12 -name sig-policy-1 -CAfile ca.crt.pem -caname rps-ca

openssl pkcs12 -export -in sig-policy-1.cer -inkey sig-policy-1 -out uat-hsm-keystore.pfx -certfile ca.crt.pem  

keytool -importkeystore -deststorepass password -destkeypass password -destkeystore uat-hsm-keystore.jks -srckeystore cert-and-key.p12 -srcstoretype PKCS12 -srcstorepass cert-and-key-password -alias 1





openssl pkcs12 -export -in [my_certificate.crt] -inkey [my_key.key] -out [keystore.p12] -name [new_alias] -CAfile [my_ca_bundle.crt] -caname root


keytool -importkeystore -deststorepass [new_keystore_pass] -destkeypass [new_key_pass] -destkeystore [keystore.jks] -srckeystore [keystore.p12] -srcstoretype PKCS12 -srcstorepass [pass_used_in_p12_keystore] -alias [alias_used_in_p12_keystore]




openssl pkcs12 -export -in server.crt -inkey server.key -out server.p12 -name [some-alias] -CAfile ca.crt -caname root
			   
keytool -importkeystore -deststorepass [changeit] -destkeypass [changeit] -destkeystore server.keystore -srckeystore server.p12 -srcstoretype PKCS12 -srcstorepass some-password -alias [some-alias]</t>
<t tx="jonathanhudson.20201006084250.213">@nocolor
openssl s_client -connect mn2-uat-regsaferproxybg.lb.local:443  | openssl x509 -noout -text | grep DNS:</t>
<t tx="jonathanhudson.20201006084250.214">@nocolor
keytool -importkeystore -srckeystore demo-gaming-site1-client.p12 -destkeystore client.jks -srcstoretype PKCS12 -deststoretype JKS -srcstorepass password -deststorepass password -noprompt

openssl pkcs12 -in demo-gaming-site1-client.p12 -clcerts -nokeys -out demo-gaming-site1-client.pem
openssl x509 -noout -text -in demo-gaming-site1-client.pem
keytool -list -keystore truststore.jks

keytool -exportcert -alias anysign -keystore truststore.jks

openssl genrsa -out rootCA.key 2048

openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem

keytool -importcert -keystore jono-truststore.jks -file rootCA.pem

keytool -list -keystore  jono-truststore.jks
</t>
<t tx="jonathanhudson.20201006084250.215">@nocolor


/usr/bin/openssl s_client -connect mn2rproxy0001u1.uk365office.co.uk:443 -cert dev-d3s-client-games.crt.pem -key d3s-client.1.key -CAfile client_ca_bundle.pem -debug -state -tls1_2


## CURL with fail if you don't use the exact same URL that is in the certificate (it is fussy)

curl --tlsv1 --max-time 20 -v --cert ./uat-d3s-client-games.crt.pem --key ./d3s-client-key.pem --cacert ./server_ca_bundle.pem -X POST --upload-file ./echo.req https://mn2rproxy0001u1.uk365office.co.uk:443/d3s/jel/dk/service/echo/v2u1


openssl s_client -connect mn2rproxy0001u1.uk365office.co.uk:443 -cert uat-d3s-client-games.crt.pem -key d3s-client-key.pem -CAfile client_ca_bundle.pem -debug -state -tls1_2





curl --tlsv1 --max-time 20 -v --cert ./dev-d3s-client-sports.pem --key ./dev-d3s-client-sports_decrypted.key --cacert ./server_ca_bundle.pem -X POST --upload-file ./echo.req https://regulatorsafe-rproxy-v1.b365dev.com:443/d3s/jel/dk/service/echo/v2u1



curl http://localhost:25080/activator/state?force=up -v





curl --tlsv1 --max-time 20 -v --cert ./dev-d3s-client-sports.pem --key ./dev-d3s-client-sports_decrypted.key --cacert ./server_ca_bundle.pem -X POST --upload-file ./echo.req https://regulatorsafe-rproxy-v1.b365dev.com:443/d3s/jel/dk/service/echo/v2u1



curl --tlsv1 --max-time 20 -v --cert ./dev-d3s-client-sports.pem --key ./dev-d3s-client-sports_decrypted.key --cacert ./server_ca_bundle.pem -X POST --upload-file ./echo.req https://regulatorsafe-rproxy-v1.b365dev.com:443/d3s/jel/dk/service/echo/v2u1


curl --tlsv1 --max-time 20 -v --cert ./mis-sports.cer.pem --key ./mis_decrypted.key --cacert ./server_ca_bundle.pem -X POST --upload-file ./echo-dk.req https://rpuat.b365uat.com:443/d3s/jel/dk/service/echo/v2u1</t>
<t tx="jonathanhudson.20201006084250.216">```
$ openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem

You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:UK
State or Province Name (full name) [Some-State]:Staffordshire
Locality Name (eg, city) []:Stoke-On-Trent
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Bet365
Organizational Unit Name (eg, section) []:RPS
Common Name (e.g. server FQDN or YOUR name) []:Jonathan Hudson
Email Address []:jonathan.hudson@bet365.com
```
</t>
<t tx="jonathanhudson.20201006084250.217">```
openssl x509 -req -in operator.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out operator.crt -days 500 -sha256

Signature ok
subject=/C=UK/ST=Staffs/L=Stoke/O=Bets'R'Us/OU=Regulator/CN=Jono Hudson/emailAddress=jonathan.hudson@bet365.com
Getting CA Private Key
```
</t>
<t tx="jonathanhudson.20201006084250.218">@language shell
-----BEGIN CERTIFICATE-----
MIIEfzCCA2egAwIBAgIKQ4F0cgAAAAAGtTANBgkqhkiG9w0BAQUFADB2MQswCQYD
VQQGEwJGUjEOMAwGA1UEBxMFUGFyaXMxDzANBgNVBAoTBkRpY3RhbzEXMBUGA1UE
CxMOMDAwMiA0MjkzODM5NzkxLTArBgNVBAMTJERpY3RhbyBUcnVzdCBTZXJ2aWNl
cyBBcHBsaWNhdGlvbiBDQTAeFw0xNDA5MTkwNzEyMDBaFw0xNzA5MTgwNzEyMDBa
MG0xCzAJBgNVBAYTAkZSMQ4wDAYDVQQHEwVQYXJpczEPMA0GA1UEChMGRGljdGFv
MRcwFQYDVQQLEw4wMDAyIDM5NzQ5MTE4NDEkMCIGA1UEAxMbU0FBUyBRQSBEM1Mg
QjM2NUhOTUwgQ2xpZW50MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
h3sKFz8fBn6ruW1t8J/ONinAGj5VYyWIGiri3L8zgVxLsbU1i7VIq+YB7EszvZ74
sy85Ibd0Cpqy63ACeEokVNAK0f9/ObnlmdzyjQdc1Lkv34XAOQEKgpHhn6Dmyugp
PqV2ONKEMWsDozcl298DHplVvCwqoSDlny1E/cpreDb4N98KfHeiEIH/TmYSH+YG
S5Knkb67H/IqK4eheoL/LcTmDF25KRUmerjocUQuU6JI7MRWxwyDelRDPRbA0rCb
TkEJoWdLrdMFXwzxWpM52edz8CMXIT1yOjLQmMyxef7hu+YON5jI2NSniTrX+KpL
9V5zppH9gZezz2htj2LhUQIDAQABo4IBFjCCARIwDgYDVR0PAQH/BAQDAgeAMBMG
A1UdJQQMMAoGCCsGAQUFBwMCMB0GA1UdDgQWBBREaWznjjwYyozOAnFaBJ2Pj22Y
izAfBgNVHSMEGDAWgBSV60cZ0RiG+83KzI2EvSKHWEaNYjBPBgNVHR8ESDBGMESg
QqBAhj5odHRwOi8vaWdjLmRpY3Rhby5jb20vZGljdGFvLXRydXN0LXNlcnZpY2Vz
LWFwcGxpY2F0aW9uLWNhLmNybDBaBggrBgEFBQcBAQROMEwwSgYIKwYBBQUHMAKG
Pmh0dHA6Ly9pZ2MuZGljdGFvLmNvbS9kaWN0YW8tdHJ1c3Qtc2VydmljZXMtYXBw
bGljYXRpb24tY2EuY3J0MA0GCSqGSIb3DQEBBQUAA4IBAQA27ompavLus8A9EQwp
oAneteXHtZHteMNZCoQyAYI2QqaitPIMQVMzzIGeAzyvze7q+Hl6DCXNYJc/vUzf
ZLSqsrOEmGB1Tuza8h5oHkBmTOekNCsLvl7v/qS9nuXo3qEc9fQy3ltfBgJQPYAV
CVzt4p0ImUv125krNM48M9J4IZ1h688Dl1BYhMGynNx2spj1weAPYdrzdmN94rLP
N8DCg3pLhML+7tLrlCSVOf581f1d6D69uJSrVgc4lOByjcq3mJ41ae2j1SYbhUE7
J7WKepdw6bicsg2ZivEAPlhU4i16+IoQ+ygj/lG0alSMdeiQOJxEu0nX3WL3JW7k
9ttx
-----END CERTIFICATE-----


-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEAh3sKFz8fBn6ruW1t8J/ONinAGj5VYyWIGiri3L8zgVxLsbU1
i7VIq+YB7EszvZ74sy85Ibd0Cpqy63ACeEokVNAK0f9/ObnlmdzyjQdc1Lkv34XA
OQEKgpHhn6DmyugpPqV2ONKEMWsDozcl298DHplVvCwqoSDlny1E/cpreDb4N98K
fHeiEIH/TmYSH+YGS5Knkb67H/IqK4eheoL/LcTmDF25KRUmerjocUQuU6JI7MRW
xwyDelRDPRbA0rCbTkEJoWdLrdMFXwzxWpM52edz8CMXIT1yOjLQmMyxef7hu+YO
N5jI2NSniTrX+KpL9V5zppH9gZezz2htj2LhUQIDAQABAoIBAGmeBuDWiYZTq+YT
pDiJLIZXB/atj32Knc7gjhlPSYTYgf4Y6ZRvkCbKWcdvXPe/fdyavOxHC0TVExSu
Lo6+JhKkWTsa9oD3JkJL2B8PlVQUxnh32cHWyO2/cmgyVJnsYXMAJOjpXLr11VEh
RfdhpRnuqcLXn6gbcEcmUHmVmzCMbk3Irt2V4WYMj9kFT7Ag5kb7CSy+jqkRNXcw
dr7tgjj5NG/Mxl2iOf4nDsTGHri/CAHittK2pH0zNvCP7ZpPw1i6RaYfLu9QjM7i
DcgOZ7Bzy8zUjNye3lWOGiOu0mmqSPA17A0u7OtJAl8a/wMX2ZH9dX3Ldwx6SXfZ
KMLBei0CgYEAzKbdQLxhSTj6nWKO1J7qPBKJV8AVlfPjmfn0Kp40nnS6tHUlYwSo
hYukSkDQr6UoMWxpDEtzNwDlqnMdrKXRguWdtRbN3QQkQEzHwKpT240pZEI1hkMh
KkPj3NzlbBvqET5070sQ0saXuoGhF0uNHHwP2oxoDbRqou37VI52mx8CgYEAqXkw
/D3SPLPNj/s83jlLAa0sC4J5NgzGpHmPRAPvK1ye9Veon+1k60pRzG79ZqVjiPae
qSwY5PRqUnC39uBv2ahe32PZAeD1O/URnLAWFOydh939EgggUy8VA+jVkVtaHSpv
lMoSpqVm0mHkFx6r5BRDAs+cPahSJnDX5aExZY8CgYEAk1qCbJ43HlXcfX39gbJ6
zY47Peecoz/cXX3QBO+sy3sZA95lcQplnOKTBLPujsnISu3YJXYeGziWJUiF0U4D
sNBL4Zvt1nmo8HNYUK1Te/4EE6Bokae8NYZ0hw62UkU3H/tsju/pyRodHblAmFlZ
3WymraHzOgrxzSa1lIiPddkCgYBrOFCElbxISV15hXMqCUdqJRyjn4V+a7DVaE76
sh6D6ZJsGGyH2hKqRhfgZiwN3/9MLPOOhmatm/EUfVn2aGJ7a2mynaIy7RdaOp9M
6Z7e2CCGUC0HVnrVBgmespVD5G7r2uhDU2Ffa2/VFLfQ5MfNggAvbBgmbjKjPWtq
TYRKAwKBgQCTLPgwaHHgAvDvWcv/Na1rP0O76BsgCQCmMpFA5+g1NOfzcx6+n/gL
wOeX1IKOr7qOvrzNAVoFEpSjVv5W06WyKTxfktcCezFRkq3Hj2IJCt82NB0YRLin
dK+Rr3rjbD6k2JZy5PkSAmGn+/1WOxR4kabz1Ddu3aZNJem+PzNAhA==
-----END RSA PRIVATE KEY-----

curl https://localhost:29443/test.html --insecure --cert ./mis.pem --key ./mis.key
curl https://mn2regcap0002d0.uk365office.co.uk:29443/test.html --insecure --cert ./mis.pem --key ./mis.key</t>
<t tx="jonathanhudson.20201006084250.219">```
openssl x509 -noout -text -in operator.crt
```
</t>
<t tx="jonathanhudson.20201006084250.22"></t>
<t tx="jonathanhudson.20201006084250.220"></t>
<t tx="jonathanhudson.20201006084250.221">@nocolor
ln -s /cygdrive/c/Program\ Files\ \(x86\)/Notepad++/notepad++.exe /usr/local/bin/npp</t>
<t tx="jonathanhudson.20201006084250.222">@nocolor
Something in cygwin doesn't work probably has missing dependencies

cygcheck /usr/bin/make.exe</t>
<t tx="jonathanhudson.20201006084250.223">
In order to run the build you will need to make some changes to cygwin to enable maven to work correctly

Assuming that you have maven, tomcat and java installed in the following locations

```
c:\apache-maven-3.3.9
c:\apache-tomcat-7.0.73
c:\java\jdk-8u112x64
c:\java\jdk-7u79
```

You will need to add the following to the ```/cygwin/home/.bashrc``` file

</t>
<t tx="jonathanhudson.20201006084250.224">Append the following to the file
```
alias mvn=/cygdrive/c/apache-maven-3.3.9/bin/mvn.cmd
alias startup.sh=/cygdrive/c/apache-tomcat-7.0.73/bin/startup.bat
alias shutdown.sh=/cygdrive/c/apache-tomcat-7.0.73/bin/shutdown.bat
alias catalina.sh=/cygdrive/c/apache-tomcat-7.0.73/bin/catalina.bat

JAVA_HOME=/java/jdk1.8.0_91
CATALINA_HOME=/apache-tomcat-7.0.73
M2_HOME=/apache-maven-3.3.9
M2=$M2_HOME/bin

PATH=$JAVA_HOME/bin:$CATALINA_HOME/bin:$M2:$PATH

export JAVA_HOME=$( cygpath "$JAVA_HOME" )
export JRE_HOME=$JAVA_HOME
export M2_HOME=$( cygpath "$M2_HOME" )
export M2=$( cygpath "$M2" )
export PATH=$( cygpath "$PATH" )
export CATALINA_HOME=$( cygpath "$CATALINA_HOME" )
```

Now you need to add a file to enable the vi editor to work correctly

</t>
<t tx="jonathanhudson.20201006084250.225">
And add the following contents to the file
```
set nocompatible
set backspace=2
```

</t>
<t tx="jonathanhudson.20201006084250.226">
Firstly backup the existing policy files 
```
copy C:\java\jdk-8u112x64\jre\lib\security\local_policy.jar c:\backup
copy C:\java\jdk-8u112x64\jre\lib\security\US_export_policy.jar c:\backup
copy C:\java\jdk-7u79\jre\lib\security\local_policy.jar c:\backup
copy C:\java\jdk-7u79\jre\lib\security\US_export_policy.jar c:\backup
```
Unzip the ```java_security_8_policy.zip``` file and replace the ```local_policy.jar``` and ```US_export_policy.jar``` files with the ones in ```C:\java\jdk-&lt;version&gt;\jre\lib\security\```
```
copy &lt;java_v8_local_policy&gt;\local_policy.jar C:\java\jdk-8u112x64\jre\lib\security\
copy &lt;java_v8_local_policy&gt;\US_export_policy.jar C:\java\jdk-8u112x64\jre\lib\security\
copy &lt;java_v7_local_policy&gt;\local_policy.jar C:\java\jdk-7u79\jre\lib\security
copy &lt;java_v7_local_policy&gt;\US_export_policy.jar C:\java\jdk-7u79\jre\lib\security
```







</t>
<t tx="jonathanhudson.20201006084250.227"></t>
<t tx="jonathanhudson.20201006084250.228">@nocolor
openssl pkcs12 -in prd-proxy-client.p12 -nocerts -out prd-proxy-client.key.pem
openssl pkcs12 -in prd-proxy-client.p12 -clcerts -nokeys -out prd-proxy-client.cert.pem

curl --max-time 20 -k --cert ./prd-proxy-client.cert.pem --key ./prd-proxy-client.key.pem --key-type PEM -X POST --upload-file test-active-games.req -H "Content-Type: text/xml; charset=utf-8" https://bgrrd.hillsidenewmedia.com/api/activegames

prd - WXgT0BewMc7e



</t>
<t tx="jonathanhudson.20201006084250.229">&lt;S:Envelope xmlns:S="http://schemas.xmlsoap.org/soap/envelope/"&gt;
   &lt;S:Body&gt;
      &lt;ns3:commitTransactions xmlns:ns2="http://dictao.com/d3s/jel/dk/service/error/v1.xsd" xmlns:ns3="http://dictao.com/d3s/jel/dk/service/fixed-odds/v2_0.wsdl" xmlns:ns4="http://dictao.com/d3s/jel/dk/service/skat/v2009_01"&gt;
         &lt;transactions&gt;
            &lt;ns4:fixedOddsInstance&gt;
               &lt;ns4:fixedOddsInstanceObject&gt;
                  &lt;ns4:fixedOddsInstanceID&gt;FastOddsTransaktionStruktur-1&lt;/ns4:fixedOddsInstanceID&gt;
                  &lt;ns4:GameCategoryName&gt;FixedOddsGame&lt;/ns4:GameCategoryName&gt;
                  &lt;ns4:GameTypeName&gt;Slot machine&lt;/ns4:GameTypeName&gt;
                  &lt;ns4:GameTypeIdentification&gt;123546&lt;/ns4:GameTypeIdentification&gt;
               &lt;/ns4:fixedOddsInstanceObject&gt;
            &lt;/ns4:fixedOddsInstance&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125410&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3250&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.0&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125411&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3251&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.1&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125412&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3252&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.2&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125413&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3253&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.3&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125414&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3254&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.4&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125415&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3255&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.5&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125416&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3256&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.6&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125417&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3257&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.7&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125418&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3258&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.8&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
            &lt;ns4:transactions&gt;
               &lt;ns4:transactionID&gt;125419&lt;/ns4:transactionID&gt;
               &lt;ns4:playerID&gt;Player3259&lt;/ns4:playerID&gt;
               &lt;ns4:transactionDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:transactionDate&gt;
               &lt;ns4:expectedEndDate&gt;2017-01-06T15:22:06.033Z&lt;/ns4:expectedEndDate&gt;
               &lt;ns4:channel&gt;Internet&lt;/ns4:channel&gt;
               &lt;ns4:wager&gt;10&lt;/ns4:wager&gt;
               &lt;ns4:CurrencyInformationCode&gt;EUR&lt;/ns4:CurrencyInformationCode&gt;
               &lt;ns4:GameTerminalIdentification&gt;145.26.45.9&lt;/ns4:GameTerminalIdentification&gt;
            &lt;/ns4:transactions&gt;
         &lt;/transactions&gt;
      &lt;/ns3:commitTransactions&gt;
   &lt;/S:Body&gt;
&lt;/S:Envelope&gt;</t>
<t tx="jonathanhudson.20201006084250.23"></t>
<t tx="jonathanhudson.20201006084250.230">@nocolor
curl --max-time 20 -k --cert ./dev-d3s-client-test.crt.pem --key ./dev-d3s-client-test.key --key-type PEM -X POST --upload-file test-cap.req https://mn2-dev-rproxy.lb.local:443/d3s/jel/dk/service/fixed-odds/v2u1

dev - xxxxxx</t>
<t tx="jonathanhudson.20201006084250.231">@nocolor
curl --max-time 20 -k --cert ./dev-d3s-client-test.crt.pem --key ./dev-d3s-client-test.key --key-type PEM -X POST --upload-file test-cap.req https://mn2-dev-rproxy.lb.local:443/d3s/jel/dk/service/fixed-odds/v2u1

dev - p&gt;h*Bfb&lt;_a



openssl pkcs12 -in path.p12 -out newfile.pem

#javax.net.ssl.keyStorePassword=xxxxxx		dev-test-1
#javax.net.ssl.keyStorePassword=xxxxxx		dev-test-2

</t>
<t tx="jonathanhudson.20201006084250.232"></t>
<t tx="jonathanhudson.20201006084250.233"># Root /  usually /usr
du -cha --max-depth=1 /usr | grep -E "M|G"


du -cha --max-depth=1 / | grep -E "M|G"

du -cha --max-depth=1 /xyz | grep -E "M|G"

du -hc /xyz | sort -rh | head -20

tree --du -h /data/bricks/dk-brick



find / -xdev -type f -size +50M

find / -xdev -type f -size +50M -exec ls -alh {} \; | sort -nk 5

find /usr -type f -printf "%s %p\n" | sort -rn | head -n 10

ls -1Rhs | sed -e "s/^ *//" | grep "^[0-9]" | sort -hr | head -n10

du -ah /var | sort -n -r | head -n 10


du -ah . | sort -n -r | head -n 10




du -cha --max-depth=1 /var | grep -E "M|G"



# Clear /tmp not used in 10 days

find /tmp -type f -atime +10 -delete
find /tmp -type f -mtime +10 -delete
</t>
<t tx="jonathanhudson.20201006084250.234">[root@mn2regcap0001u1 ~]# lvdisplay -m


  --- Logical volume ---
  LV Path                /dev/vg_root/lv_swap
  LV Name                lv_swap
  VG Name                vg_root
  LV UUID                oOzLQH-Vghj-PZTy-6zVz-6cjv-XHdS-oq5HBk
  LV Write Access        read/write
  LV Creation host, time localhost, 2017-02-15 09:42:18 +0000
  LV Status              available
  # open                 2
  LV Size                &lt;3.91 GiB
  Current LE             1000
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:1
   
  --- Segments ---
  Logical extents 0 to 999:
    Type		linear
    Physical volume	/dev/sda2
    Physical extents	0 to 999
   
   
  --- Logical volume ---
  LV Path                /dev/vg_root/lv_root
  LV Name                lv_root
  VG Name                vg_root
  LV UUID                7634IM-JTNr-cdHH-KVEm-tbxZ-tyRP-clR7d1
  LV Write Access        read/write
  LV Creation host, time localhost, 2017-02-15 09:42:18 +0000
  LV Status              available
  # open                 1
  LV Size                35.56 GiB
  Current LE             9104
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:0
   
  --- Segments ---
  Logical extents 0 to 9103:
    Type		linear
    Physical volume	/dev/sda2
    Physical extents	1000 to 10103
   
   
  --- Logical volume ---
  LV Path                /dev/vg_data/lv_data
  LV Name                lv_data
  VG Name                vg_data
  LV UUID                8kIJ1f-q2mA-g4WY-U2q6-gkaY-WhX2-Rlgj3u
  LV Write Access        read/write
  LV Creation host, time localhost, 2017-02-15 09:42:17 +0000
  LV Status              available
  # open                 1
  LV Size                49.99 GiB
  Current LE             12798
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:2
   
  --- Segments ---
  Logical extents 0 to 2558:
    Type		linear
    Physical volume	/dev/sdb1
    Physical extents	0 to 2558
   
  Logical extents 2559 to 12797:
    Type		linear
    Physical volume	/dev/sdc1
    Physical extents	0 to 10238
   
</t>
<t tx="jonathanhudson.20201006084250.235"> du -sh &lt;foldername&gt;
 
 
 du -h | sort -rh | more
 
 
</t>
<t tx="jonathanhudson.20201006084250.236"></t>
<t tx="jonathanhudson.20201006084250.237">date; find /mnt/SAFE_DATA -type f -name "*.zip" -mmin +15 | wc -l ; date


date; find /mnt/SAFE_DATA -type f -name "*.zip" -mmin +10 | wc -l ; date


date; find /mnt/SAFE_DATA/d3s-dk/storage/b365 -type f -name "*.zip" -mmin +15 | wc -l ; date




        logger.debug( "Files : " + fileX.countFiles( "C:/mnt/SAFE_DATA/d3s-dk/storage/Dictao1/", "D|yyyy-MM-dd|-0","zip", 15 ) );
        //logger.debug( "Files : " + fileX.countFiles( "C:/mnt/SAFE_DATA/d3s-bg/storage/Dictao1/", "D|yyyy\\MM\\dd|-0","zip", 15 ) );


/mnt/SAFE_DATA/d3s-dk/storage/b365/folderstruktur-spilsystem/Zip/2017-09-29

/mnt/SAFE_DATA/d3s-bg/storage/b365sports/data/2017/09/29

####### Filenames with date that are within min and max

/mnt/SAFE_DATA/d3s-es/storage/b365/CNJ/1013/JU/20170929

/mnt/SAFE_DATA/d3s-es/storage/b365sports/CNJ/1007/OP/AHC/Mensual/OPT

/mnt/SAFE_DATA/d3s-es/storage/b365sports/CNJ/1007/JU/20170929

}





/mnt/SAFE_DATA/d3s-dk/storage


date; find /mnt/SAFE_DATA -type f -name "*.zip" | wc -l ; date


date; find . -type f -name '*20171002*' -mmin +60 | wc -l ; date


date; find . -type f -name '*2017-10-02*' -mmin +60 | wc -l ; date




find . -mmin -60 -type f -exec ls -l {} +

</t>
<t tx="jonathanhudson.20201006084250.238">
find /mnt/emergency/d3s-dk/tampertokens/ -type f -mindepth 1 -mtime +165 -depth -print | wc -l
find /mnt/emergency/d3s-dk/tampertokens/ -type f -mindepth 1 -mtime +165 -delete


find . -maxdepth 1 -type f |head -1000|xargs cp -t "$destdir"

# -print0 and xargs -0 work with ' ' separated list ( no wc -l effectively all on 1 line )
find /mnt/emergency/d3s-dk/tampertokens/ -type f -print0 | xargs -0 rm


# Remove the first 1000 files found recursuvely
find /mnt/emergency/d3s-dk/tampertokens/ -type f -path '*closed*' -print | wc -l

find /mnt/emergency/d3s-dk/tampertokens/ -type f -path '*closed*' -print | head -1000 | xargs rm

find /mnt/emergency/d3s-dk/tampertokens/ -type f -path '*closed*' -print | head -1000 | wc -l









find /mnt/SAFE_DATA/d3s-dk/tampertokens/b365 -type f -path '*closed*' | wc -l
find /mnt/SAFE_DATA/d3s-dk/tampertokens/b365sports -type f -path '*closed*' | wc -l
find /mnt/SAFE_DATA/d3s-dk/tampertokens/Dictao1 -type f -path '*closed*' | wc -l
find /mnt/SAFE_DATA/d3s-dk/tampertokens/Dictao2 -type f -path '*closed*' | wc -l





find /mnt/emergency/d3s-dk/tampertokens/b365 -type f -path '*closed*' | wc -l
find /mnt/emergency/d3s-dk/tampertokens/b365sports -type f -path '*closed*' | wc -l
find /mnt/emergency/d3s-dk/tampertokens/Dictao1 -type f -path '*closed*' | wc -l
find /mnt/emergency/d3s-dk/tampertokens/Dictao2 -type f -path '*closed*' | wc -l


find /mnt/emergency/d3s-dk/tampertokens/b365 -type f -path '*closed*' -print | head -21000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/b365sports -type f -path '*closed*' -print | head -11000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao1 -type f -path '*closed*' -print | head -37000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao2 -type f -path '*closed*' -print | head -18000 | xargs rm


find /mnt/emergency/d3s-dk/tampertokens/b365 -type f -path '*closed*' -print | head -5000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/b365sports -type f -path '*closed*' -print | head -5000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao1 -type f -path '*closed*' -print | head -5000 | xargs rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao2 -type f -path '*closed*' -print | head -5000 | xargs rm


# dev cap 1
find /mnt/emergency/d3s-dk/tampertokens/b365 -type f -path '*closed*' -print | head -10000 | xargs /bin/rm
find /mnt/emergency/d3s-dk/tampertokens/b365sports -type f -path '*closed*' -print | head -1000 | xargs /bin/rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao1 -type f -path '*closed*' -print | head -10000 | xargs /bin/rm
find /mnt/emergency/d3s-dk/tampertokens/Dictao2 -type f -path '*closed*' -print | head -1000 | xargs /bin/rm
</t>
<t tx="jonathanhudson.20201006084250.239">find . -type d -printf "%P\n" | sort &gt; file1

find . -type d -printf "%P\n" | sort | diff - file1 </t>
<t tx="jonathanhudson.20201006084250.24"></t>
<t tx="jonathanhudson.20201006084250.240">export months="\(201912\|202001\)"  # 201912 or 202001

find /mnt/SAFE_DATA/d3s-es/storage/ -type f  -regextype sed -regex ".*${months}[0-9]\{2\}\.zip\.[[:digit:]]\{3\}"</t>
<t tx="jonathanhudson.20201006084250.241">date; find . -type f -path '*2017-10-17*' -name '*.zip' | wc -l ; date
date; find . -type f -path '*2017/10/18*' -name '*.zip' | wc -l ; date

cd /mnt/SAFE_DATA/d3s-dk/storage

date; find . -type f -path '*/b365/folderstruktur-spilsystem/Zip/2017-10-18/*' -name '*.zip' | wc -l ; date
date; find . -type f -path '*/b365sports/folderstruktur-spilsystem/Zip/2017-10-18/*' -name '*.zip' | wc -l ; date


cd /mnt/SAFE_DATA/d3s-bg/storage

date; find . -type f -path '*b365sports/data*2017/10/18*' -name '*.zip' | wc -l ; date
date; find . -type f -path '*b365sports/receipts*2017/10/18*' -name '*.zip' | wc -l ; date






# the --include='*/'  &lt;- is critical to it working

rsync -rv --stats --include='*.zip' --include='*/' --include='2017-10-18/' --include='2017-10-19/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/


rsync -rv --stats --include='*/' --include='2017-10-18/' --include='2017-10-19/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/

rsync -rv --stats --include='*/' --include='2017-10-18/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/


rsync -rv --stats --include='*/' --include='2017-10-19/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/




rsync -rv --stats --ignore-existing --include='*/' --include='2017-10-16/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/





rsync -rv --stats --include='*/' --include='/mnt/SAFE_DATA/d3s-dk/storage/b365sports/folderstruktur-spilsystem/Zip/2017-10-19/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/



date; rsync -rv --stats --include='*/' --include='/mnt/SAFE_DATA/d3s-dk/storage/b365sports/folderstruktur-spilsystem/Zip/2017-01-01/' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/ ; date



date; rsync -rv --stats --include='*/' --include='/b365sports/folderstruktur-spilsystem/Zip/2017-01-02/*' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/ ; date


date; rsync -rv --stats --include='*/' --include='/b365sports/folderstruktur-spilsystem/Zip/2017-10-19/*' --exclude='*' /mnt/SAFE_DATA/d3s-dk/storage/ ml3regrep0001p1:/mnt/SAFE_DATA/d3s-dk/storage/ ; date


</t>
<t tx="jonathanhudson.20201006084250.242">
# files in safe created with 1 day

find /mnt/SAFE_DATA/d3s-es/storage -iname "*.zip" -mtime -1 -print
</t>
<t tx="jonathanhudson.20201006084250.243">
du -a / 2&gt;/dev/null | sort -n -r | head -n 20
</t>
<t tx="jonathanhudson.20201006084250.244">find  . -name "*.jar" | xargs -L 1 zipinfo | grep error | grep invalid
</t>
<t tx="jonathanhudson.20201006084250.245">


mkdir -p /data/home/d3s-bg/tmp/receipts-backup/

# BG2 FSH

find /data/SAFE_DATA/d3s-bg/storage/b365sports/receipts/2017/12/ -type d -name "b365sports*" -printf "mv %p /data/home/d3s-bg/tmp/receipts-backup/\n" &gt; /data/home/d3s-bg/tmp/receipts-backup/move-receipts-folders.sh

# IR1 BKP

find /mnt/SAFE_DATA/d3s-bg/storage/b365sports/receipts/2017/12/ -type d -name "b365sports*" -printf "mv %p /data/home/d3s-bg/tmp/receipts-backup/\n" &gt; /data/home/d3s-bg/tmp/receipts-backup/move-receipts-folders.sh




less /data/home/d3s-bg/tmp/receipts-backup/move-receipts-folders.sh</t>
<t tx="jonathanhudson.20201006084250.246">$ dig +short myip.opendns.com @resolver1.opendns.com
81.94.213.9
</t>
<t tx="jonathanhudson.20201006084250.247">find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -i "&lt;Registro xsi:type=" | wc -l' \; -print

# Show record ids
find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print


# regular expression ( 3 characters before and 4 chars after 
-P '.{0,3}string.{0,4}'


pg_ctl stop
pg_ctl restart -l /var/log/pgsql/startup.log

pg_ctl start -l /var/log/pgsql/startup.log


# On lOcal Linux

systemctl stop postgresql-9.6
systemctl start postgresql-9.6


# Check Xml files

grep -o -i "&lt;Registro xsi:type=" soap-fixedodds-betting-details-response-*.xml | wc -l

grep -o -i "&lt;Registro xsi:type=" source-lote.xml | wc -l




# Specific

find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' -print \; | grep "SubregistroId&gt;2420"
find . -name '*.zip' -print -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' -print \; | grep "SubregistroId&gt;2420"

find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_c5ccb7e8-0d82-4864-b099-c8d6b49f5c53.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_b4b31bcd-ce9e-4e6f-ae13-fccfbd3ffd16.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_feed5705-35ae-41e9-98e5-58fc59026547.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_6a6e08c4-2cf8-4d67-839a-b49f180d5d5e.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_3808ac14-ecb0-4dc4-ab6c-c267a0c8b13a.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_2f5d07a6-a971-4308-a13f-d93d8140cece.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_f387cb83-21b7-4155-a73e-37c133b895a4.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print
find ./Dictao2/CNJ/Dictao2/JU/Mensual/JUA/Dictao2_Dictao2_JU_JUA_M_201305_5b8f65d5-2595-420a-9289-e5a2126909a4.zip -name '*.zip' -exec sh -c '7za x -ppassword -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print



1391

&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2581&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2582&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2583&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2584&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2585&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2586&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;2587&lt;/Subr

&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;1691&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;1692&lt;/Subr
&gt;&lt;RegistroId&gt;1&lt;/RegistroId&gt;&lt;SubregistroId&gt;1693&lt;/Subr


find . -name '*.zip' -exec sh -c '7za x -p"OeKsGySbXgUpFnVfRtCo" -so {} lote.xml | grep -o -P ".\&lt;RegistroId.{0,40}"' \; -print &gt; /tmp/recordIds.txt
</t>
<t tx="jonathanhudson.20201006084250.248">find . -name '*.zip' -exec sh -c '7za x -ppassword -so {} data.zip | bsdtar -tf- | grep "record" | wc -l' \; -print



</t>
<t tx="jonathanhudson.20201006084250.249">rsync -rm --stats --delete --include="*/" --include="*.zip" --exclude="*" /mnt/SAFE_DATA/d3s-bg/storage/b365sports/data/ mn2regmok0001u0:/mnt/SAFE_DATA/d3s-bg/storage/b365sports/data/

rsync -rm --stats --delete --include="*/" --include="*.log" --exclude="*" /mnt/SAFE_DATA/d3s-bg/storage/b365sports/logs/ mn2regrep0001u1:/mnt/SAFE_DATA/d3s-bg/storage/b365sports/logs/
rsync -rm --stats --delete --include="*/" --include="*.log" --exclude="*" /mnt/SAFE_DATA/d3s-bg/storage/b365sports/errlogs/ mn2regrep0001u1:/mnt/SAFE_DATA/d3s-bg/storage/b365sports/errlogs/
rsync -rm --stats --delete --include="*/" --include="*.zip" --exclude="*" /mnt/SAFE_DATA/d3s-bg/storage/b365sports/receipts/ mn2regrep0001u1:/mnt/SAFE_DATA/d3s-bg/storage/b365sports/receipts/
rsync -rm --stats --delete --include="*/" --include="*.xml" --exclude="*" /mnt/SAFE_DATA/d3s-bg/storage/b365sports/receipts/synccalls/ mn2regrep0001u1:/mnt/SAFE_DATA/d3s-bg/storage/b365sports/receipts/synccalls/



find /mnt/SAFE_DATA/d3s-bg/storage/b365sports -type f ! -path '*data*' | awk -vFS=/ -vOFS=/ '{ print $NF,$0 }' | sort -n -t / | cut -f2- -d/ &gt; /tmp/files-a.txt

find /mnt/SAFE_DATA/d3s-bg/storage/b365sports -type f ! -path '*data*' | awk -vFS=/ -vOFS=/ '{ print $NF,$0 }' | sort -n -t / | cut -f2- -d/ &gt; /tmp/files-b.txt
scp /tmp/files-b.txt mn2regrep0001u1:/tmp/


</t>
<t tx="jonathanhudson.20201006084250.25"></t>
<t tx="jonathanhudson.20201006084250.250">

find . -name &lt;name&gt; -delete       !Important - put delete at end not hte beginning

find . -name &lt;name&gt; -print0 | xargs -0 rm


find . -name &lt;name&gt; -exec rm {} \;
</t>
<t tx="jonathanhudson.20201006084250.251">@language md

### Example find with regex
```
find . -regextype sed -regex ".*/[a-f0-9\-]\{36\}\.jpg"
```

Note that you need to specify .*/ in the beginning because find matches the whole path.

### No Regex
```
find . -name "*.jpg"

./foo-111.jpg
./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg
./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg
```
### With Regex
```
find . -regextype sed -regex ".*/[a-f0-9\-]\{36\}\.jpg"

./test/81397018-b84a-11e0-9d2a-001b77dc0bed.jpg
./81397018-b84a-11e0-9d2a-001b77dc0bed.jpg
```</t>
<t tx="jonathanhudson.20201006084250.252">find /mnt/emergency/d3s-dk/tampertokens/Dictao1/closed/ -type f -mtime +30 -exec mv '{}' /mnt/SAFE_DATA/d3s-dk/backup/ddMMyyyy/tampertokens/ \;


find /storage/current/dbdumps/ -type f -mtime +30 -print | xargs -I {} mv "{}" /storage/archive/dbdumps/
</t>
<t tx="jonathanhudson.20201006084250.253"></t>
<t tx="jonathanhudson.20201006084250.254">
grep -r -i --include \*.h --include \*.cpp CP_Image ~/path[12345] | mailx -s GREP email@domain.com



grep -r -i --include \*.java 'lingala'

grep -r -i --include \*.java 'setPassword'


grep -r -i --include \*.java 'setPassword'


grep -r -i --include \*.txt 'mount'
</t>
<t tx="jonathanhudson.20201006084250.255">tail -f dictao-d3s-std.2018-10-09.log | grep 'messages in ( [1-9][0-9]* ) ms'




UAT

tail -f dictao-d3s-std.2018-10-17.log | grep 'messages in ( [1-9][0-9]* ) ms'

2018-10-09 14:46:03,094 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 44 ) messages in ( 7381 ) ms, with ( 39 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:04,099 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 9 ) messages in ( 1005 ) ms, with ( 3 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:06,322 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 21 ) messages in ( 2223 ) ms, with ( 6 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:12,841 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 21125 ) ms, with ( 20184 ) in addMessage() of which ( 3626 ) was enqueing
2018-10-09 14:46:20,163 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 70 ) messages in ( 13841 ) ms, with ( 42 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:21,342 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 75 ) messages in ( 1179 ) ms, with ( 29 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:25,714 DEBUG pool-3-ES-batch-build-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:26,714 DEBUG pool-3-ES-batch-build-4:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:46:27,348 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 14507 ) ms, with ( 14478 ) in addMessage() of which ( 3790 ) was enqueing
2018-10-09 14:46:40,140 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 12791 ) ms, with ( 12193 ) in addMessage() of which ( 3387 ) was enqueing
2018-10-09 14:46:57,451 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 17311 ) ms, with ( 11328 ) in addMessage() of which ( 3118 ) was enqueing
2018-10-09 14:47:02,328 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 640 ) messages in ( 40986 ) ms, with ( 1792 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:02,329 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:03,421 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 134 ) messages in ( 1092 ) ms, with ( 89 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:04,488 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 130 ) messages in ( 1067 ) ms, with ( 55 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:05,625 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 130 ) messages in ( 1137 ) ms, with ( 90 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:06,951 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 352 ) messages in ( 1325 ) ms, with ( 222 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:07,985 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 52 ) messages in ( 1034 ) ms, with ( 20 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:09,050 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 152 ) messages in ( 1065 ) ms, with ( 56 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:12,279 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 140 ) messages in ( 3229 ) ms, with ( 80 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:13,492 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 140 ) messages in ( 1213 ) ms, with ( 59 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:20,376 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 22925 ) ms, with ( 21898 ) in addMessage() of which ( 4773 ) was enqueing
2018-10-09 14:47:20,714 DEBUG pool-3-ES-batch-build-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:25,715 DEBUG pool-3-ES-batch-build-2:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:26,714 DEBUG pool-3-ES-batch-build-4:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:32,141 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 11765 ) ms, with ( 8531 ) in addMessage() of which ( 2051 ) was enqueing
2018-10-09 14:47:40,714 DEBUG pool-3-ES-batch-build-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:47:49,675 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 17534 ) ms, with ( 9668 ) in addMessage() of which ( 2640 ) was enqueing
2018-10-09 14:47:58,714 DEBUG pool-3-ES-batch-build-4:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:02,035 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 519 ) messages in ( 48543 ) ms, with ( 1466 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:03,233 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 105 ) messages in ( 1197 ) ms, with ( 82 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:04,287 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 109 ) messages in ( 1054 ) ms, with ( 47 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:05,359 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 105 ) messages in ( 1071 ) ms, with ( 69 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:09,585 DEBUG pool-4-ES-check-unique-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1000 ) messages in ( 19910 ) ms, with ( 14730 ) in addMessage() of which ( 3643 ) was enqueing
2018-10-09 14:48:12,714 DEBUG pool-3-ES-batch-build-2:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:13,217 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 375 ) messages in ( 7858 ) ms, with ( 310 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:14,218 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 1 ) messages in ( 1000 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:14,714 DEBUG pool-3-ES-batch-build-3:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 0 ) messages in ( 1 ) ms, with ( 0 ) in addMessage() of which ( 0 ) was enqueing
2018-10-09 14:48:19,704 DEBUG pool-3-ES-batch-build-1:  [com.dictao.d3s.jel.server.manager.batch.BatchHandlerBase] Batch Handler message consumer received : ( 185 ) messages in ( 5486 ) ms, with ( 63 ) in addMessage() of which ( 0 ) was enqueing
</t>
<t tx="jonathanhudson.20201006084250.256">@nocolor
grep -r --include='*.java' 'dictao' ./    | wc -l
grep -r --include='*.xml' 'dictao' ./    | wc -l
grep -r --include='*.properties' 'dictao' ./    | wc -l
grep -r --include='*.sh' 'dictao' ./    | wc -l</t>
<t tx="jonathanhudson.20201006084250.257"></t>
<t tx="jonathanhudson.20201006084250.258">## Block access to Postgres

iptables -L

#iptables -A FORWARD -p tcp --dport 5432 -j DROP
iptables -A INPUT -p tcp --dport 5432 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 5432 -m state --state NEW,ESTABLISHED -j DROP

&gt; /dev/tcp/localhost/5432

# Perform Testing


## Reset the Firewall

iptables -F

## Check Connection 

&gt; /dev/tcp/localhost/5432
</t>
<t tx="jonathanhudson.20201006084250.259"># On the GlusterFS brick nodes only

## Check
```
iptables -L
'''

### Loopback
```
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
```

### SSH
```
iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 22 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### HTTP / HTTPS
```
iptables -A INPUT -p tcp --dport 80 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 80 -m conntrack --ctstate ESTABLISHED -j ACCEPT
iptables -A INPUT -p tcp --dport 443 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 443 -m conntrack --ctstate ESTABLISHED -j ACCEPT
iptables -A INPUT -p tcp -m multiport --dports 80,443 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp -m multiport --dports 80,443 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### Postgres
```
iptables -A INPUT -p tcp -s 15.15.15.0/24 --dport 5432 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 5432 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### SMTP
```    
iptables -A INPUT -p tcp --dport 25 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 25 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### IMAP / IMAPS
```
iptables -A INPUT -p tcp --dport 143 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 143 -m conntrack --ctstate ESTABLISHED -j ACCEPT
iptables -A INPUT -p tcp --dport 993 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 993 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### POP3 / POP3S
```
iptables -A INPUT -p tcp --dport 110 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 110 -m conntrack --ctstate ESTABLISHED -j ACCEPT
iptables -A INPUT -p tcp --dport 995 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 995 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

### GlusterFS
```
iptables -A INPUT -d localhost -p tcp --dport 24007:24020 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
iptables -A INPUT -s &lt;client.ip&gt; -p tcp --dport 24007:24008 -m conntrack --ctstate NEW,ESTABLISHED-j ACCEPT
iptables -A INPUT -s &lt;client.ip&gt; -p tcp --dport 49152:49170 -m conntrack --ctstate NEW,ESTABLISHED-j ACCEPT
iptables -A OUTPUT -p tcp --sport 24007:24008 -m conntrack --ctstate ESTABLISHED -j ACCEPT
iptables -A OUTPUT -p tcp --sport 49152:49170 -m conntrack --ctstate ESTABLISHED -j ACCEPT
```

service iptables save
service iptables restart


# On first node
gluster peer probe &lt;second_host&gt;

# Check peers status
gluster peer status
</t>
<t tx="jonathanhudson.20201006084250.26"># Userful Commands

ansible-playbook hello-world.yml

ansible-playbook -i "localhost," -c local hello-world.yml

#ansible all -m shell -a 'echo hello world'

ansible -m ping test -k

ansible -m ping uat_db -k</t>
<t tx="jonathanhudson.20201006084250.260"></t>
<t tx="jonathanhudson.20201006084250.261">mtr --report --report-cycles 10 www.bbc.co.uk &gt; mtr-bbc-report.txt
^C

cat mtr-bbc-report.txt

</t>
<t tx="jonathanhudson.20201006084250.262">tcpdump -i eth0 -s0 -w test.pcap

# Get

tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420'

# Post 

tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354'

# HTTP Data Packets

tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)

# Source Destination

dst 10.32.108.28 


tcpdump -i eno1 src 10.224.36.38 port 443 -w test-upload.pcap

tcpdump -r test-upload.pcap



tcpdump -tttttni eno1 port 443


dd if=/dev/urandom of=sample.txt bs=64M count=16



tcpdump -tttttni eno1 dst 10.32.63.228 -w test-upload-1.pcap

tcpdump -r test-upload-1.pcap | more


tcpdump -r test-upload-1.pcap | more
tcpdump -tttttni eno1 dst 10.32.63.228 -w test-upload-2.pcap
tcpdump -r test-upload-2.pcap | more

nslookup 10.32.63.228
ping ir1sftrpx0001p1

tcpdump -tttttni eno1 dst 10.32.108.28 -w test-upload-rpx.pcap
tcpdump -r test-upload-rpx.pcap | more

tcpdump -r test-upload-2.pcap | more
tcpdump -tttttni eno1 dst 10.32.108.28 -w test-upload-rpx-1.pcap
tcpdump -r test-upload-rpx-1.pcap | more
</t>
<t tx="jonathanhudson.20201006084250.263">tcp dump


tcpdump -X host ir2regcap0001p1 port 111 and port 2049
</t>
<t tx="jonathanhudson.20201006084250.264">@nocolor
tcpdump src 10.225.36.17 port 5432


tcpdump host 10.225.36.17 

.PSQLException: FATAL: no pg_hba.conf entry for host "10.36.63.250", user "safeuser", database "safedb"


ir1regpdg0001p0.uk365office.co.uk.postgres &gt; hls-f10299.uk365office.co.uk.49246:

10.32.63.250.12998 &gt; ir1regpdg0001p0.uk365office.co.uk.postgres: Flags [.], ack 2, win 8188, length 0
10.36.63.250.25424 

tcpdump port 5432 | grep ir1-regsafedb.lb.local

tcpdump tcp port 5432

tcpdump tcp port 5432 | grep ack

10.36.63.250



#APPLICATION SERVICE IP ADDRESSES
## primary servers
host    all             all             10.32.114.16/32          md5
host    all             all             10.32.114.24/32          md5
host    all             all             10.32.63.142/32          md5
host    all             all             10.32.63.250/32          md5
host    all             all             10.36.63.250/32          md5
host    all             all             10.32.63.10/32           md5
</t>
<t tx="jonathanhudson.20201006084250.265"></t>
<t tx="jonathanhudson.20201006084250.266">
pidof python
26629 932
grep --color VmSwap /proc/26629/status
VmSwap:   998888 kB

awk '/VmSwap/{print $2 " " $3}' /proc/26629/status
998888 kB

for file in /proc/*/status ; do awk '/VmSwap|Name/{printf $2 " " $3}END{ print ""}' $file; done | sort -k 2 -n -r | less
</t>
<t tx="jonathanhudson.20201006084250.267">

dd if=/dev/urandom of=/file.dat bs=10M count=1 oflag=direct



dd if=/dev/urandom of=file-10G.dat bs=1G count=10

dd if=/dev/urandom of=file-1G.dat bs=64M count=16 iflag=fullblock

dd if=/dev/urandom of=file-2G-1.dat bs=1G count=2
dd if=/dev/urandom of=file-2G-2.dat bs=1G count=2
dd if=/dev/urandom of=file-2G-3.dat bs=1G count=2
dd if=/dev/urandom of=file-2G-4.dat bs=1G count=2
dd if=/dev/urandom of=file-2G-5.dat bs=1G count=2
dd if=/dev/urandom of=file-2G-6.dat bs=1G count=2


sha1sum file-2G-1.dat	d39e739cd209009c90da601a14755ef120c27694	
sha1sum file-2G-2.dat	eeff259cf3cd806610960d2fd836156ca0fcf84f
sha1sum file-2G-3.dat	47b649de6c07d1480637db5572b6c41a654ee423
sha1sum file-2G-4.dat	e7b1df3610fccdabb7017bcb48d31722072cfb7d
sha1sum file-2G-5.dat	e5397185f759ed48936f0eb2dcd79870d6fc295d
sha1sum file-2G-6.dat	2148bed1d52d164e4dd055734adfefe42d4acb6c

</t>
<t tx="jonathanhudson.20201006084250.268">## Shows Network IO ( Rx / Tx ) in KB/s

sar -n DEV 1 3 | grep -C 1 'IFACE' | tail -n1 | awk '{print $5, $6}'</t>
<t tx="jonathanhudson.20201006084250.269">tc qdisc add dev eno1 root netem delay 7000ms

tc qdisc del dev eno1 root



</t>
<t tx="jonathanhudson.20201006084250.27">---
- hosts: linux
  vars:
    users:
      - { name: 'd3s-es', delivery: 'dictao-d3s-jel-setup-server', module: 'dictao-d3s-jel-setup-server', symlink: 'd3s', component: 'd3s', target: 'OB1-001122' }
      - { name: 'd3s-bg-nraproxy', delivery: 'dictao-d3s-jel-setup-bg-nraproxy', module: 'dictao-d3s-jel-setup-bg-nraproxy', symlink: 'd3s-nraproxy', component: 'nraproxy', target: 'XYZ-001122' }
      - { name: 'd3s-bg-pushsrv', delivery: 'dictao-d3s-jel-setup-bg-pushsrv', module: 'dictao-d3s-jel-setup-bg-pushsrv', symlink: 'd3s-pushsrv', component: 'pushsrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-nramocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-nramocksrv', component: 'nramocksrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-opermocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-opermocksrv', component: 'opermocksrv', target: 'XYZ-001122' }      
    jurisdiction: 'es'
    configFile: []
  tasks:
  - name: check configure.properties file
    local_action: stat path=/cygdrive/c/data/code/safe-configs/Spain/LCL/{{jurisdiction}}-d3s-configure.properties    
    register: configCheck
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: show configCheck
    debug: msg={{ configCheck }}
    
  - name: filter users
    debug: msg={{ users | selectattr( 'symlink', 'equalto', 'd3s' ) | list }}
#    with_items: '{{ users | map( attribute='target' ) | list | unique }}'

  - name: filter configCheck with stat that exists
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}  

  - name: filter configCheck with stat that exists and filter on path
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | selectattr( 'stat.path', 'search', 'configure.properties' ) | list }}  
    
  - name: set source config missing
    set_fact: configFile="{{ item }}"
    with_items: "{{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}"
    
  - name: dump configFile
    debug: msg={{ configFile }}
    
#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties-1 dest=/cygdrive/c/temp/es-d3s-configure.properties-1
#    when: not configFile.results[0].stat.exists and item.target in ansible_hostname

#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties dest=/cygdrive/c/temp/es-d3s-configure.properties
#    when: configFile.results[0].stat.exists and item.target in ansible_hostname

</t>
<t tx="jonathanhudson.20201006084250.270"># Slow network

tc qdisc del dev eno1 root

tc qdisc add dev eno1 root netem delay 6000ms
tc qdisc add dev eno1 root netem delay 3500ms
tc qdisc add dev eno1 root netem delay 1000ms
tc qdisc add dev eno1 root netem delay 500ms

tc qdisc add dev eno1 root handle 1: prio
tc qdisc add dev eno1 parent 1:3 handle 30: netem delay 500ms

tc filter add dev eno1 protocol ip parent 1:0 prio 3 u32 match ip dst 10.15.2.231/32 flowid 1:3
tc filter add dev eno1 protocol ip parent 1:0 prio 3 u32 match ip dst 10.15.2.177/32 flowid 1:3

# Firewall  block

iptables -A INPUT -p tcp --dport 2049 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 2049 -m state --state NEW,ESTABLISHED -j DROP
iptables -A INPUT -p tcp --dport 111 -m state --state NEW,ESTABLISHED -j DROP
iptables -A OUTPUT -p tcp --dport 111 -m state --state NEW,ESTABLISHED -j DROP

&gt; /dev/tcp/localhost/111
&gt; /dev/tcp/localhost/2049
&gt; /dev/tcp/regsafe-dev/111
&gt; /dev/tcp/regsafe-uat/2049

iptables -F

# Simulate network unplugged

ip link set eno1 down
ip link set eno1 up

# Disable NFS services

service rpcbind stop
service nfs stop

# Change host ip address

10.10.10.10   regsafe-dev
10.10.10.10   regsafe-uat

# dk-configuration.xml

MaxDelayInSecond="60"
&lt;maxTimeOut&gt;120000&lt;/maxTimeOut&gt;
</t>
<t tx="jonathanhudson.20201006084250.271">@language md
# Cgroups

First, we need to make sure the ‘blkio’ controller is available:

```
# grep blkio /proc/mounts || mkdir -p /cgroup/blkio ; mount -t cgroup -o blkio none /cgroup/blkio
```
# dm-delay

## NOTICE

Do not change 
/dev/dm-0	centos-root
/dev/dm-1 	centos-swap

The number of ramdisks and their size is configured by passing arguments to modprobe: rd_nr is the maximum number of ramdisks and rd_size is the size of each ramdisk in kibibytes.

## Create ramdisk
```
$ sudo modprobe brd rd_nr=1 rd_size=1048576
$ ls -l /dev/ram0
brw-rw---- 1 root disk 1, 0 Aug 24 20:00 /dev/ram0
$ sudo blockdev --getsize /dev/ram0 # Display the size in 512-byte sectors
2097152
```

## Deallocate ramdisk

```
blockdev --flushbufs /dev/ram0
```

## Creating a delayed target with dm-delay

Delayed Read

```
#!/bin/sh
# Create a block device that delays reads for 500 ms
size=$(blockdev --getsize $1) # Size in 512-bytes sectors
echo "0 $size delay $1 0 500" | dmsetup create delayed

~/scripts/create-delayed-target.sh /dev/ram0
```

check created target

```
ls -al /dev/mapper
```

Delayed Read Write

```
#!/bin/sh
# Create a block device that delays reads for 500 ms and writes for 300 ms
size=$(blockdev --getsize $1) # Size in 512-bytes sectors
echo "0 $size delay $1 0 500 $1 0 300" | dmsetup create delayed

blockdev --getsize /dev/ram0
echo "0 2097152 delay /dev/ram0 0 2000 /dev/ram0 0 2000" | dmsetup create delayed
echo "0 524288 delay /dev/ram0 0 2000 /dev/ram0 0 2000" | dmsetup create delayed

ls -al /dev/mapper

dmsetup ls
dmsetup info /dev/dm-2

lvdisplay|awk  '/LV Name/{n=$3} /Block device/{d=$3; sub(".*:","dm-",d); print d,n;}'
```

## Create a filesystem on it
```
mkfs -t ext3 /dev/dm-2
```

## Mount it
```
mount -t ext3 /dev/dm-2 /mnt/SAFE_SLOW
```

## Checking the latency of dm-delay

```
dd if=/dev/zero of=/mnt/SAFE_DATA/testfile bs=100M count=1 oflag=direct
dd if=/dev/zero of=/mnt/SAFE_SLOW/testfile bs=10M count=1 oflag=direct
```

## Remove delayed target

```
dmsetup remove /dev/dm-2
```

## Suspending I/O

```
dmsetup suspend /dev/dm-2
dmsetup resume  /dev/dm-2
```

## Deallocate ramdisk

```
blockdev --flushbufs /dev/ram0
```

## Unload brd module

```
# modprobe -rv brd
```

## Check stack trace of process
```
pstack &lt;pid&gt;
```
# Slow Hard Drive

Module is not enabled automatically by default, we have to install it first, and then enable it:

```
# install nbd module
sudo yum install nbd
 
# load nbd module
sudo modprobe nbd
 
# check nbd module is really loaded
lsmod | grep nbd
nbd                    20480  0
```

Now that nbd is installed and the module loaded we create a configuration file for its daemon:

```
# run this command as root
"cat &gt; /etc/nbd-server/config" &lt;&lt;EOF
[generic]
[test]
    exportname = /home/pantinor/test_nbd
    copyonwrite = false
EOF
```

Where exportname is a path to a file that will represent your slow virtual hardisk.

You can create the file with this command:

```
# create an empty file, and reserve it 1GB of space
dd if=/dev/zero of=/home/pantinor/test_nbd bs=1G count=1
```

Now that config and the destination files are in place, you can start the nbd-server using daemon:

```
# start ndb-server daemon
sudo systemctl start nbd-server.service
 
# monitor the daaemon start up with:
journalctl -f --unit nbd-server.service
```

At this point you have a server network process, listening on port 10809 that any client over your network can connect to , to mount it as a network block device.

We can mount it with this this command:

```
# "test" corresponds to the configuration section in daemon  config file
sudo nbd-client -N test  127.0.0.1 10809  /dev/nbd0
# my Centos 6 version of nbd-client needs a slightly different synatx:
#    sudo nbd-client -N test  127.0.0.1   /dev/nbd0
```

Now we have created a virtual block device, called /dev/nbd0. Now we can format it like it was a normal one:

```
# format device
sudo mkfs /dev/nbd0
 
# create folder for mounting
sudo mkdir /mnt/nbd
 
# mount device, sync option is important to not allow the kernel to cheat!
sudo mount -o sync /dev/nbd0 /mnt/nbd
 
# add write permissions to everyone
sudo chmod a+rwx /mnt/nbd
```

Not that we have passed to mount command the flag -o sync. This command has an important function: to disable an enhancement in the linux Kernel that delays the completion of write operations to the devices. Without that all the write operations will look like instantaneous, and the kernel will actually complete the write requests in background. With this flag instead, all the operation will wait until the operation has really completed.

You can check that now you are able to read and write on the mount point /mnt/nbd.

Let’s now temporarily unmount and disconnect from nbd-server:

```
sudo umount /mnt/nbd
 
sudo nbd-client -d /dev/nbd0
```

And let’s introduce trickle.

Trickle is a software you can use to wrap other processes and to limit their networking bandwidth.

You can use it to limit any other program. A simple test you can perform with it is to use it with curl:

```
# download a sample file and limits download speed to 50 KB/s
trickle -d 50 -u 50  curl -O  http://download.thinkbroadband.com/5MB.zip
```

Now, as you can expect, we just need to join trickle and nbd-server behavior, to obtain the desired behavior.

Let’s start stopping current nbd-server daemon to free up its default port

```
sudo systemctl stop nbd-server.service
```

And let’s start it via trickle:

```
# start nbd-server limiting its network throughput
trickle -d 20 -u 20 -v nbd-server -d
```

-d attaches the server process to the console, so the console will be blocked and it will be freed only one you close the process or when a client disconnects.
Ignore the error message: trickle: Could not reach trickled, working independently: No such file or directory

Now you can re-issue the commands to connect to nbd-server and re mount it:

```
sudo nbd-client -N test  127.0.0.1 10809  /dev/nbd0
 
sudo mount -o sync /dev/nbd0 /mnt/nbd
```

And you are done! Now you have a slow hardisk mounted on /dev/nbd0.

You can verify the slow behavior in this way:

```
sudo dd if=/dev/nbd0 of=/dev/null bs=65536 skip=100 count=10
10+0 records in
10+0 records out
655360 bytes (655 kB) copied, 18.8038 s, 34.9 kB/s
 
# when run against an nbd-server that doesn't use trickle the output is:
# 655360 bytes (655 kB) copied, 0.000723881 s, 905 MB/s
```

Now that you have a slow partition, you can just put the files of your sw there to simulate a slow i/o.

All the above steps can be converted to helper scripts that will make the process much simpler like those described here: http://philtortoise.blogspot.it/2013/09/simulating-slow-drive.html.




</t>
<t tx="jonathanhudson.20201006084250.272"></t>
<t tx="jonathanhudson.20201006084250.273">readelf -d  /opt/nfast/toolkits/hwcrhk/libnfhwcrhk.so | grep SONAME

nm -D /opt/nfast/toolkits/hwcrhk/libnfhwcrhk.so</t>
<t tx="jonathanhudson.20201006084250.274"></t>
<t tx="jonathanhudson.20201006084250.275">Reducing load caused by Rsync
=============================

We're currently utilising rsync to take daily snapshots of our servers and backup to remote locations, lately this has been trigger high load notifications quite frequently. In order to reduce the load caused by these scripts I have gone with three small changes:

Rsync --whole-file
------------------

Rsync by default on local copies will just copy the whole file, but to reduce network traffic remote syncs are diff'd which greatly increases the CPU load. In our case we have our servers connected to each other on a gigabit port so network transfers are going to be fast, so we can force whole-file transfers and reduce the CPU load e.g.
	
rsync -av --whole-file -e ssh {source} {destination}

Weaker SSH Cipher
-----------------

We can further reduce CPU load and increase the transfer rate on the remote transfers by using a weaker SSH encryption cipher. After some reading around and checking out benchmarks I decided to go with 'arcfour'
	
rsync -av --whole-file -e ssh -c arcfour {source} {destination}

Lower execution priority
------------------------

FInally, I change the execution priority of the cronjobs so they run on spare processing power and should never take priority over processes - the backups are important but we can't have them causing delays in service. To do this we use 'nice' e.g.
	
nice -n 19 {command here}

So now my cronjobs look like this:
	
0 6 * * *   /usr/bin/nice -n 19 /root/scripts/backup.sh &gt; /dev/null 2&gt;&amp;1</t>
<t tx="jonathanhudson.20201006084250.276">Rsync copies the source(s) to the destination. If you pass *.pdf as sources, the shell expands this to the list of files with the .pdf extension in the current directory. No recursive traversal happens because you didn't pass any directory as a source.

So you need to run rsync -a ~/LaTeX/ ~/Output/, but with a filter to tell rsync to copy .pdf files only. Rsync's filter rules can seem daunting when you read the manual, but you can construct many examples with just a few simple rules.

Inclusions and exclusions:
	Excluding files by name or by location is easy: --exclude=*~, --exclude=/some/relative/location (relative to the source argument, e.g. this excludes ~/LaTeX/some/relative/location).
	If you only want to match a few files or locations, include them, include every directory leading to them (for example with --include=*/), then exclude the rest with --exclude='*'. This is because:
	If you exclude a directory, this excludes everything below it. The excluded files won't be considered at all.
	If you include a directory, this doesn't automatically include its contents. In recent versions, --include='directory/***' will do that.
	For each file, the first matching rule applies (and anything never matched is included).

Patterns:
	If a pattern doesn't contain a /, it applies to the file name sans directory.
	If a pattern ends with /, it applies to directories only.
	If a pattern starts with /, it applies to the whole path from the directory that was passed as an argument to rsync.
	* any substring of a single directory component (i.e. never matches /); ** matches any path substring.

If a source argument ends with a /, its contents are copied (rsync -r a/ b creates b/foo for every a/foo). Otherwise the directory itself is copied (rsync -r a b creates b/a).

Thus here we need to include *.pdf, include directories containing them, and exclude everything else.

Note that the --include='*/' is fundamental

rsync -a --include='*.pdf' --include='*/' --exclude='*' ~/LaTeX/ ~/Output/

Note that this copies all directories, even the ones that contain no matching file or subdirectory containing one. This can be avoided with the --prune-empty-dirs option (it's not a universal solution since you then can't copy a directory even by matching it explicitly, but that's a rare requirement).

rsync -am --include='*.pdf' --include='*/' --exclude='*' ~/LaTeX/ ~/Output/

</t>
<t tx="jonathanhudson.20201006084250.277">@language md

#### Copy Directory Structure but Skip Files
Rsync allows you to transfer only directory structure if you do not need the files at another location.

To do so, add -f"+ */" -f"- *" before the source directory.

For example, to copy the structure of the Linux directory to Documents, enter:

```
rsync -av -f"+ */" -f"- *"  /home/test/Desktop/Linux /home/test/Documents
```

#### Add Date Stamp to Directory Name

You can easily add a date to a directory name if you want to put a date stamp to your transfers.

Append $(date +\\%Y-\\%m-\\%d) to the destination directory name you want to create. This option is useful when you want to keep track of when transfers took place without opening directory properties.

For example:

```
rsync -av /home/test/Desktop/Linux /home/test/Desktop/rsync$(date +\\%Y-\\%m-\\%d)
```</t>
<t tx="jonathanhudson.20201006084250.278"></t>
<t tx="jonathanhudson.20201006084250.279">Display CXV in columns

cat tecmint-authors.txt
 
pos|author|articles|comments
1|ravisaive|431|9785
2|aaronkili|369|7894
3|avishek|194|2349
4|cezarmatei|172|3256
5|gacanepa|165|2378
6|marintodorov|44|144
7|babin lonston|40|457
8|hannyhelal|30|367
9|gunjit kher|20|156
10|jesseafolabi|12|89 

cat tecmint-authors.txt  | column -t -s "|"


mount | column -t

mount | column -t &gt;mount.out
</t>
<t tx="jonathanhudson.20201006084250.28">---
- hosts: all
  tasks:
    - shell: echo "hello world"
- hosts: linux
  tasks:
    - shell: echo "hello linux"
</t>
<t tx="jonathanhudson.20201006084250.280"># script =&gt; ./generate-files.sh
for i in $(seq 1 $NUMBER)
do 
	dd if=/dev/urandom of=$TARGET/file_$i bs=$SIZE count=$COUNT 2&gt;&amp;1 | grep -v records
done



#### Creating 10240 files of 100k 
export NUMBER=10240
export COUNT=1
export TARGET=`pwd`/100k
export SIZE=100K
./generate-files.sh &gt; 100k.log   

#### Creating 1024 files of 1M 
export NUMBER=1024
export COUNT=1
export TARGET=`pwd`/1M
export SIZE=1M
./generate-files.sh &gt; 1M.log   

#### Creating 100 files of 10M 
export NUMBER=100
export COUNT=1
export TARGET=`pwd`/10M
export SIZE=10M
./generate-files.sh &gt; 10M.log   

#### Creating 10 files of 100M 
export NUMBER=10
export COUNT=100
export TARGET=`pwd`/100M
export SIZE=1M
./generate-files.sh &gt; 100M.log   

#### Creating 1 file of 1G 
export NUMBER=1
export TARGET=`pwd`/1G
export SIZE=1M
export COUNT=1024
./generate-files.sh &gt; 1G.log  
</t>
<t tx="jonathanhudson.20201006084250.281">echo "Comparing ALL"

diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2) &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2)

diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2) &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2)

echo "Only the DIFFS"

diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | cut -f1 -d" ") &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | cut -f1 -d" ")
diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | cut -f1 -d" ") &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | cut -f1 -d" ")

diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | sed 's/ .*\// /') &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao1/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | sed 's/ .*\// /')
diff &lt;(find /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | sed 's/ .*\// /') &lt;(find /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ -type f -exec md5sum {} + | sort -k 2 | sed 's/ .*\// /')

git diff --no-index /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/ /mnt/SAFE_DATA_ML3/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2018-12-14/

</t>
<t tx="jonathanhudson.20201006084250.282">diff -r dir1 dir2 | grep dir1 | awk '{print $4}' &gt; difference1.txt

diff -r setup-server-5.1.0.9-SNAPSHOT setup-server-5.1.0.9-SNAPSHOT-Copy | grep setup-server-5.1.0.9-SNAPSHOT | awk '{print $4}'
</t>
<t tx="jonathanhudson.20201006084250.283">cat /dev/null &gt; filename</t>
<t tx="jonathanhudson.20201006084250.284">ir1repbkp0001p0
ir1regcap0001p1
ir1regcap0001p2


while true; do ps aux | grep python | grep gofer; sleep 5; done</t>
<t tx="jonathanhudson.20201006084250.285">
watch -n 'some command'

watch -n 5 ls -l

watch -n 5 --differences ls -l</t>
<t tx="jonathanhudson.20201006084250.286"># Special Characters

:set list 
:set nolist 

%s/abc/def/g



:4,$-2d				# delete leaving first 3 and last 2





</t>
<t tx="jonathanhudson.20201006084250.287"></t>
<t tx="jonathanhudson.20201006084250.288">@language md

#### Notes

Check how much swap space each process is using

#### Script

```
#!/bin/bash
# Get current swap usage for all running processes
SUM=0
OVERALL=0
for DIR in `find /proc/ -maxdepth 1 -type d -regex "^/proc/[0-9]+"`
do
    PID=`echo $DIR | cut -d / -f 3`
    PROGNAME=`ps -p $PID -o comm --no-headers`
    for SWAP in `grep VmSwap $DIR/status 2&gt;/dev/null | awk '{ print $2 }'`
    do
        let SUM=$SUM+$SWAP
    done
    if (( $SUM &gt; 0 )); then
        echo "PID=$PID swapped $SUM KB ($PROGNAME)"
    fi
    let OVERALL=$OVERALL+$SUM
    SUM=0
done
echo "Overall swap used: $OVERALL KB"
```</t>
<t tx="jonathanhudson.20201006084250.289">@language md

#### Notes

Setup the password for user

```
smbpasswd -a jonathanhudson
```

Then mount

```
\\10.224.36.38\secured\
```

#### Configs

```
[root@OB1-001122 samba]# cat smb.conf
# See smb.conf.example for a more detailed config file or
# read the smb.conf manpage.
# Run 'testparm' to verify the config is correct after
# you modified it.

[global]
        workgroup = RPS
        security = user
        server role = standalone
        passdb backend = tdbsam

        create mask = 0664
        directory mask = 0775

        unix extensions = no
        follow symlinks = yes
        wide links = yes

[homes]
        comment = Home Directories
        valid users = %S, %D%w%S
        browseable = No
        read only = No
        inherit acls = Yes

[secured]
        path = /data
        valid users = jonathanhudson
        browsable = No
        writable = Yes
        read only = No
        inherit acls = Yes
```
</t>
<t tx="jonathanhudson.20201006084250.29">---
- hosts: linux 
  vars:
    delivery: '5.0.35.5-SNAPSHOT'  
    users:
      - { name: 'safe-mon-test', groups: 'd3s,safe-mon-test', gid: '1222', uid: '1222', symlink: 'd3s' }
  tasks:
  - name: Test Priv elevation and create user 
    user: name=safe-mon-test comment="safe-mon-test account" state=present uid=1222 group=d3s groups=d3s home=/data/home/safe-mon-test
    become: yes
    become_user: root
    become_method: dzdo
    
  - name: Switch to safe-mon-test user orig
    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }} &amp;&amp; echo ' delivery' {{delivery}}"    
    register: response
    become: yes
    become_user: safe-mon-test
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"
</t>
<t tx="jonathanhudson.20201006084250.290"></t>
<t tx="jonathanhudson.20201006084250.291">#!/bin/bash

sed -i -e 's/\r$//' &lt;scripts.sh&gt;</t>
<t tx="jonathanhudson.20201006084250.292">
Delete first 42 lines in file

sed -i 1,42d bigfile.txt


sed -i 1,315234d error-bg-bushsrv-2010-01-01.log

sed -i 1950,300028d error-bg-bushsrv-2010-01-01.log
</t>
<t tx="jonathanhudson.20201006084250.293"></t>
<t tx="jonathanhudson.20201006084250.294">@language md
### Leo Install
```
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org PyQt5
yum install qt.x86_64
yum install qt5-qtbase-devel
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org PyQt
yum install libxkbcommon.x86_64
ldd /usr/lib64/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so | grep 'not found'
yum install libxkbcommon-x11.x86_64
ldd /usr/lib64/python3.6/site-packages/PyQt5/Qt/plugins/platforms/libqxcb.so | grep 'not found'
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org PyEnchant
pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org markdown
```
</t>
<t tx="jonathanhudson.20201006084250.295">@language md
### Tech Lead
```
mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=1287771594,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Regulatory\ Platform\ Services\ Technical\ Lead /mnt/windows
```
### Junior
```
mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=1287771594,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Junior\ Regulatory\ Platform\ Services\ Developer /mnt/junior
```
### Safe Data RO
```
mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=1287771594,gid=1101,iocharset=utf8 //fs0001p/SAFE_DATA /mnt/SAFE_DATA_RO
```</t>
<t tx="jonathanhudson.20201006084250.296"></t>
<t tx="jonathanhudson.20201006084250.297">Add the users to the 'd3s' group by editing the /etc/group file and add manually

uat_akgwkwsg_sa - Games - 1013
uat_uppdsqss_sa - Sports - 1007

Extra SSH config would look something like this - 

#Subsystem       sftp    /usr/libexec/openssh/sftp-server
Subsystem       sftp    internal-sftp

Match User uat_akgwkwsg_sa
	ForceCommand internal-sftp -l INFO
	AllowTcpForwarding no
	ChrootDirectory /data/home/uat_akgwkwsg_sa/d3s-es/storage/1013
	Banner "This service is monitored; all commands and transactions are logged."

Match User uat_uppdsqss_sa
	ForceCommand internal-sftp -l INFO
	AllowTcpForwarding no
	ChrootDirectory /data/home/uat_uppdsqss_sa/d3s-es/storage/1007
	Banner "This service is monitored; all commands and transactions are logged."
		
Match User uat_test_sa
	ForceCommand internal-sftp -l INFO
	AllowTcpForwarding no
	ChrootDirectory /data/home/uat_test_sa/d3s-es/storage/Dictao1
	Banner "This service is monitored; all commands and transactions are logged."
		
		
		
an in-process sftp server that requires no support files when used with ChrootDirectory.


uat_uppdsqss_sa
uat_test_sa


service sshd restart


ForceCommand internal-sftp -l INFO

Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: Accepted password for uat_akgwkwsg_sa from 10.225.36.17 port 51316 ssh2
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: pam_unix(sshd:session): session opened for user uat_akgwkwsg_sa by (uid=0)
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: fatal: bad ownership or modes for chroot directory component "/mnt/SAFE_DATA/" [postauth]
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: pam_unix(sshd:session): session closed for user uat_akgwkwsg_sa

d3s-es:x:1105:1001:D3S User Spain:/data/home/d3s-es:/bin/bash


sshd has a certain level of paranoia when it comes to chroot directories. I do not think this can be disabled (even with StrictModes no). The chroot directory and all parent directories must be properly set:
1.The chroot directory and all of its parents must not have group or world write capabilities (ie chmod 755)
2.The chroot directory and all of its parents must be owned by root.

In your case the login error can be fixed with chmod 755 /home/DUMP Your apparent intent to have a world-writable directory that sftpuser can log into and everyone can put files in can be solved by making that directory a subdirectory of /home/DUMP/

/data/home/uat_akgwkwsg_sa
/data/home/uat_uppdsqss_sa
/data/home/uat_akgwkwsg_sa/d3s-es/storage/1013
/data/home/uat_uppdsqss_sa/d3s-es/storage/1007
</t>
<t tx="jonathanhudson.20201006084250.298"># Chroot Work around Spain
# Note

You have to make the home folder and parent folder owned by root
You can add a child folder that has ug+rwX

See example below

sudo chown root /home/bob
sudo chmod go-w /home/bob
sudo mkdir /home/bob/writable
sudo chown bob:sftponly /home/bob/writable
sudo chmod ug+rwX /home/bob/writable

# ---------------------------------------------------------------------------------
# prd_test_sa

Users home : /data/home/prd_test_sa

# Bind
mkdir /data/home/prd_test_sa/CNJ
chown d3s-es:d3s /data/home/prd_test_sa/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/Dictao1/CNJ /data/home/prd_test_sa/CNJ


# ---------------------------------------------------------------------------------
# sftp_akgwkwsg

Users home : /data/home/sftp_akgwkwsg

# Bind
mkdir /data/home/sftp_akgwkwsg/CNJ
chown d3s-es:d3s /data/home/sftp_akgwkwsg/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/b365/CNJ /data/home/sftp_akgwkwsg/CNJ

# ---------------------------------------------------------------------------------
# sftp_uppdsqss

Users home : /data/home/sftp_uppdsqss

# Bind
mkdir /data/home/sftp_uppdsqss/CNJ
chown d3s-es:d3s /data/home/sftp_uppdsqss/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/b365sports/CNJ /data/home/sftp_uppdsqss/CNJ

</t>
<t tx="jonathanhudson.20201006084250.299">Add Users and set Group
=======================

Note: You have to make the home folder and parent folder owned by root. You can add a child folder that has ug+rwX ( using mount --bind we can map CNJ into the users home folder CNJ )

username=prd_test_sa
project_group=d3s
groupadd ${username} -g 1121
useradd -d /data/home/${username} -g ${project_group} -G  ${username} ${username} -u 1121
chmod g+rwx /data/home/${username}

usermod -a -G d3s prd_test_sa
usermod -a -G d3s sftp_akgwkwsg
usermod -a -G d3s sftp_uppdsqss


Ensure Users are in Groups
==========================

Add the users to the 'd3s' group by editing the /etc/group file and add manually

sftp_akgwkwsg - Games  - 1013
u9YXdneZtfD4rvV

sftp_uppdsqss - Sports - 1007
mQRvaiEvoUSVz5Y

prd_test_sa - Dictao1 - Dictao
Cd5TDS3M4q1!


Update the sshd config
======================

Extra SSH config would look something like this - 

# override default of no subsystems
#Subsystem      sftp    /usr/libexec/openssh/sftp-server
Subsystem       sftp    internal-sftp

# Example of overriding settings on a per-user basis
#Match User anoncvs
#       X11Forwarding no
#       AllowTcpForwarding no
#       PermitTTY no
#       ForceCommand cvs server

Match User sftp_akgwkwsg
        ForceCommand internal-sftp -l INFO -R
        AllowTcpForwarding no
        ChrootDirectory /data/home/sftp_akgwkwsg
        Banner "This service is monitored; all commands and transactions are logged."

Match User sftp_uppdsqss
        ForceCommand internal-sftp -l INFO -R
        AllowTcpForwarding no
        ChrootDirectory /data/home/sftp_uppdsqss
        Banner "This service is monitored; all commands and transactions are logged."

Match User prd_test_sa
        ForceCommand internal-sftp -l INFO -R
        AllowTcpForwarding no
        ChrootDirectory /data/home/prd_test_sa
        Banner "This service is monitored; all commands and transactions are logged."
		
		
Now restart sshd
================

service sshd restart

Now Bind user subfolder CNJ to d3s-es storage folder CNJ for tenant
===================================================================

# prd_test_sa

Users home : /data/home/prd_test_sa

mkdir /data/home/prd_test_sa/CNJ
chown d3s-es:d3s /data/home/prd_test_sa/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/Dictao1/CNJ /data/home/prd_test_sa/CNJ

# sftp_akgwkwsg

Users home : /data/home/sftp_akgwkwsg

mkdir /data/home/sftp_akgwkwsg/CNJ
chown d3s-es:d3s /data/home/sftp_akgwkwsg/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/b365/CNJ /data/home/sftp_akgwkwsg/CNJ

# sftp_uppdsqss

Users home : /data/home/sftp_uppdsqss

mkdir /data/home/sftp_uppdsqss/CNJ
chown d3s-es:d3s /data/home/sftp_uppdsqss/CNJ
mount --bind /mnt/SAFE_DATA/d3s-es/storage/b365sports/CNJ /data/home/sftp_uppdsqss/CNJ

vi /etc/fstab
/mnt/SAFE_DATA/d3s-es/storage/Dictao1/CNJ /data/home/prd_test_sa/CNJ            none    bind
/mnt/SAFE_DATA/d3s-es/storage/b365/CNJ /data/home/sftp_akgwkwsg/CNJ             none    bind
/mnt/SAFE_DATA/d3s-es/storage/b365sports/CNJ /data/home/sftp_uppdsqss/CNJ       none    bind


Test Access with Keys
=====================

sftp sftp_akgwkwsg@acc-sec-es.rp.bet365.com
sftp sftp_uppdsqss@acc-sec-es.rp.bet365.com

sftp sftp_akgwkwsg@md1regrep0001p1
pwd: u9YXdneZtfD4rvV
pha: HTZOelxPZVkVMwu

sftp sftp_uppdsqss@md1regrep0001p1
pwd: mQRvaiEvoUSVz5Y
pha: k8lwAEvCSyHUY5d


Info Folder Ownership
=====================

an in-process sftp server that requires no support files when used with ChrootDirectory.

Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: Accepted password for uat_akgwkwsg_sa from 10.225.36.17 port 51316 ssh2
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: pam_unix(sshd:session): session opened for user uat_akgwkwsg_sa by (uid=0)
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: fatal: bad ownership or modes for chroot directory component "/mnt/SAFE_DATA/" [postauth]
Sep 12 10:00:48 mn2regrep0001u1 sshd[31052]: pam_unix(sshd:session): session closed for user uat_akgwkwsg_sa

d3s-es:x:1105:1001:D3S User Spain:/data/home/d3s-es:/bin/bash

sshd has a certain level of paranoia when it comes to chroot directories. I do not think this can be disabled (even with StrictModes no). The chroot directory and all parent directories must be properly set:
1.The chroot directory and all of its parents must not have group or world write capabilities (ie chmod 755)
2.The chroot directory and all of its parents must be owned by root.

In your case the login error can be fixed with chmod 755 /home/DUMP Your apparent intent to have a world-writable directory that sftpuser can log into and everyone can put files in can be solved by making that directory a subdirectory of /home/DUMP/

/data/home/sftp_akgwkwsg
/data/home/sftp_uppdsqss
/data/home/sftp_akgwkwsg/d3s-es/storage/1013
/data/home/sftp_uppdsqss/d3s-es/storage/1007
</t>
<t tx="jonathanhudson.20201006084250.3"></t>
<t tx="jonathanhudson.20201006084250.30">---
- hosts: linux
  remote_user: root
  vars:
    env: 'dev'
    sourceDrive: '/cygdrive/c'
    users:
      - { name: 'safe-mon', groups: 'd3s,safe-mon', gid: '1111', uid: '1111' }
  tasks:  
  - file: path=/var/log/ansible state=touch
  
  - name: log execution
    lineinfile: dest=/var/log/ansible line='{{ ansible_date_time.date }} install {{ item.name }} systemd complete' insertafter='EOF'
    become: yes
    become_user: root
    become_method: dzdo
    with_items: "{{users}}"
    
  - name: update ansible log
    shell: echo "ANSIBLE | {{ ansible_date_time.iso8601 }} | root | install {{ item.name }} systemd complete" &gt;&gt; /var/log/ansible    
    become: yes
    become_user: root
    become_method: dzdo
    with_items: "{{users}}"
</t>
<t tx="jonathanhudson.20201006084250.300">@language md

Keep Alive

```
ssh -o ServerAliveInterval=60 mn2regcap0001d1
```</t>
<t tx="jonathanhudson.20201006084250.301">@language md

#### Notes

https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/

```
tmux
```
#### detach
```
C-b d
```
#### list
```
tmux ls
```
#### attach
```
tmux attach -t 0
tmux attach -t database
```
#### creates a new session called database
```
tmux new -s database   
```
#### rename
```
tmux rename-session -t 0 database # rename a session
```
</t>
<t tx="jonathanhudson.20201006084250.302">@language md

#### Notes

List files in folder with username and permissions


```
tree -pufid
```


Show space on Brick

```
tree --du -h /data/bricks/dk-brick
```
</t>
<t tx="jonathanhudson.20201006084250.303"></t>
<t tx="jonathanhudson.20201006084250.304">
project_group=d3s
username=uat_test_sa

groupadd ${project_group} -g 1001
groupadd ${username} -g 1121

useradd -d /data/home/${username} -g ${project_group} -G  ${username} ${username} -u 1121

chmod g+rwx /data/home/${username}</t>
<t tx="jonathanhudson.20201006084250.305">project_group=d3s
username=perf-mon

#groupadd ${project_group} -g 1001
groupadd ${username} -g 1112

useradd -d /data/home/${username} -g ${project_group} -G  ${username} ${username} -u 1112

chmod g+rwx /data/home/${username}

</t>
<t tx="jonathanhudson.20201006084250.306"></t>
<t tx="jonathanhudson.20201006084250.307">@nocolor
cd /data/code/safe-spain

mvn clean clover:setup verify clover:log clover:aggregate clover:clover -Dmaven.clover.includeFailedTestCoverage=true -Dclover.logging.level=debug -Dclover.logging.adapter=log4j -f pom.xml

#### Setup a config 

Right click on "cog" icon and select the "m" to create a new run configuration

```
clover:setup verify clover:log clover:aggregate clover:clover -Dmaven.clover.includeFailedTestCoverage=true -Dclover.logging.level=debug -Dclover.logging.adapter=log4j -f pom.xml
```
</t>
<t tx="jonathanhudson.20201006084250.308">@nocolor
export JAVA_HOME=/usr/java/jdk1.8.0_162
export M2_HOME=/usr/maven/apache-maven-3.3.9
export PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH

mvn install -pl safe-test-main -DskipTests 


mn2regbls0001d0

/data/home/jenkins/workspace/safe-integration/integration/safe-spain

/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home



/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1//CNJ/Dictao/RU/Diario/RUD/Dictao_Dictao1_RU_RUD_D_20111124_f77e2241-5be4-4454-bbc1-eaf23473971f.zip
/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1//CNJ/Dictao/RU/Mensual/RUD/Dictao_Dictao1_RU_RUD_M_201111_8b94e432-83be-4c49-b5b7-be2f322cc091.zip
/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1//CNJ/Dictao/RU/Diario/RUD/Dictao_Dictao1_RU_RUD_D_20111124_eea11c1f-babd-4808-b3be-e1c738bc9fd8.zip
/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1//CNJ/Dictao/RU/Diario/RUT/Dictao_Dictao1_RU_RUT_D_20111124_94a939e8-0992-48cc-9da7-c2b9477835e2.zip
/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1//CNJ/Dictao/RU/Diario/RUT/Dictao_Dictao1_RU_RUT_D_20111124_a6b68313-2fdb-4d7b-9e2a-dc3a85e157bf.zip



til.concurrent.CountDownLatch@77303fea[Count = 0]
2018-09-10 04:40:17,568 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] Msg Player-Registration-Detail-Replace : - isUnique( false ) - Checking
2018-09-10 04:40:17,572 DEBUG pool-2-recidemp-db-4:  [com.dictao.d3s.jel.server.safe.recidemp.jpa.JpaRecordIdempotenceCache] Tenant Dictao1: inserted entity id 'IdempotenceCacheId [cacheName=Dictao1, key=cb7bUDl
nz23QX4s/Pb+mWpMkTKLPTpnvIUmsSExxREROOATSSYRhJ0TFvLbM1MJR9SoEUFvPrDH8eWhUdKKiiA==]', localRef 'RUD[1/1] Player-Registration-Detail-Replace'
2018-09-10 04:40:17,572 DEBUG pool-2-recidemp-db-4:  [com.dictao.d3s.jel.server.safe.recidemp.jpa.JpaRecordIdempotenceCache] Tenant Dictao1: successfully retrieved cached entry from record idempotence cache in 2
 milliseconds.
2018-09-10 04:40:17,572 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] Digest Info ( 819, cb7bUDlnz23QX4s/Pb+mWpMkTKLPTpnvIUmsSExxREROOATSSYRhJ0T
FvLbM1MJR9SoEUFvPrDH8eWhUdKKiiA== )
2018-09-10 04:40:17,572 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] Local  RecordRef ( RUD[1/1] Player-Registration-Detail-Replace )
2018-09-10 04:40:17,572 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] Msg Player-Registration-Detail-Replace : - isUnique( true ) - Enable Proce
ssing
2018-09-10 04:40:17,572 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] BatchMsg Relay : RUD[1/1] Player-Registration-Detail-Replace
2018-09-10 04:40:17,572 DEBUG RMI TCP Connection(2)-10.15.35.63:  [com.dictao.d3s.jel.es.server.manager.batch.CheckUniqueBatchHandlerEs] BatchMsh - isUnique : ( true )


[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.602 s &lt;&lt;&lt; FAILURE! - in com.bet365.rps.tests.smoke.CasinoRouletteESSpec
[ERROR] Casino Roulette Test(com.bet365.rps.tests.smoke.CasinoRouletteESSpec)  Time elapsed: 2.601 s  &lt;&lt;&lt; ERROR!
java.lang.RuntimeException: Failed to locate tenant data
	at com.bet365.rps.tests.smoke.CasinoRouletteESSpec.Casino Roulette Test_closure7(CasinoRouletteESSpec.groovy:90)
	at com.bet365.rps.tests.smoke.CasinoRouletteESSpec.Casino Roulette Test(CasinoRouletteESSpec.groovy:73)
	

Init Response : 44f2b196-565f-453a-9d06-4ccc76cad8b4
Replace Response : 5bbc8960-2160-4e5f-8f6a-7a632184a355
Cancel Response : 0b965a6a-da64-49bb-9e8b-8a2bdd00fbda
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 12.512 s &lt;&lt;&lt; FAILURE! - in com.bet365.rps.tests.smoke.CasinoBlackjackESSpec
[ERROR] Casino Blackjack Test(com.bet365.rps.tests.smoke.CasinoBlackjackESSpec)  Time elapsed: 12.511 s  &lt;&lt;&lt; ERROR!
java.lang.RuntimeException: Failed to locate tenant data
	at com.bet365.rps.tests.smoke.CasinoBlackjackESSpec.Casino Blackjack Test_closure7(CasinoBlackjackESSpec.groovy:88)
	at com.bet365.rps.tests.smoke.CasinoBlackjackESSpec.Casino Blackjack Test(CasinoBlackjackESSpec.groovy:71)	
	
/data/code/safe-spain/custom/jel/setup/setup-server/target/d3s_home/data/safe/Dictao1/CNJ/Dictao/JU/20180910/JUT/PUN	

/data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-test-framework/safe-test-main/target/test-classes

 mn2regbls0001d0
 
 Running on mn2regbls0001d0 in /data/home/jenkins/workspace/safe-integration/integration/safe-spain
 
 /data/home/jenkins/workspace/safe-integration/integration/safe-spain/safe-test-framework
 
</t>
<t tx="jonathanhudson.20201006084250.309">@nocolor
export JAVA_HOME=/usr/java/jdk1.8.0_162
export M2_HOME=/usr/maven/apache-maven-3.3.9
export PATH=$JAVA_HOME/bin:$M2_HOME/bin:$PATH

cd /data/home/jenkins/workspace/safe-integration/integration/safe-spain

mvn -f safe-test-framework/pom.xml clean install -DskipITs=true -Dcargo.maven.skip=true -Dskip=true
                                  
mvn -f safe-test-framework/safe-test-es/pom.xml clean verify -P local
</t>
<t tx="jonathanhudson.20201006084250.31">---
- hosts: linux 
  vars:
    delivery: '5.0.35.5-SNAPSHOT'  
    users:
      - { name: 'safe-mon-test', groups: 'd3s,safe-mon-test', gid: '1222', uid: '1222', symlink: 'd3s' }
  tasks:
  - name: test priv elevation and create user 
    user: name=safe-mon-test comment="safe-mon-test account" state=present uid=1222 group=d3s groups=d3s home=/data/home/safe-mon-test
    become: yes
    become_user: root
    become_method: dzdo

  - name: set group permissions
    file: path=/data/home/{{ item.name }} state=directory mode=775
    with_items: "{{users}}"
    
  - name: switch to safe-mon-test user
    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }} &amp;&amp; echo ' delivery' {{delivery}}"    
    register: response
    become: yes
    become_user: "{{ item.name }}" 
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"
  
  - name: set group permissions
    file: path=/data/home/{{ item.name }} state=directory mode=775
    with_items: "{{users}}"
  
  - name: adding JAVA_HOME to the .bashrc file
    lineinfile: dest=/data/home/{{ item.name }}/.bashrc line='export JAVA_HOME=/usr/java/jdk1.8.0_111' insertafter='EOF' regexp='export JAVA_HOME=' state=present
    with_items: "{{users}}"    
  
  - name: adding CATALINA_HOME to the .bashrc file
    lineinfile: dest=/data/home/{{ item.name }}/.bashrc line='export CATALINA_HOME=/usr/catalina/apache-tomcat-7.0.73' insertafter='EOF' regexp='export CATALINA_HOME=' state=present
    with_items: "{{users}}"        
  
  - name: adding umask to the .bashrc file
    lineinfile: dest=/data/home/{{ item.name }}/.bashrc line='umask 0002' insertafter='EOF' regexp='umask 0002' state=present
    with_items: "{{users}}"        
  
  - name: adding JAVA to the path in the .bashrc file
    lineinfile: dest=/data/home/{{ item.name }}/.bashrc line='export PATH="$JAVA_HOME/bin:$CATALINA_HOME/bin:$PATH"' insertafter='EOF' regexp='export PATH="\$JAVA_HOME/bin:\$CATALINA_HOME/bin:\$PATH"' state=present
    with_items: "{{users}}"            
  
#  - name: Remove user safe-mon-test 
#    user: name=safe-mon-test state=absent remove=yes
#    become: yes
#    become_user: root
#    become_method: dzdo
</t>
<t tx="jonathanhudson.20201006084250.310">@language md

#### Deploy 3rd Party Jar

```
mvn deploy:deploy-file -DgroupId=com.symantec -DartifactId=protection-engine -Dversion=7.9.1.1 -Dpackaging=jar -Dfile=&lt;path-to-file&gt; -DrepositoryId=nexus
  -Durl=http://mn2regblm0001d0:7002/repository/3rd_party/
```
</t>
<t tx="jonathanhudson.20201006084250.311"></t>
<t tx="jonathanhudson.20201006084250.312">@language md
```
pg_dump -U safeuser -p 5432 -d safedb -W -F t &gt; C:\postgres\dumps\safe-master-dmp.tar

pg_restore -U safeuser -p 5433 -d safedb -c -v C:\postgres\dumps\safe-master-dmp.tar
```
### From Standby side connecting to the Master ( data folder needs to be empty so take backup of config files first )
```
pg_basebackup -U replication -h localhost -p 5432 -D C:\postgres\pgsql-slave-backup\data --xlog-method=stream --checkpoint=fast -R

select pg_last_xlog_receive_location() "receive_location", pg_last_xlog_replay_location() "replay_location", pg_is_in_recovery() "recovery_status";
```</t>
<t tx="jonathanhudson.20201006084250.313">exportDate=`date -dtoday +%d%m%Y`

pg_dump -Fc -d safedb -t dk_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-dk-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t bg_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-bg-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t es_idempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-es-idemp-export-${exportDate}.dat

tar czvPf /data/backup/idemp-exports/${exportDate}/prd-idemp-backup-${exportDate}.tar.gz /data/backup/idemp-exports/${exportDate}/*.dat

# Restpre data to UAT

## Truncate tables

psql -d safedb -c "truncate table bg_recidempcacheentry;"

## Restore Data

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry prd-bg-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry prd-dk-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t es_idempcacheentry prd-es-idemp-export-19062018.dat 2&gt;&amp;1 &amp;


# DK Data
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t dk_recidempcacheentry -D

./perform-pg-repack.sh dk_recidempcacheentry

# BG Data
SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t bg_recidempcacheentry -D

./perform-pg-repack.sh bg_recidempcacheentry


# Create table containing deletes

create table bg_recidempcacheentry_deleted as select * from bg_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );
create table dk_recidempcacheentry_deleted as select * from dk_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );

# Export table for backup

exportDate=`date -dtoday +%d%m%Y`
pg_dump -Fc -d safedb -t bg_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-bg-deleted-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t dk_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-dk-deleted-idemp-export-${exportDate}.dat

# Restore deleted records if deleted table no longer exists

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry uat-bg-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry uat-dk-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;

# Restore from deleted table 
-- psql -d safedb -c 'copy ( select * from bg_recidempcacheentry_deleted ) to stdout' | psql -d safedb -c 'copy bg_recidempcacheentry from stdin'

insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;
insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;

# SQL Commands

psql -d safedb -c "select count(*) from bg_recidempcacheentry;"
</t>
<t tx="jonathanhudson.20201006084250.314">


[postgres@mn2regpdg0001u0 scripts]$ ./perform-pg-repack.sh bg_recidempcacheentry
Perform pg_repack on ( bg_recidempcacheentry ) running : 2018-06-20 14:46:54

Executing pg_repack

INFO: repacking table "bg_recidempcacheentry"
Call complete no errors detected
Perform pg_repack completed : 2018-06-20 15:02:12

</t>
<t tx="jonathanhudson.20201006084250.315">#!/bin/bash 

echo "Restore Deleted Data"

nohup psql -d safedb -c "insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;

nohup psql -d safedb -c "insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;

echo "Completed"
</t>
<t tx="jonathanhudson.20201006084250.316">exportDate=`date -dtoday +%d%m%Y`

pg_dump -Fc -d safedb -t dk_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-dk-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t bg_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-bg-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t es_idempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-es-idemp-export-${exportDate}.dat

tar czvPf /data/backup/idemp-exports/${exportDate}/prd-idemp-backup-${exportDate}.tar.gz /data/backup/idemp-exports/${exportDate}/*.dat

# Restpre data to UAT

## Truncate tables

psql -d safedb -c "truncate table bg_recidempcacheentry;"

## Restore Data

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry prd-bg-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry prd-dk-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t es_idempcacheentry prd-es-idemp-export-19062018.dat 2&gt;&amp;1 &amp;


# DK Data
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t dk_recidempcacheentry -D

./perform-pg-repack.sh dk_recidempcacheentry

# BG Data
SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t bg_recidempcacheentry -D

./perform-pg-repack.sh bg_recidempcacheentry


# Create table containing deletes

create table bg_recidempcacheentry_deleted as select * from bg_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );
create table dk_recidempcacheentry_deleted as select * from dk_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );

# Export table for backup

exportDate=`date -dtoday +%d%m%Y`
pg_dump -Fc -d safedb -t bg_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-bg-deleted-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t dk_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-dk-deleted-idemp-export-${exportDate}.dat

# Restore deleted records if deleted table no longer exists

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry uat-bg-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry uat-dk-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;

# Restore from deleted table 
-- psql -d safedb -c 'copy ( select * from bg_recidempcacheentry_deleted ) to stdout' | psql -d safedb -c 'copy bg_recidempcacheentry from stdin'

insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;
insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;

# SQL Commands

psql -d safedb -c "select count(*) from bg_recidempcacheentry;"

psql -d safedb -c "insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;"
psql -d safedb -c "insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;"

nohup psql -d safedb -c "insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;
nohup psql -d safedb -c "insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;

delete from bg_recidempcacheentry t1 using bg_recidempcacheentry_deleted t2 where t1.key = t2.key;

nohup psql -d safedb -c "delete from bg_recidempcacheentry t1 using bg_recidempcacheentry_deleted t2 where t1.key = t2.key;" 2&gt;&amp;1 &amp;
nohup psql -d safedb -c "delete from dk_recidempcacheentry t1 using dk_recidempcacheentry_deleted t2 where t1.key = t2.key;" 2&gt;&amp;1 &amp;
</t>
<t tx="jonathanhudson.20201006084250.317">exportDate=`date -dtoday +%d%m%Y`

pg_dump -Fc -d safedb -t dk_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-dk-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t bg_recidempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-bg-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t es_idempcacheentry &gt; /data/backup/idemp-exports/${exportDate}/prd-es-idemp-export-${exportDate}.dat

tar czvPf /data/backup/idemp-exports/${exportDate}/prd-idemp-backup-${exportDate}.tar.gz /data/backup/idemp-exports/${exportDate}/*.dat

# Restpre data to UAT

## Truncate tables

psql -d safedb -c "truncate table bg_recidempcacheentry;"

## Restore Data

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry prd-bg-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry prd-dk-idemp-export-19062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t es_idempcacheentry prd-es-idemp-export-19062018.dat 2&gt;&amp;1 &amp;


# DK Data
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM DK_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t dk_recidempcacheentry -D

./perform-pg-repack.sh dk_recidempcacheentry

# BG Data
SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );
delete FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );

psql -d safedb -c "DELETE FROM BG_RECIDEMPCACHEENTRY where ENTRYTIME &lt; ( CURRENT_TIMESTAMP -  INTERVAL '60 day' );"
--pg_repack -d safedb -t bg_recidempcacheentry -D

./perform-pg-repack.sh bg_recidempcacheentry


# Create table containing deletes

create table bg_recidempcacheentry_deleted as select * from bg_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );
create table dk_recidempcacheentry_deleted as select * from dk_recidempcacheentry where entrytime &lt; ( current_timestamp -  interval '60 day' );

# Export table for backup

exportDate=`date -dtoday +%d%m%Y`
pg_dump -Fc -d safedb -t bg_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-bg-deleted-idemp-export-${exportDate}.dat
pg_dump -Fc -d safedb -t dk_recidempcacheentry_deleted &gt; /data/backup/idemp-exports/${exportDate}/uat-dk-deleted-idemp-export-${exportDate}.dat

# Restore deleted records if deleted table no longer exists

nohup pg_restore -Fc -d safedb -j 4 -t bg_recidempcacheentry uat-bg-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;
nohup pg_restore -Fc -d safedb -j 4 -t dk_recidempcacheentry uat-dk-deleted-idemp-export-20062018.dat 2&gt;&amp;1 &amp;

# Restore from deleted table 
-- psql -d safedb -c 'copy ( select * from bg_recidempcacheentry_deleted ) to stdout' | psql -d safedb -c 'copy bg_recidempcacheentry from stdin'

insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;
insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;

# SQL Commands

psql -d safedb -c "select count(*) from bg_recidempcacheentry;"

psql -d safedb -c "insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;"
psql -d safedb -c "insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;"

nohup psql -d safedb -c "insert into bg_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from bg_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;
nohup psql -d safedb -c "insert into dk_recidempcacheentry ( entrytime, lastmodified, localref, cachename, key ) select entrytime, lastmodified, localref, cachename, key from dk_recidempcacheentry_deleted;" 2&gt;&amp;1 &amp;

delete from bg_recidempcacheentry t1 using bg_recidempcacheentry_deleted t2 where t1.key = t2.key;

nohup psql -d safedb -c "delete from bg_recidempcacheentry t1 using bg_recidempcacheentry_deleted t2 where t1.key = t2.key;" 2&gt;&amp;1 &amp;
nohup psql -d safedb -c "delete from dk_recidempcacheentry t1 using dk_recidempcacheentry_deleted t2 where t1.key = t2.key;" 2&gt;&amp;1 &amp;
</t>
<t tx="jonathanhudson.20201006084250.318">dev sft : xxx


pg_dump -U sftuser -h mn2regpog0001d0 -p 5432 -d sftdb -W -F t &gt; /var/lib/pgsql/dumps/sft-dev-db-dmp.tar

pg_restore -U sftuser -h localhost -p 5432 -d sftdb -c -v /var/lib/pgsql/dumps/sft-dev-db-dmp.tar



uat sft : xxx


pg_dump -U sftuser -h mn2regpdg0001u0 -p 5432 -d sftdb -W -F t &gt; /var/lib/pgsql/dumps/sft-dev-db-dmp.tar

pg_restore -U sftuser -h localhost -p 5432 -d sftdb -c -v /var/lib/pgsql/dumps/sft-dev-db-dmp.tar


prd sft : xxx

pg_dump -U sftuser -h ir1-sftdb.lb.local -p 5432 -d sftdb -W -F t &gt; /var/lib/pgsql/dumps/sft-prd-db-dmp.tar

pg_restore -U sftuser -h localhost -p 5432 -d sftdb -c -v /var/lib/pgsql/dumps/sft-prd-db-dmp.tar
</t>
<t tx="jonathanhudson.20201006084250.319">cd $PDATA

pg_controldata $PGDATA

ls -altr  

00000003000000050000002A

pg_archivecleanup -n $PGDATA/pg_xlog 000000030000000600000098
pg_archivecleanup $PGDATA/pg_xlog 000000030000000600000098
du -h /data

ls -altr  
pg_archivecleanup -n $PGDATA/pg_xlog 000000030000000300000070
pg_archivecleanup $PGDATA/pg_xlog 000000030000000300000070




latest checkpoint

0000000300000003000000C3



0000000300000006000000D1





pg_controldata $PGDATA

00000003000000870000001B

pg_archivecleanup -n $PGDATA/pg_xlog 00000003000000870000001B
pg_archivecleanup $PGDATA/pg_xlog 00000003000000870000001B
</t>
<t tx="jonathanhudson.20201006084250.32">---
- hosts: rtl_d3s
  vars:
    users:
      - { name: 'root' }
  tasks:
  - name: Switch to test user
    shell: "whoami &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
</t>
<t tx="jonathanhudson.20201006084250.320"># Manual Installation of Binaries ---------------------------------------------------------------------------------------------------------------------------

cp pg_repack.so /usr/local/pgsql/lib/
cp pg_repack /usr/local/pgsql/bin/
cp pg_repack.control /usr/local/pgsql/share/extension/
cp pg_repack--1.4.2.sql /usr/local/pgsql/share/extension/

su - postgres

psql -c "CREATE EXTENSION pg_repack" -d safedb

# Compile from source ----------------------------------------------------------------------------------------------------------------------------------------

http://reorg.github.io/pg_repack/

yum install postgresql96-devel.x86_64    // Not required on dev
yum groupinstall "Development tools"
yum -y install openssl-devel
yum install readline-devel

make
make install

su - postgres
psql -d safedb -c "DROP EXTENSION pg_repack CASCADE"
psql -c "CREATE EXTENSION pg_repack" -d safedb

# You can remove pg_repack using DROP EXTENSION pg_repack or just dropping the repack schema. ----------------------------------------------------------------

/usr/pgsql-9.6/bin/pg_repack

export PATH=/usr/pgsql-9.6/bin:$PATH

psql -U safeuser -d safedb -a -f show-table-sizes.sql

pg_repack -a safedb

psql -U safeuser -d safedb -a -f show-table-sizes.sql

# Yum Repo Used ( ob1-001122 ) -------------------------------------------------------------------------------------------------------------------------------

[root@OB1-001122 ~]# cd /etc/yum.repos.d/
[root@OB1-001122 yum.repos.d]# ls -al
total 48
drwxr-xr-x.   2 root root 4096 Nov  7 13:06 .
drwxr-xr-x. 148 root root 8192 Feb 13 09:58 ..
-rw-r--r--.   1 root root 1664 Dec  9  2015 CentOS-Base.repo
-rw-r--r--.   1 root root 1309 Dec  9  2015 CentOS-CR.repo
-rw-r--r--.   1 root root  649 Dec  9  2015 CentOS-Debuginfo.repo
-rw-r--r--.   1 root root  290 Dec  9  2015 CentOS-fasttrack.repo
-rw-r--r--.   1 root root  630 Dec  9  2015 CentOS-Media.repo
-rw-r--r--.   1 root root 1331 Dec  9  2015 CentOS-Sources.repo
-rw-r--r--.   1 root root 1952 Dec  9  2015 CentOS-Vault.repo
-rw-r--r--.   1 root root 1012 Sep 20  2016 pgdg-96-centos.repo
[root@OB1-001122 yum.repos.d]# cat pgdg-96-centos.repo
[pgdg96]
name=PostgreSQL 9.6 $releasever - $basearch
baseurl=https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-$releasever-$basearch
enabled=1
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-96

[pgdg96-source]
name=PostgreSQL 9.6 $releasever - $basearch - Source
failovermethod=priority
baseurl=https://download.postgresql.org/pub/repos/yum/srpms/9.6/redhat/rhel-$releasever-$basearch
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-96

[pgdg96-updates-testing]
name=PostgreSQL 9.6 $releasever - $basearch
baseurl=https://download.postgresql.org/pub/repos/yum/testing/9.6/redhat/rhel-$releasever-$basearch
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-96

[pgdg96-source-updates-testing]
name=PostgreSQL 9.6 $releasever - $basearch - Source
failovermethod=priority
baseurl=https://download.postgresql.org/pub/repos/yum/srpms/testing/9.6/redhat/rhel-$releasever-$basearch
enabled=0
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-PGDG-96





</t>
<t tx="jonathanhudson.20201006084250.321">/data/code/pg_squeeze-REL1_0_1_PG_9_6

     22 Installation
     23 ------------
     24 
     25 1. Set PG_CONFIG envoirnment variable to point to pg_config command of your
     26    PostgreSQL installation.
     27 
     28 2. make
     29 
     30 3. sudo make install
     31 
     32 4. Apply the following settings to postgresql.conf:
     33 
     34    wal_level = logical
     35 
     36    max_replication_slots = 1 # ... or add 1 to the current value.
     37 
     38    shared_preload_libraries = 'pg_squeeze' # ... or add the library to the existing ones.
     39 
     40 5. Start the PG cluster.
     41 
     42 6. As a superuser, run
     43 
     44    CREATE EXTENSION pg_squeeze;
     45
     46 Note: If you want remove the extension from particular database, make sure the
     47 database has no "squeeze worker" (see "Enable / disable table processing"
     48 section) running. Otherwise the DROP EXTENSION command will hang until it's
     49 cancelled.
     50 



/usr/bin/mkdir -p '/usr/pgsql-9.6/lib'
/usr/bin/mkdir -p '/usr/pgsql-9.6/share/extension'
/usr/bin/mkdir -p '/usr/pgsql-9.6/share/extension'
/usr/bin/install -c -m 755  pg_squeeze.so '/usr/pgsql-9.6/lib/pg_squeeze.so'
/usr/bin/install -c -m 644 .//pg_squeeze.control '/usr/pgsql-9.6/share/extension/'
/usr/bin/install -c -m 644 .//pg_squeeze--1.0.sql  '/usr/pgsql-9.6/share/extension/'
[root@OB1-001122 pg_squeeze-REL1_0_1_PG_9_6]# 


</t>
<t tx="jonathanhudson.20201006084250.322">
psql -U postgres

createdb -U postgres safedb

psql -U postgres -d safedb

CREATE USER safeuser PASSWORD 'password';

GRANT ALL PRIVILEGES ON DATABASE safedb TO safeuser;

SELECT * FROM pg_catalog.pg_tables where tableowner = 'safeuser';



#### With utf-8 on Windows

psql -Upostgres


&gt; CREATE USER safeuser PASSWORD 'password';

&gt; CREATE DATABASE "safedb" WITH OWNER "safeuser" ENCODING 'UTF8' LC_COLLATE = 'en-GB' LC_CTYPE = 'en-GB' TEMPLATE template0;

&gt; GRANT ALL PRIVILEGES ON DATABASE safedb TO safeuser;

&gt; SELECT * FROM pg_catalog.pg_tables where tableowner = 'safeuser';</t>
<t tx="jonathanhudson.20201006084250.323">@language md

- Elevate to postgres and amend the postgresql.conf file
```
cd $PGDATA
vim postgresql.conf
```

- edit the line setting synchronous_commit and save:
```
synchronous_commit = off
:wq!
```

- Reload config
```
pg_ctl reload
```</t>
<t tx="jonathanhudson.20201006084250.324">

usermod -a -G postgres safe-mon

chmod 640 /var/log/pgsql/postgresql-*

service safe-mon restart



su - postgres
pg_ctl stop
pg_ctl start -l /var/log/pgsql/startup.log

</t>
<t tx="jonathanhudson.20201006084250.325">Database Failover Testing

[x] 1.	Shutdown  IR2001 and test all should be ok
[x] 2.	Startup IR2001 and test all should be ok
[x] 3.	Shutdown IR1002 and test all should be ok
[x] 4.	Startup IR1002 and test all should be ok
[x] 5.	Switch over Master to a IR1002 and then update the VIP and test all should be ok
[x] 6.  Perform smoke test from BCM to ensure connectivity to new Master database
[x] 7.	Switch back Master from Slave and then update the VIP and test all should be ok
[x] 8.	Switch over Master to IR2001 and then Shutdown a slave leaving new Master and 1 remaining slave and test all should be ok
[x] 9.	Switch back Master from Slave and start a Slave and test all should be ok


select * from public.dk_recidempcacheentry where entrytime &gt; to_timestamp('12-06-2017 06:00', 'DD-MM-YYYY HH24:MI') order by entrytime desc;

</t>
<t tx="jonathanhudson.20201006084250.326">

./pg365monitor.sh -a ir1regpdg0001p0 -b ir1regpdg0002p0 -c ir2regpdg0001p0

ssh -q postgres@ir1regpdg0002p0 "/usr/bin/rsync /data/pgsql/data/pg_xlog/* ir2regpdg0001p0:/data/pgsql/data/pg_xlog"

vim $PGDATA/postgresql.conf
pg_ctl reload
./pg365.sh -c ir1regpdg0001p0 -s ir1regpdg0002p0 -o ir2regpdg0001p0

vim $PGDATA/postgresql.conf
pg_ctl reload
./pg365.sh -c ir1regpdg0002p0 -s ir1regpdg0001p0 -o ir2regpdg0001p0

</t>
<t tx="jonathanhudson.20201006084250.327">&lt;!-- https://mvnrepository.com/artifact/org.postgresql/postgresql --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.postgresql&lt;/groupId&gt;
    &lt;artifactId&gt;postgresql&lt;/artifactId&gt;
    &lt;version&gt;42.1.4&lt;/version&gt;
&lt;/dependency&gt;
</t>
<t tx="jonathanhudson.20201006084250.328">



psql -d safedb -c "VACUUM (VERBOSE, ANALYZE) DK_RECIDEMPCACHEENTRY;"

psql -d safedb -c "VACUUM (VERBOSE, ANALYZE) BG_RECIDEMPCACHEENTRY;"

</t>
<t tx="jonathanhudson.20201006084250.329">Disk full on IR1 postgres node (slave).
 
Postgres had stopped and was unable to start due to no disk space.
 
The $PGDATA\pg_xlog folder was over 1TB (much larger than the gigs on MN2 (master)).
 
Ran the command
 
pg_controldata $PGDATA 
 
to find the latest checkpoint's REDO WAL file 
 
then ran a clean up with the file name as list only:
 
pg_archivecleanup -n $PGDATA/pg_xlog 0000000500000F3C000000C3
 
then without the -n to actually run:
 
pg_archivecleanup $PGDATA/pg_xlog 0000000500000F3C000000C3
 
took a few minutes to complete.
 
Then started up postgres
 
pg_ctl start -l /var/log/pgsql/start.log
 
Checked replication status (the state first showed as "catching up" before changing to the normal "streaming" value) on the MN2 node:
 
SELECT pid, usename, application_name, client_addr::text, client_port, backend_start, state, sync_priority, sync_state, sent_location::text, write_location::text, 
flush_location::text, replay_location::text FROM pg_stat_replication WHERE pid &lt;&gt; pg_backend_pid();

SELECT * FROM pg_replication_slots;
</t>
<t tx="jonathanhudson.20201006084250.33">---
- hosts: linux
  vars:
    users:
      - { name: 'd3s-es', delivery: 'dictao-d3s-jel-setup-server', module: 'dictao-d3s-jel-setup-server', symlink: 'd3s', component: 'd3s', target: 'OB1-001122' }
      - { name: 'd3s-bg-nraproxy', delivery: 'dictao-d3s-jel-setup-bg-nraproxy', module: 'dictao-d3s-jel-setup-bg-nraproxy', symlink: 'd3s-nraproxy', component: 'nraproxy', target: 'XYZ-001122' }
      - { name: 'd3s-bg-pushsrv', delivery: 'dictao-d3s-jel-setup-bg-pushsrv', module: 'dictao-d3s-jel-setup-bg-pushsrv', symlink: 'd3s-pushsrv', component: 'pushsrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-nramocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-nramocksrv', component: 'nramocksrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-opermocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-opermocksrv', component: 'opermocksrv', target: 'XYZ-001122' }      
    jurisdiction: 'es'
    configFile: []
  tasks:
  - name: check configure.properties file
    local_action: stat path=/cygdrive/c/data/code/safe-configs/Spain/LCL/{{jurisdiction}}-d3s-configure.properties    
    register: configCheck
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: show configCheck
    debug: msg={{ configCheck }}
    
  - name: filter users
    debug: msg={{ users | selectattr( 'symlink', 'equalto', 'd3s' ) | list }}
#    with_items: '{{ users | map( attribute='target' ) | list | unique }}'

  - name: filter configCheck with stat that exists
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}  

  - name: filter configCheck with stat that exists and filter on path
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | selectattr( 'stat.path', 'search', 'configure.properties' ) | list }}  
    
  - name: set source config missing
    set_fact: configFile="{{ item }}"
    with_items: "{{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}"
    
  - name: dump configFile
    debug: msg={{ configFile }}
    
#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties-1 dest=/cygdrive/c/temp/es-d3s-configure.properties-1
#    when: not configFile.results[0].stat.exists and item.target in ansible_hostname

#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties dest=/cygdrive/c/temp/es-d3s-configure.properties
#    when: configFile.results[0].stat.exists and item.target in ansible_hostname

</t>
<t tx="jonathanhudson.20201006084250.330">Postgres Commands

psql

psql -t -c "SELECT COUNT(*) FROM pg_stat_replication WHERE pid &lt;&gt; pg_backend_pid();"

</t>
<t tx="jonathanhudson.20201006084250.331">pg_ctl start -l /var/log/pgsql/startup.log
pg_ctl stop
pg_ctl status
pg_ctl reload</t>
<t tx="jonathanhudson.20201006084250.332"># Postgres Installed on DEV mn2regpog0001d0




   48  gcc
   49  gmake
   50  cd /usr/local/
   51  gunzip /tmp/postgresql-9.6.1.tar.gz
   52  tar xf /tmp/postgresql-9.6.1.tar
   53  cd postgresql-9.6.1
   54  ./configure -without-readline
   55  gmake
   56  groupadd postgres
   57  adduser -g postgres postgres
   58  passwd postgres

   59  dzdo su - postgres
   60  gmake install
   61  mkdir /data/pgsql
   62  mkdir /var/log/pgsql
   63  chown postgres:postgres /data/pgsql
   64  chown postgres:postgres /var/log/pgsql
   65  dzdo su - postgres
   66  su -c '/usr/local/pgsql/bin/pg_ctl start -D /var/lib/pgsql/data -l /var/log/pgsql/startup.log ' postgres
   67  dzdo su - postgres
   68  cd /usr/local/postgresql-9.6.1/contrib/hstore
   69  make install
   70  dzdo su - postgres
   71  mkdir /data/pg365
   72  cp /tmp/pg365*.sh /data/pg365/
   73  cd /tmp
   74  ls
   75  chown postgres:postgres /data/pg365
   76  chown postgres:postgres /data/pg365/pg*.sh
   77  cd /data
   78  ls -lrt
   79  chmod 777 /data/pg365
   80  ls -lrt
   81  cp /tmp/pg365*.sh /data/pg365/
   82  pwd
   83  cd pg365
   84  ls l
   85  ls-lrt
   86  ls -lrt
   87   cp /tmp/pg365*.sh /data/pg365/
   88  cd /tmp
   89  ls -lrt
   90  cd /data/pg365
   91  ls -lrt
   92  chown postgres:postgres /data/pg365/pg*.sh
   93  chmod 777 /data/pg365/pg*.sh
   94  exit

</t>
<t tx="jonathanhudson.20201006084250.333">10.36.63.4




IR1

10.32.114.16
10.32.114.17

10.32.114.19 
10.32.114.18

10.32.114.20
10.32.114.21

IR2

10.36.114.15
10.36.114.16

10.36.114.17
10.36.114.18

10.36.114.19
10.36.114.20



host    all             all             10.32.114.16/32          md5
host    all             all             10.32.114.17/32          md5
host    all             all             10.32.114.24/32          md5
host    all             all             10.32.63.142/32          md5
host    all             all             10.32.63.250/32          md5
host    all             all             10.36.63.250/32          md5
host    all             all             10.32.63.10/32           md5
host    all             all             10.36.63.4/32            md5



pg_ctl start -l /var/log/pgsql/startup.log
pg_ctl stop
pg_ctl status
pg_ctl reload</t>
<t tx="jonathanhudson.20201006084250.334"># Using db-purge

cd /data/code/safe-tools/db-purge/target

java -jar db-purge-1.0-RELEASE-full.jar -pt dk_recidempcacheentry -rd 30 -pu lcl-idempotence

# SQL Statements

--select count(*)from public.bg_recidempcacheentry;

--select count(*) from public.bg_idempcacheentry where cachename in ( 'b365', 'b365sports' );
--select count(*) from public.bg_recidempcacheentry where cachename in ( 'b365', 'b365sports' );

--select min(lastmodified) from public.dk_recidempcacheentry where cachename = 'b365';

--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('07-08-2017 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000;

--SELECT distinct entrytime FROM BG_RECIDEMPCACHEENTRY;
--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY;
--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where entrytime between to_timestamp('20-05-2017 00:00', 'DD-MM-YYYY HH24:MI') and to_timestamp('07-08-2017 23:59', 'DD-MM-YYYY HH24:MI'); 
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where entrytime between to_timestamp('01-01-2018 23:59', 'DD-MM-YYYY HH24:MI') and to_timestamp('10-01-2018 23:59', 'DD-MM-YYYY HH24:MI'); 
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where entrytime between to_timestamp('01-01-2018 23:59', 'DD-MM-YYYY HH24:MI') and to_timestamp('20-01-2018 23:59', 'DD-MM-YYYY HH24:MI');

delete from DK_RECIDEMPCACHEENTRY where entrytime between to_timestamp('01-01-2018 23:59', 'DD-MM-YYYY HH24:MI') and to_timestamp('20-01-2018 23:59', 'DD-MM-YYYY HH24:MI');

--DELETE FROM BG_RECIDEMPCACHEENTRY WHERE EXISTS ( SELECT * FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('10-01-2018 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000 );
--SELECT FROM BG_RECIDEMPCACHEENTRY WHERE EXISTS ( SELECT * FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('07-09-2017 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000 );

delete from DK_RECIDEMPCACHEENTRY where cachename = 'jono365';

# ------------------------------------------------------------------

/usr/pgsql-9.6/bin/pg_repack

export PATH=/usr/pgsql-9.6/bin:$PATH

psql -U safeuser -d safedb -a -f show-table-sizes.sql

pg_repack -a safedb
</t>
<t tx="jonathanhudson.20201006084250.335">--select count(*)from public.bg_recidempcacheentry;

--select count(*) from public.bg_idempcacheentry where cachename in ( 'b365', 'b365sports' );
--select count(*) from public.bg_recidempcacheentry where cachename in ( 'b365', 'b365sports' );

--select min(lastmodified) from public.dk_recidempcacheentry where cachename = 'b365';

--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('07-08-2017 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000;

--SELECT distinct entrytime FROM BG_RECIDEMPCACHEENTRY;
--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY;
--SELECT count(*) FROM BG_RECIDEMPCACHEENTRY where entrytime between to_timestamp('20-05-2017 00:00', 'DD-MM-YYYY HH24:MI') and to_timestamp('07-08-2017 23:59', 'DD-MM-YYYY HH24:MI'); 
SELECT count(*) FROM DK_RECIDEMPCACHEENTRY where entrytime between to_timestamp('01-01-2018 23:59', 'DD-MM-YYYY HH24:MI') and to_timestamp('10-01-2018 23:59', 'DD-MM-YYYY HH24:MI'); 



--DELETE FROM BG_RECIDEMPCACHEENTRY WHERE EXISTS ( SELECT * FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('10-01-2018 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000 );
--SELECT FROM BG_RECIDEMPCACHEENTRY WHERE EXISTS ( SELECT * FROM BG_RECIDEMPCACHEENTRY where entryTime &lt; to_timestamp('07-09-2017 00:00', 'DD-MM-YYYY HH24:MI') LIMIT 100000 );
</t>
<t tx="jonathanhudson.20201006084250.336"></t>
<t tx="jonathanhudson.20201006084250.337">CREATE OR REPLACE FUNCTION insert_gamedata_gr( recordid character varying(255),
											   endtime timestamp, 
                                               entrytime timestamp,
                                               fee numeric(20,10),
											   gametype character varying(255),
									           gametypename character varying(255),
											   jackpot numeric(38,0),
											   nohands integer,
											   playerid character varying(255),
											   sessionid character varying(255),
											   staketype character varying(255),
											   stakes numeric(38,0),
											   starttime timestamp,
											   winnings numeric(38,0)
) RETURNS text AS
$$
DECLARE records_inserted text := 'OK';
BEGIN
    INSERT INTO gamedata_gr ( endtime, entrytime, fee, gametype, gametypename, jackpot, nohands, playerid, sessionid, staketype, stakes, starttime, winnings, recordid ) 
	VALUES ( endtime, entrytime, fee, gametype, gametypename, jackpot, nohands, playerid, sessionid, staketype, stakes, starttime, winnings, recordid );
	
	GET DIAGNOSTICS records_inserted = ROW_COUNT;
	
	RETURN cast ( records_inserted as text );
END
$$
  LANGUAGE 'plpgsql';</t>
<t tx="jonathanhudson.20201006084250.338">ssh -q postgres@ir2regpdg0001p0 "/usr/local/pgsql/bin/pg_ctl status -D $PGDATA | grep 'no server running'"

psql

select * from pg_stat_replication;

-- ALTER SYSTEM SET synchronous_standby_names = '1( slave_node1, slave_node2, slave_node3 )';
SELECT application_name, sync_priority, sync_state FROM pg_stat_replication ORDER BY application_name;

-- SELECT pg_reload_conf();</t>
<t tx="jonathanhudson.20201006084250.339">delete from dk_billing_volume;
delete from dk_eod_aggreg;
delete from dk_eod_aggreg_line;
delete from dk_eod_ctrl;
delete from dk_eod_src;
delete from dk_idempcacheentry;
delete from dk_player_0
delete from dk_player_1;
delete from dk_player_2;
delete from dk_player_3;
delete from dk_player_4;
delete from dk_player_5;
delete from dk_player_6;
delete from dk_recidempcacheentry;
</t>
<t tx="jonathanhudson.20201006084250.34">---
- hosts: linux
  vars:
    users:
      - { name: 'd3s-es', delivery: 'dictao-d3s-jel-setup-server', module: 'dictao-d3s-jel-setup-server', symlink: 'd3s', component: 'd3s', target: 'OB1-001122' }
      - { name: 'd3s-bg-nraproxy', delivery: 'dictao-d3s-jel-setup-bg-nraproxy', module: 'dictao-d3s-jel-setup-bg-nraproxy', symlink: 'd3s-nraproxy', component: 'nraproxy', target: 'XYZ-001122' }
      - { name: 'd3s-bg-pushsrv', delivery: 'dictao-d3s-jel-setup-bg-pushsrv', module: 'dictao-d3s-jel-setup-bg-pushsrv', symlink: 'd3s-pushsrv', component: 'pushsrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-nramocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-nramocksrv', component: 'nramocksrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-opermocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-opermocksrv', component: 'opermocksrv', target: 'XYZ-001122' }      
    jurisdiction: 'es'
  tasks:
  - name: check configure.properties file
    local_action: stat path=/cygdrive/c/data/code/safe-configs/Spain/LCL/{{jurisdiction}}-d3s-configure.properties    
    register: configFile
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: filter stat results
    set_fact: configFile="{{ item }}"
    with_items: "{{ configFile.results | selectattr( 'stat', 'defined' ) | list }}"
    
  - name: show configFile
    debug: msg={{ configFile }}

  - name: copy configure.properties for component to remote symlink
    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties-1 dest=/cygdrive/c/temp/es-d3s-configure.properties-1
    with_items: '{{users}}'
    when: not configFile.stat.exists and item.target in ansible_hostname

  - name: copy configure.properties for component to remote symlink
    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties dest=/cygdrive/c/temp/es-d3s-configure.properties
    with_items: '{{users}}'
    when: configFile.stat.exists and item.target in ansible_hostname

    
#  - name: filter users
#    debug: msg={{ users | selectattr( 'symlink', 'equalto', 'd3s' ) | list }}
#    with_items: '{{ users | map( attribute='target' ) | list | unique }}'

#  - name: filter configFile with stat that exists
#    debug: msg={{ configFile.results | selectattr( 'stat', 'defined' ) | list }}  

#  - name: filter configFile with stat that exists and filter on path
#    debug: msg={{ configFile.results | selectattr( 'stat', 'defined' ) | selectattr( 'stat.path', 'search', 'configure.properties' ) | list }}  
    
#  - name: set source config missing
#    set_fact: configFile="{{ item }}"
#    with_items: "{{ configFile.results | selectattr( 'stat', 'defined' ) | list }}"
    
#  - name: dump configFile
#    debug: msg={{ configFile }}
    
</t>
<t tx="jonathanhudson.20201006084250.340">SELECT nspname || '.' || relname AS "relation",
 pg_size_pretty(pg_total_relation_size(C.oid)) AS "total_size"
 FROM pg_class C
 LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
 WHERE nspname NOT IN ('pg_catalog', 'information_schema')
 AND C.relkind &lt;&gt; 'i'
 AND nspname !~ '^pg_toast'
 ORDER BY pg_total_relation_size(C.oid) DESC
 LIMIT 15;</t>
<t tx="jonathanhudson.20201006084250.341">SELECT pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) AS size FROM pg_database;</t>
<t tx="jonathanhudson.20201006084250.342">@nocolor



select u.username, f.fileboxname from usertable u
    join userfilebox uf on u.id = uf.user_id
    join filebox f on uf.filebox_id = f.id
    where u.id 
union
select u.username, f.fileboxname from usertable u
    join user_group ug on u.id = ug.user_id
    join groupfilebox gf on ug.group_id = gf.group_id
    join filebox f on gf.filebox_id = f.id
order by fileboxname;

</t>
<t tx="jonathanhudson.20201006084250.343">@nocolor





SELECT u1 from User u where 'Admin' IN ( SELECT r.roleName FROM User u2 JOIN u.roles r WHERE u1.id= u2.id )

</t>
<t tx="jonathanhudson.20201006084250.344">systemctl restart postgresql-9.6

systemctl stop postgresql-9.6

systemctl start postgresql-9.6

</t>
<t tx="jonathanhudson.20201006084250.345">netstat -tulpn

pg_ctl start -D /usr/local/pgsql/data -l serverlog

pg-ctl start

/data/pgsql/data</t>
<t tx="jonathanhudson.20201006084250.346">VACUUM (VERBOSE, ANALYZE) public.bg_recidempcacheentry;


VACUUM (VERBOSE, ANALYZE) public.dk_eod_aggreg;
VACUUM (VERBOSE, ANALYZE) public.dk_eod_aggreg_line;


--VACUUM (VERBOSE, ANALYZE) public.dk_eod_aggreg;
--VACUUM (VERBOSE, ANALYZE) public.dk_eod_aggreg_line;
--VACUUM (VERBOSE, ANALYZE) public.dk_eod_src;
--VACUUM (VERBOSE, ANALYZE) public.dk_eod_ctrl;

--VACUUM FULL public.dk_recidempcacheentry;

The normal way to check if you are getting the benefit of HOT updates or not is to monitor pg_stat_user_tables and compare the counts for n_tup_upd
(regular updates) versus n_tup_hot_upd.


SELECT
nspname,relname,
round(100 * pg_relation_size(indexrelid) / pg_relation_size(indrelid)) / 100
AS index_ratio,
pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
pg_size_pretty(pg_relation_size(indrelid)) AS table_size
FROM pg_index I
LEFT JOIN pg_class C ON (C.oid = I.indexrelid)
LEFT JOIN pg_namespace N ON (N.oid = C.relnamespace)
WHERE
nspname NOT IN ('pg_catalog', 'information_schema', 'pg_toast') AND
C.relkind='i' AND
pg_relation_size(indrelid) &gt; 0;</t>
<t tx="jonathanhudson.20201006084250.347">INFO:  vacuuming "public.bg_recidempcacheentry"
INFO:  index "bg_recidempcacheentry_pkey" now contains 15006575 row versions in 326790 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.27s/0.34u sec elapsed 0.61 sec.
INFO:  "bg_recidempcacheentry": found 0 removable, 14 nonremovable row versions in 1 out of 492025 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
Skipped 0 pages due to buffer pins.
0 pages are entirely empty.
CPU 0.27s/0.34u sec elapsed 0.62 sec.
INFO:  vacuuming "pg_toast.pg_toast_16957"
INFO:  index "pg_toast_16957_index" now contains 0 row versions in 1 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
INFO:  "pg_toast_16957": found 0 removable, 0 nonremovable row versions in 0 out of 0 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
Skipped 0 pages due to buffer pins.
0 pages are entirely empty.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
INFO:  analyzing "public.bg_recidempcacheentry"
INFO:  "bg_recidempcacheentry": scanned 30000 of 492025 pages, containing 922253 live rows and 0 dead rows; 30000 rows in sample, 15009926 estimated total rows
VACUUM

Query returned successfully in 1 secs.</t>
<t tx="jonathanhudson.20201006084250.348"></t>
<t tx="jonathanhudson.20201006084250.349">@language md

#### Create Files

```
import os

for x in range(1010):
	print( x )
	newFileName = "testFile-{}.txt".format( x )
	newfile = open( newFileName, "ab" )
	newfile.write ( os.urandom( 500000 ) )    # generate 5MB random content file
	newfile.close ()
```</t>
<t tx="jonathanhudson.20201006084250.35">---
- hosts: linux
  tasks:
    - name: Priv Execution Example
      shell: echo "hello world"
      #become: yes
      #become_user: d3s-dk
      #become_method: dzdo
</t>
<t tx="jonathanhudson.20201006084250.350"></t>
<t tx="jonathanhudson.20201006084250.351">@language md

#### Notes
From windows git bash
```
cd /c/temp/check
./checker gi --list --verbose --recordid recids.txt /y/d3s-gi/storage/b365/data/2020/03/
```
and the recids.txt
```
bet365_CAS_20200324_17863
bet365_CAS_20200324_17864
bet365_CAS_20200324_17865
bet365_CAS_20200324_17866
bet365_CAS_20200324_17867
bet365_CAS_20200324_17868
bet365_CAS_20200325_17900
bet365_CAS_20200325_17901
```
</t>
<t tx="jonathanhudson.20201006084250.352"></t>
<t tx="jonathanhudson.20201006084250.353"></t>
<t tx="jonathanhudson.20201006084250.354"></t>
<t tx="jonathanhudson.20201006084250.355">@nocolor
#!/bin/bash
#
# /etc/rc.d/init.d/d3sd -&gt; .../&lt;D3S_HOME&gt;/bin/rhel-etc-init.d-d3sd.sh
#
# D3S service
#
# chkconfig: 2345 97 8
# description: D3S

. /etc/rc.d/init.d/functions

QS="@"
if [ "${QS}d3s.os.user${QS}" = "d3s-dk" ]; then
    echo "ERROR D3S invalid configuration; d3s.os.user configuration key missing."
    exit -1
fi

if [ -z "d3s-dk" ]; then
    echo "ERROR D3S invalid configuration; d3s.os.user configuration key empty."
    exit -1
fi

if [ "`whoami`" != "d3s-dk" ] ; then
    echo "impersonate as d3s-dk"
    exec su - "d3s-dk" "$0" "$@"
fi

dxxadmin="$(dirname $(readlink -f $0))/d3sadmin"

cmd=${1:-help}
case "$cmd" in

    start)
        echo -n "Starting D3S: ";
        ${dxxadmin} start &amp;&amp; success || failure;
        echo;;

    stop)
        echo -n "Stopping D3S: ";
        ${dxxadmin} stop &amp;&amp; success || failure;
        echo;;

    restart)
        echo -n "Restarting D3S: ";
        ${dxxadmin} stop &amp;&amp; ${dxxadmin} start &amp;&amp; success || failure;
        echo;;

    status)
        ${dxxadmin} status;;

    *)
        echo "Usage:  start|stop|status|restart|help";;
esac
</t>
<t tx="jonathanhudson.20201006084250.356">@nocolor

systemctl list-unit-files


chkconfig httpd on

 
service ntpd start
 
chkconfig ntpd on

ntpq -p

ntpdate -u 10.15.69.1</t>
<t tx="jonathanhudson.20201006084250.357">@nocolor
#!/bin/bash
#
# /etc/rc.d/init.d/d3sd -&gt; .../&lt;D3S_HOME&gt;/bin/rhel-etc-init.d-d3sd.sh
#
# D3S service
#
# chkconfig: 2345 97 8
# description: D3S

. /etc/rc.d/init.d/functions

QS="@"
if [ "${QS}d3s.os.user${QS}" = "d3s-dk" ]; then
    echo "ERROR D3S invalid configuration; d3s.os.user configuration key missing."
    exit -1
fi

if [ -z "d3s-dk" ]; then
    echo "ERROR D3S invalid configuration; d3s.os.user configuration key empty."
    exit -1
fi

if [ "`whoami`" != "d3s-dk" ] ; then
    echo "impersonate as d3s-dk"
    exec su - "d3s-dk" "$0" "$@"
fi

dxxadmin="$(dirname $(readlink -f $0))/d3sadmin"

cmd=${1:-help}
case "$cmd" in

    start)
        echo -n "Starting D3S: ";
        ${dxxadmin} start &amp;&amp; success || failure;
        echo;;

    stop)
        echo -n "Stopping D3S: ";
        ${dxxadmin} stop &amp;&amp; success || failure;
        echo;;

    restart)
        echo -n "Restarting D3S: ";
        ${dxxadmin} stop &amp;&amp; ${dxxadmin} start &amp;&amp; success || failure;
        echo;;

    status)
        ${dxxadmin} status;;

    *)
        echo "Usage:  start|stop|status|restart|help";;
esac
</t>
<t tx="jonathanhudson.20201006084250.358"></t>
<t tx="jonathanhudson.20201006084250.359">@nocolor
# Input
# /opt/nfast/kmdata/local (world, module-*)



mkdir /tmp/sources_nfast
cp -a /root/nfast/* /tmp/sources_nfast
cd /
tar -xf /tmp/sources_nfast/hwsp/agg.tar
tar -xf /tmp/sources_nfast/ctls/agg.tar
tar -xf /tmp/sources_nfast/hwcrhk/user.tar
tar -xf /tmp/sources_nfast/pkcs11/user.tar

# Edit install script to avoid perl
vi /opt/nfast/scripts/lib/inst-platform.sh
    #tgr="`id -- $us 2&gt;/dev/null | perl -ne 'print $1 if m/ gid=\d+\(([-a-z]+)\)\s/'`"
    tgr="`id -- $us 2&gt;/dev/null | sed -e 's/.* gid=[0-9]*(\(.*\)) .*/\1/g'`"

    #tgr="`id -- $us | perl -ne 'print $1 if m/ gid=\d+\(([-a-z]+)\)\s/'`"
    tgr="`id -- $us | sed -e 's/.* gid=[0-9]*(\(.*\)) .*/\1/g'`"

	
/opt/nfast/sbin/install
passwd nfast

vi /opt/nfast/cknfastrc
CKNFAST_LOADSHARING=1

echo "nfast   ALL=(root) NOPASSWD: /etc/init.d/nc_hardserver restart" &gt; /etc/sudoers.d/nfast


touch /opt/nfast/.bash_profile
chown nfast:nfast /opt/nfast/.bash_profile
su - nfast
vi .bash_profile
PATH=$PATH:/opt/nfast/bin


source .bash_profile
enquiry
nethsmenroll 172.30.26.81


-bash-4.3$ enquiry
Server:
 enquiry reply flags  none
 enquiry reply level  Six
 serial number
 mode                 operational
 version              2.92.1
 speed index          0
 rec. queue           2..50
 level one flags      none
 version string       2.92.1cam12,
 checked in           00000000487debd5 Wed Jul 16 14:38:45 2008
 level two flags      none
 max. write size      8192
 level three flags    none
 level four flags     ServerHasPollCmds ServerHasLongJobs ServerHasCreateClient
 module type code     0
 product name         nFast server
 device name
 EnquirySix version   4
 impath kx groups
 feature ctrl flags   none
 features enabled     none
 version serial       0
 remote server port   9004
-bash-4.3$ enquiry^C
-bash-4.3$ nethsmenroll 172.30.26.81
Remote module returned ESN: 69E6-06D8-B9D5
                    HKNETI: a2617980b3c7aad42b5315774494798f5440091e
Is the above correct? (yes/no): yes
OK configuring hardserver's nethsm imports
-bash-4.3$


config-serverstartup -s

# List softCards:
/opt/nfast/bin/ppmk
09:54:49 WARNING: Module #1: Module has failed
86f05599f2196f38aee1c3d8554baa936da0edef        dss-softcard
# Your IP is not authorized


# Declare the used slot
echo """name=nCipher-slot1
description=SunPKCS11 accessing nCipher Cryptographic Framework
library=/opt/nfast/toolkits/pkcs11/libcknfast.so
#slot=761406614 
slotListIndex=1""" &gt; /opt/sunpkcs11-ncipher-slot1.cfg

# Create a softCard
/opt/nfast/bin/ppmk -n -m 1 dss-softcard

# Change password, if needed
/opt/nfast/bin/ppmk -C dss-softcard

# List soft card
/opt/nfast/bin/ppmk 

# If native HSM is used (not pkcs11 interface)
# find /opt/nfast/ -name "*.jar"
# cp /opt/nfast/java/classes/nCipherKM.jar /opt/java/jdk1.8/jre/lib/ext/

# Install Java and tomcat now from the common documentation

# Add security provider $JAVA_HOMR/jre/lib/security/java.security
cd /usr/java/jdk1.8.0_111/jre/lib/security
cp java.security java.security.bkp
cp java.security java.security.soft
mv java.security java.security.hsm
ln -s java.security.hsm java.security


#vi java.security.hsm
security.provider.1=sun.security.pkcs11.SunPKCS11 /opt/sunpkcs11-ncipher-slot1.cfg
#security.provider.1=com.ncipher.provider.km.nCipherKM
security.provider.2=sun.security.provider.Sun
security.provider.3=sun.security.rsa.SunRsaSign
security.provider.4=sun.security.ec.SunEC
security.provider.5=com.sun.net.ssl.internal.ssl.Provider
security.provider.6=com.sun.crypto.provider.SunJCE
security.provider.7=sun.security.jgss.SunProvider
security.provider.8=com.sun.security.sasl.Provider
security.provider.9=org.jcp.xml.dsig.internal.dom.XMLDSigRI
security.provider.10=sun.security.smartcardio.SunPCSC


# Should be done, only on the first instance, the second instance should get the same keys as the first one
# cp -r /opt/nfast/kmdata/local/* root@50.50.1.249:/opt/nfast/kmdata/local/

---------- First instance -----
mkdir /root/certs

# Create a signature key inside the HSM   
/opt/nfast/bin/generatekey -g -m 1 pkcs11 protect=softcard softcard=dss-softcard selfcert=no certreq=yes nvram=no recovery=yes type=RSA size=2048 digest=sha256 pubexp=10001 plainname=sig-policy-1 embedsavefile=sig-policy-1 x509country=UK  x509org=BET365 x509orgunit="BET365" x509dnscommon="Bet365 Test Signature key" x509email="" x509locality="" x509province=""

# Please note the Id (uc50e0845399ff722072c95892d86e3eba07eee5d1-8d6c49578be7096a7c04c7f8f036fecb541a854a)

# Create a signature key inside the HSM   
/opt/nfast/bin/generatekey -g -m 1 pkcs11 protect=softcard softcard=dss-softcard selfcert=no certreq=yes nvram=no recovery=yes type=RSA size=2048 digest=sha256 pubexp=10001 plainname=ts-policy-1 embedsavefile=ts-policy-1 x509country=UK  x509org=BET365 x509orgunit="BET365" x509dnscommon="Bet365 Test TimeStamping key" x509email="" x509locality="" x509province=""

# Please note the Id (uc50e0845399ff722072c95892d86e3eba07eee5d1-5e6277271afcd662276ddfbf30bd53d69a244940)

# Signing CSR with AnySignCA
cd S:\DSA\Engineering\Public\Produits-DTS\DictaoDemoCA\DictaoRootCA\data
S:

openssl ca -in C:\Users\hzrari\Documents\workspace\bet365\docs_delivery\certificates\HSM\sig-policy-1_req -notext -out C:\Users\hzrari\Documents\workspace\bet365\docs_delivery\certificates\HSM\sig-policy-1.cer -name CA_all -extensions v3_cert_signer -config "..\original-configs\openssl.cnf" -passin pass:password 

openssl ca -in C:\Users\hzrari\Documents\workspace\bet365\docs_delivery\certificates\HSM\ts-policy-1_req -notext -out C:\Users\hzrari\Documents\workspace\bet365\docs_delivery\certificates\HSM\ts-policy-1.cer -name CA_all -extensions v3_cert_signer -config "..\original-configs\openssl - TimeStamping.cnf" -passin pass:password 




cd /opt/nfast/bin/
# Get the private keyId
./cklist -l sig-policy-1 2&gt;&amp;1| grep CKA_NFKM_ID

# Import the Signature signed certificate
./ckcerttool --certname=sig-policy-1 --cardset=dss-softcard -f /root/certs/sig-policy-1.cer --keyident="uc50e0845399ff722072c95892d86e3eba07eee5d1-8d6c49578be7096a7c04c7f8f036fecb541a854a"


# Import the Timestamping signed certificate
./cklist -l ts-policy-1 2&gt;&amp;1| grep CKA_NFKM_ID
./ckcerttool --certname=ts-policy-1 --cardset=dss-softcard -f /root/certs/ts-policy-1.cer --keyident="uc50e0845399ff722072c95892d86e3eba07eee5d1-5e6277271afcd662276ddfbf30bd53d69a244940"

# Import the CA certificate
./ckcerttool -T --certname=anysign --cardset=dss-softcard -f /root/certs/anysignca.cer

./cklist -l anysign 2&gt;&amp;1| grep CKA_NFKM_ID


------------------ End of first instance -------------



# Create the User/Group
mkdir -p /data/home

project_group=d3s
groupadd ${project_group} -g 501

username=dss
groupadd ${username} -g 506
useradd -d /data/home/${username} -g ${project_group} -G  ${username} ${username} -u 506

# Install the component
username=dss
su - ${username}
 
 
# Add the following lines to your ~/.bashrc

echo '''
export JAVA_HOME=~/3rd_parties/jdk8
export CATALINA_HOME=~/3rd_parties/tomcat7
export PATH="$JAVA_HOME/bin:$PATH"

export delivery="dictao-dss-setup-server"
export symlink="dss"
export bin_admin="dssadmin"
''' &gt;&gt; ~/.bashrc

# Load your bashrc
source ~/.bashrc

# Link the external dependencies

mkdir 3rd_parties
cd 3rd_parties

# Supposing that only one java-8 is in /usr/java
ln -s /usr/java/jdk1.8.0_* jdk8

# Supposing that only one tomcat7 is in /usr/catalina
ln -s /usr/catalina/apache-tomcat-7.* tomcat7

# Install the delivered package
mkdir ~/rd_deliveries
cd ~/rd_deliveries

# Set your version in this variable
version="1.0.0.rc5"

# Place here your archive ${delivery}-${version}
# cp /root/dictao-dss-setup-server-1.0.0.rc5.tar.gz ~dss/rd_deliveries/
# Please set properly the permission, it should be:
# chown dss:d3s ~dss -R
# chmod g+rwx -R ~dss
# su - ${username}

# Extract the archive
cd ~/rd_deliveries
tar -xvzf ${delivery}-${version}.tar.gz
cd ..

# Create a symlik for the current installation 
ln -s rd_deliveries/${delivery}-${version} ${symlink}
ln -s ${symlink}/catalina/logs logs

# Create a copy for the original configuration file before editing it
# The copy will help to recover and/or to update the component version
cp -i ${symlink}/configure.properties ${symlink}/configure.properties.bkp
dos2unix ${symlink}/configure.properties

# Externalize the default certificate
cp -a ~/${symlink}/conf/certificates ~/certificates
mkdir ~/conf
cp -a ~/${symlink}/conf/dss-policy.xml ~/conf

# Create component used tree

# Please apply this patch, it's generated with: diff -U 0 -w ~/${symlink}/configure.properties.bkp ~/${symlink}/configure.properties
echo '''
''' | patch ~/${symlink}/configure.properties


# Set Aliases in p12
keytool -changealias  -keystore ~/certificates/signature/Demo-DSS-Signature.p12 -storetype PKCS12 -storepass password -alias 1 -destalias sig-keystore-1
keytool -changealias  -keystore  ~/certificates/timestamp/Demo-DSS-Timestamp.p12 -storetype PKCS12 -storepass password -alias 1 -destalias tsa-keystore-1

~/${symlink}/bin/dssc cipherallpwd password

# Bug JAVA for Fedora
# http://bugs.java.com/view_bug.do?bug_id=4705093
cp dss/lib/template/catalina/bin/setenv.sh dss/lib/template/catalina/bin/setenv.sh.bkp
# Add a line
# vi ~/dss/lib/template/catalina/bin/setenv.sh 
# CATALINA_OPTS="$CATALINA_OPTS -Djava.security.egd=file:/dev/urandom"


# Force IPv4 /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.eth0.disable_ipv6 = 1

cd conf/
ln -s dss-policy.xml.hsm dss-policy.xml
# To get back to soft :
unlink dss-policy.xml
ln -s dss-policy.xml.soft dss-policy.xml
cd


~/${symlink}/bin/${bin_admin} configure
~/${symlink}/bin/${bin_admin} start

# Wait until monitoring is update (HTTPS)
cd /data/home/dss/certificates/tls
curl https://localhost:38443/dss-signature/mon/supervision/status -k --cert ./Demo-DSS-Client.pem --key ./Demo-DSS-Client_decrypted.key 
cd

curl http://localhost:38080/dss-signature/mon/supervision/ -v

# Add the instance to the LB pool:
curl http://localhost:38080/dss-activator/state/set?force=UP -v
curl http://localhost:38080/dss-activator/state/get -v


# Signature test
cd
~/${symlink}/bin/dssc sign --tid "demo-policy-xades-basic" --reqid "requestid" --tag "tag" --dataToSign "${HOME}/${symlink}/test/sign-in.xml" --out "${HOME}/${symlink}/test/sign-out.xml" --insecure  --signType ENVELOPED -sysprop default

~/${symlink}/bin/dssc sign --tid "demo-policy-xades-basic" --reqid "requestid" --tag "tag" --dataToSign "${HOME}/${symlink}/test/sign-in.xml" --out "${HOME}/${symlink}/test/sign-out.xml" --insecure  --signType DETACHED -sysprop default --signParams "&lt;Parameters&gt;&lt;Manifest&gt;&lt;Reference&gt;&lt;DigestValue&gt;cLwprmuhGjImRMXClcicc3mVM4tk3Ii3gukT98n2Mjc=&lt;/DigestValue&gt;&lt;DigestMethod&gt;SHA256&lt;/DigestMethod&gt;&lt;URI&gt;file://../xades/dataToSign.xml&lt;/URI&gt;&lt;/Reference&gt;&lt;/Manifest&gt;&lt;/Parameters&gt;"

# With HSM
~/${symlink}/bin/dssc sign --tid "demo-policy-xades-t-internal-hsm" --reqid "requestid" --tag "tag" --dataToSign "${HOME}/${symlink}/test/sign-in.xml" --out "${HOME}/${symlink}/test/sign-out.xml" --insecure  --signType ENVELOPED -sysprop default



# Add the package as a service
vi /etc/systemd/system/dssd.service
chmod 664 /etc/systemd/system/dssd.service
systemctl daemon-reload
systemctl enable dssd

---- Service start/stop -----
service dssd start
service dssd stop


-------- Stopping firewall ----------
</t>
<t tx="jonathanhudson.20201006084250.36">---
- hosts: dev_all 
  vars:
    users:
      - { name: 'safe-mon', target: 'mn2reg' }
#      - { name: 'd3s-dk' }
  tasks:
  - name: Switch to safe-mon user orig
    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: dzdo
    with_items: "{{users}}"    
    when: item.target in ansible_hostname

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
</t>
<t tx="jonathanhudson.20201006084250.360">@nocolor
JAVA_OPTS="$JAVA_OPTS -Dcom.sun.xml.ws.transport.http.client.HttpTransportPipe.dump=true"
com.sun.xml.ws.transport.http.HttpAdapter.dump=true

SoapAction          : "http://www.dictao.com/D2S/Interface/signatureEx"
JAX-WS RI 2.1.7-b01-: Stub for https://mn2regsig0001u2:38443/dss-signature/secure/v0/SignatureWebServiceImpl
d2sTransactionId    : CS_JEL_BG
d2sRequestId        : 17e368f3-bef6-45da-b934-6f5ad90b28cd ( uuid )
&lt;Parameters&gt;&lt;Data&gt;&lt;XmlDocument&gt;&lt;/XmlDocument&gt;&lt;/Data&gt;&lt;/Parameters&gt;


data : 

&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;manifest xmlns="http://dictao.com/d3s/jel/bg/bachmanifest/v2014_08.xsd" xmlns:ns2="http://www.w3.org/2000/09/xmldsig#"&gt;
	&lt;ownerName&gt;Dictao1&lt;/ownerName&gt;
	&lt;creationDate&gt;2017-07-11T07:07:56.980Z&lt;/creationDate&gt;
	&lt;batchId&gt;3084ffa5-c7b2-4625-8cbd-116d0b0a5793&lt;/batchId&gt;
	&lt;metadata&gt;
		&lt;recordCount&gt;4&lt;/recordCount&gt;
		&lt;firstRecordDate&gt;2017-07-11T07:07:42.499Z&lt;/firstRecordDate&gt;
		&lt;lastRecordDate&gt;2017-07-11T07:07:55.279Z&lt;/lastRecordDate&gt;
		&lt;sizeUncompressed&gt;3705&lt;/sizeUncompressed&gt;
		&lt;sizeCompressed&gt;2887&lt;/sizeCompressed&gt;
	&lt;/metadata&gt;
	&lt;ns2:Manifest Id="M1"&gt;
		&lt;ns2:Reference URI="file:data/2017/07/11/Dictao1_a_20170711_100756_3084ffa5-c7b2-4625-8cbd-116d0b0a5793.zip/data.zip"&gt;
			&lt;ns2:DigestMethod Algorithm="http://www.w3.org/2001/04/xmlenc#sha256"/&gt;
			&lt;ns2:DigestValue&gt;2AFLu2E3Y254H0MGIT0hvAe2voIs3ZMLLMd0Mye3UXE=&lt;/ns2:DigestValue&gt;
		&lt;/ns2:Reference&gt;
	&lt;/ns2:Manifest&gt;
&lt;/manifest&gt;

&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;S:Envelope xmlns:S="http://schemas.xmlsoap.org/soap/envelope/"&gt;
    &lt;S:Body&gt;
        &lt;signatureEx xmlns="http://www.dictao.com/D2S/Interface"&gt;
            &lt;requestId&gt;18bc0d1a-7112-4cc6-9d00-ece0fd9573de&lt;/requestId&gt;
            &lt;transactionId&gt;CS_JEL_BG&lt;/transactionId&gt;
            &lt;tag/&gt;
            &lt;dataToSign&gt;
                &lt;binaryValue&gt;PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9InllcyI/PjxtYW5pZmVzdCB4bWxucz0iaHR0cDovL2RpY3Rhby5jb20vZDNzL2plbC9iZy9iYWNobWFuaWZlc3QvdjIwMTRfMDgueHNkIiB4bWxuczpuczI9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvMDkveG1sZHNpZyMiPjxvd25lck5hbWU+RGljdGFvMTwvb3duZXJOYW1lPjxjcmVhdGlvbkRhdGU+MjAxNy0wNy0xMVQwNzoyNTo0MC44MzdaPC9jcmVhdGlvbkRhdGU+PGJhdGNoSWQ+Yjk5ODc3NjMtYzlhZC00NDRlLWIzMDUtZTE1M2IzNjkwZjg0PC9iYXRjaElkPjxtZXRhZGF0YT48cmVjb3JkQ291bnQ+NDwvcmVjb3JkQ291bnQ+PGZpcnN0UmVjb3JkRGF0ZT4yMDE3LTA3LTExVDA3OjI1OjMzLjc5MVo8L2ZpcnN0UmVjb3JkRGF0ZT48bGFzdFJlY29yZERhdGU+MjAxNy0wNy0xMVQwNzoyNTozOS4xODNaPC9sYXN0UmVjb3JkRGF0ZT48c2l6ZVVuY29tcHJlc3NlZD4zNzA1PC9zaXplVW5jb21wcmVzc2VkPjxzaXplQ29tcHJlc3NlZD4yODg3PC9zaXplQ29tcHJlc3NlZD48L21ldGFkYXRhPjxuczI6TWFuaWZlc3QgSWQ9Ik0xIj48bnMyOlJlZmVyZW5jZSBVUkk9ImZpbGU6ZGF0YS8yMDE3LzA3LzExL0RpY3RhbzFfYV8yMDE3MDcxMV8xMDI1NDBfYjk5ODc3NjMtYzlhZC00NDRlLWIzMDUtZTE1M2IzNjkwZjg0LnppcC9kYXRhLnppcCI+PG5zMjpEaWdlc3RNZXRob2QgQWxnb3JpdGhtPSJodHRwOi8vd3d3LnczLm9yZy8yMDAxLzA0L3htbGVuYyNzaGEyNTYiLz48bnMyOkRpZ2VzdFZhbHVlPlp2Z2ZVVXlCZE5wckgrWUducXBFVksyNUtLWlFGSGRlZmJTZUxMVlhOT1k9PC9uczI6RGlnZXN0VmFsdWU+PC9uczI6UmVmZXJlbmNlPjwvbnMyOk1hbmlmZXN0PjwvbWFuaWZlc3Q+&lt;/binaryValue&gt;
            &lt;/dataToSign&gt;
            &lt;signatureFormat&gt;XADES&lt;/signatureFormat&gt;
            &lt;signatureType&gt;ENVELOPED&lt;/signatureType&gt;
            &lt;signatureParameter&gt;&amp;lt;Parameters&amp;gt;&amp;lt;Data&amp;gt;&amp;lt;XmlDocument&amp;gt;&amp;lt;/XmlDocument&amp;gt;&amp;lt;/Data&amp;gt;&amp;lt;/Parameters&amp;gt;&lt;/signatureParameter&gt;
        &lt;/signatureEx&gt;
    &lt;/S:Body&gt;
&lt;/S:Envelope&gt;





</t>
<t tx="jonathanhudson.20201006084250.361">@language md

```
cd /mnt/SAFE_DATA
```
### d3s-bg
```
find /mnt/SAFE_DATA/d3s-bg/storage/Dictao*/data/ -type f -mtime +30 -delete
find /mnt/SAFE_DATA/d3s-bg/storage/Dictao*/logs/ -type f -mtime +30 -delete
find /mnt/SAFE_DATA/d3s-bg/storage/Dictao*/errlogs/ -type f -mtime +30 -delete
find /mnt/SAFE_DATA/d3s-bg/storage/Dictao*/receipts/ -type f -mtime +30 -delete
```
### d3s-dk
```
find /mnt/SAFE_DATA/d3s-dk/storage/Dictao*/folderstruktur-spilsystem/Zip/ -type f -mtime +30 -delete
```
### d3s-es
```
find /mnt/SAFE_DATA/d3s-dk/storage/Dictao1//CNJ/Dictao/  -type f -mtime +30 -delete
find /mnt/SAFE_DATA/d3s-dk/storage/Dictao2//CNJ/Dictao2/  -type f -mtime +30 -delete
```

## Recreate Folders

### d3s-bg
```
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao1/logs
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao1/errlogs
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao1/receipts
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao1/data

chmod 775 /mnt/SAFE_DATA/d3s-bg/storage/Dictao1 -R

mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao2/logs
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao2/errlogs
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao2/receipts
mkdir /mnt/SAFE_DATA/d3s-bg/storage/Dictao2/data

chmod 775 /mnt/SAFE_DATA/d3s-bg/storage/Dictao2 -R
```

### d3s-es
```

```</t>
<t tx="jonathanhudson.20201006084250.362"></t>
<t tx="jonathanhudson.20201006084250.363">MN2REGCAP0001U1.UK365OFFICE.CO.UK|1506612596953</t>
<t tx="jonathanhudson.20201006084250.364">@language md

#### safe-scheduler

The Anteriores scheduled task has been moved to the capture servers and configured as a cluster (except RTL).  Either of the instances can run the task and the underlying safe-cache will ensure they don’t run at the same time.

In PRD, BCM and RTL there is a dedicated safe-scheduler instance.  In UAT, UAB and DEV the safe-scheduler is deployed on the safe-mon Tomcat instance.  I have listed the URLs below.  They will also be in the wiki under environments/URLs/safe-scheduler eventually 

There are also some diagrams in the wiki under environments/Network-Diagrams/SafeScheduler-SafeCache that show the task deployment and port configurations.

Let me know if you have any questions.

Neil

```
http://ir1regcap0003p1:23280/safe-scheduler/jsondoc-ui.html?url=http://ir1regcap0003p1:23280/safe-scheduler/jsondoc#
http://ir1regcap0003p2:23280/safe-scheduler/jsondoc-ui.html?url=http://ir1regcap0003p2:23280/safe-scheduler/jsondoc#

http://ir3regcap0003p1:23280/safe-scheduler/jsondoc-ui.html?url=http://ir3regcap0003p1:23280/safe-scheduler/jsondoc#
http://ir3regcap0003p2:23280/safe-scheduler/jsondoc-ui.html?url=http://ir3regcap0003p2:23280/safe-scheduler/jsondoc#

http://ir1regcap0003r0:23280/safe-scheduler/jsondoc-ui.html?url=http://ir1regcap0003r0:23280/safe-scheduler/jsondoc#

http://mn2regcap0002u1:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0002u1:8080/safe-scheduler/jsondoc#
http://mn2regcap0002u2:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0002u2:8080/safe-scheduler/jsondoc#

http://mn2regcap0001u1:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0001u1:8080/safe-scheduler/jsondoc#
http://mn2regcap0001u2:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0001u2:8080/safe-scheduler/jsondoc#

http://mn2regcap0001d1:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0001d1:8080/safe-scheduler/jsondoc#
http://mn2regcap0001d2:8080/safe-scheduler/jsondoc-ui.html?url=http://mn2regcap0001d2:8080/safe-scheduler/jsondoc#
```

Local safe-scheduler

```
http://127.0.0.1:64080/safe-scheduler/jsondoc-ui.html?url=http://127.0.0.1:64080/safe-scheduler/jsondoc#
```

</t>
<t tx="jonathanhudson.20201006084250.365">@language md

#### Debugging Safe-Mon

```
export SAFE_MON_CONFIG_PASSWORD=$(systemd-ask-password "safe-mon config password")
export JPDA_ADDRESS=0.0.0.0:8000
export CATALINA_HOME=/usr/catalina/apache-tomcat-9.0.36

./catalina.sh jpda start
```</t>
<t tx="jonathanhudson.20201006084250.366">@language md

#### URLs

With the latest safe-mon release there is an additional read only endpoint available:

http://ir3rstats0001p0:8080/safe-monitoring/service/safe/data/dump
http://ir1rstats0001p0:8080/safe-monitoring/service/safe/data/dump

Calling this via curl / or in a browser will give you a JSON representation of any errors safe-mon is currently tracking without having to logon to the box.
</t>
<t tx="jonathanhudson.20201006084250.367"></t>
<t tx="jonathanhudson.20201006084250.368">@language shell
#!/bin/bash

PATH=/bin:/usr/bin:/usr/local/bin

check-nfs () {
    local TMPFILE=/tmp/checknfs.$$ RET=0 ORPHAN SUBSHELLPID

    if [ "$#" -eq 0 ]; then
        cat&lt;&lt;EOF
usage: check-nfs NFS-DIRECTORY...
Check if accessing any of NFS-DIRECTORY failed
EOF
        return 1
    fi

    while [ -n "$1" ]; do
        read -t 1 &lt; &lt;(echo $BASHPID &gt;"$TMPFILE"; stat -t "$1" 2&gt;/dev/null)
        if [ "$?" -gt 128 ]; then
	    #echo "error: $1"
	    ORPHAN=$(cat $TMPFILE)
	    SUBSHELLPID=$(ps --ppid $ORPHAN -o pid=)
	    [ -n "$SUBSHELLPID" ] &amp;&amp; kill -9 $SUBSHELLPID
	    kill -9 $ORPHAN
	    RET=1
        fi
        shift
        rm -f $TMPFILE
    done
    return "$RET"
}

check-nfs "$@"

</t>
<t tx="jonathanhudson.20201006084250.369">@language shell
#!/bin/bash
stat_timeout=0.1 # seconds
umount_timeout=1200 # seconds

# Force/Lazy unmount hung NFS filesystems
for mp in $(mount -t nfs4 | awk '{print $3}'); do
    timeout -s kill $stat_timeout stat -t "$mp" &gt; /dev/null
    if [[ $? == 137 ]]; then
        echo "Hung : $mp"
    fi
done
</t>
<t tx="jonathanhudson.20201006084250.37">---
- hosts: dev_all 
  vars:
    users:
      - { name: 'safe-mon' }
#      - { name: 'd3s-dk' }
  tasks:
  - name: Switch to safe-mon user orig
    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
</t>
<t tx="jonathanhudson.20201006084250.370"></t>
<t tx="jonathanhudson.20201006084250.371">@language md

#### Splunk instances

https://splunk
https://splunk-dev

#### Splunk Links

Search command reference: https://docs.splunk.com/Documentation/SplunkCloud/8.0.2001/SearchReference/Eval
Splunkbase (App store): https://splunkbase.splunk.com/

The index you guys will use mainly is **regplat** within that there are currently 2 sourcetypes
```
syslog:regplat:sft
symantec:ep:apiscan:file
```

The Application you guys can use is **RegPlat** or **regplat_app**


#### Search Modes:

- Fast Mode - No field extraction, best used for free text searches
- Smart Mode - Pulls out some fields but not all and does not allow you to swap to Events when using Statistics commands
- Verbose Mode - Slowest but pulls out all fields and allows you to see raw events even when using Statistics commands

#### Useful Searches

Authentication Events
```
index=regplat sourcetype="syslog:regplat:sft" "Authentication"
```
Get User downloads and count by File Box
```
index=regplat sourcetype="syslog:regplat:sft" "User Downloaded File" | stats count by FileBox
```
Count number of Downloads over time by FileBox
```
index=regplat sourcetype="syslog:regplat:sft" "User Downloaded File" | timechart count by FileBox
```
Count overtime by Filebox and rename FileBox to Application Security
```
index=regplat sourcetype="syslog:regplat:sft" "User Downloaded File" | eval FileBox=if(FileBox=="ISC_AppSec", "Application Security", FileBox) | stats count by FileBox
```
Filter Sourcetype using Wildcard:
```
index=regplat sourcetype="symantec*"
```
Extract scan_duration and produce the average 
```
index=regplat sourcetype="symantec*" | rex "Scan Duration \(sec\) = (?&lt;scan_duration&gt;\d+\.\d+)" | stats avg(scan_duration) 
```            
Extract scan_duration and produce the average and round to 2 decimal place
```
index=regplat sourcetype="symantec*" | rex "Scan Duration \(sec\) = (?&lt;scan_duration&gt;\d+\.\d+)" | stats avg(scan_duration) AS average_scan | eval average_scan=round(average_scan, 2)
```
Lookup department based on FileBox using a custom lookup called **FileboxByDepartment**
```
index=regplat sourcetype="syslog*" | lookup FileboxByDepartment FileBox AS FileBox | stats count by Department
```
</t>
<t tx="jonathanhudson.20201006084250.372">@language md

**List Softcards**
```
ppmk
```

**Restart nfast**
```
/opt/nfast/sbin/init.d-ncipher restart
```

**Module Info**
```
nfkminfo -l

nfkmcheck

enquire
```

</t>
<t tx="jonathanhudson.20201006084250.373">@language md

### Links

https://community.talend.com/t5/Design-and-Development/Mappping-CSV-DATA-to-XML/td-p/66631

</t>
<t tx="jonathanhudson.20201006084250.374"></t>
<t tx="jonathanhudson.20201006084250.375">@language md

Change screen lock idle timeout

#### Registry Editor 

Look for

7bc4a2f9-d8fc-4469-b07b-33eb785aaca0

Computer\HKEY_LOCAL_MACHINE\SYSTEM\ControlSet001\Control\Power\PowerSettings\7516b95f-f776-4464-8c53-06167f40cc99\8EC4B3A5-6868-48c2-BE75-4F3044BE88A7

Select Attriubutes and changed from 1 to 2 then close

#### Power Settings

Control Panel\All Control Panel Items\Power Options\Edit Plan Settings

Change advanced power settings

Sleep\System unattended sleep timeout\

Change default from 2 mins to 20 mins

#### Using Command Prompt

Command Prompt ( Admin )

```
powercfg.exe /SETACVALUEINDEX SCHEME_CURRENT SUB_VIDEO VIDEOCONLOCK 999
powercfg.exe /SETACTIVE SCHEME_CURRENT
```
</t>
<t tx="jonathanhudson.20201006084250.38">---
- hosts: linux 
  vars:
    users:
      - { name: 'safe-mon' }
      - { name: 'postgres' }
  tasks:
  - name: Switch to safe-mon user orig
#    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }}"    
    shell: "whoami &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: sudo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
</t>
<t tx="jonathanhudson.20201006084250.39">---
- hosts: uat_db
  tasks:
  - name: print to stdout
    shell: ". /data/home/safe-mon/.bashrc &amp;&amp; uname"
    register: response
    become: yes
    become_user: safe-mon
    become_method: su
  - debug: msg="{{ response.stdout }}"
  - debug: msg="{{ response.stderr }}"
</t>
<t tx="jonathanhudson.20201006084250.4" __bookmarks="7d7100580700000069735f6475706571014930300a732e">@language md
```
adflush
adreload
adinfo -a

(13:19 jonathanhudson@OB1-001122 safe-deployments)&gt;adinfo -a
Local host name:   ob1-001122
Joined to domain:  uk365office.co.uk
Joined as:         ob1-001122.uk365office.co.uk
Pre-win2K name:    ob1-001122
Current DC:        mn2addsdc0001p0.uk365office.co.uk
Preferred site:    UK0HB
Zone:              uk365office.co.uk/Unix/Zones/Desktop
Last password set: 2020-02-05 23:02:09 UTC
CentrifyDC mode:   connected
Licensed Features: Enabled

```
To list the commands that you can call
```
dzdo -l 
```</t>
<t tx="jonathanhudson.20201006084250.40">---
- hosts: uat_db
  tasks:
  - name: print to stdout
    shell: ". /data/home/safe-mon/.bashrc &amp;&amp; uname &amp;&amp; id"
    register: response
    become: yes
    become_user: safe-mon
    become_method: dzdo
  - debug: msg="{{ response.stdout }}"
  - debug: msg="{{ response.stderr }}"
</t>
<t tx="jonathanhudson.20201006084250.41">---
- hosts: uat_db 
  vars:
    users:
      - { name: 'safe-mon' }
      - { name: 'postgres' }
  tasks:
  - name: Switch to safe-mon user orig
#    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }}"    
    shell: "whoami &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
</t>
<t tx="jonathanhudson.20201006084250.42">---
- hosts: uat_db
  tasks:
  - name: print to stdout
    shell: ". /data/home/safe-mon/.bashrc &amp;&amp; uname"
    register: response
    become: yes
    become_user: safe-mon
    become_method: dzdo
  - debug: msg="{{ response.stdout }}"
  - debug: msg="{{ response.stderr }}"
</t>
<t tx="jonathanhudson.20201006084250.43">---
- hosts: test
  tasks:
  - name: print to stdout
    shell: ". /data/home/d3s-dk/.bashrc &amp;&amp; uname"
    register: hello
    become: yes
    become_user: d3s-dk
    become_method: dzdo
  - debug: msg="{{ hello.stdout }}"
  - debug: msg="{{ hello.stderr }}"
</t>
<t tx="jonathanhudson.20201006084250.44">---
- hosts: test
  tasks:
  - name: Switch and check username
    shell: ". /data/home/d3s-dk/.bashrc &amp;&amp; echo 'I am user : ' &amp;&amp; whoami &amp;&amp; echo 'delivery' $delivery"
    register: hello
    become: yes
    become_user: d3s-dk
    become_method: dzdo
  - debug: msg="{{ hello.stdout }}"
  - debug: msg="{{ hello.stderr }}"
</t>
<t tx="jonathanhudson.20201006084250.45"></t>
<t tx="jonathanhudson.20201006084250.46"></t>
<t tx="jonathanhudson.20201006084250.47">Test
</t>
<t tx="jonathanhudson.20201006084250.48"></t>
<t tx="jonathanhudson.20201006084250.49"></t>
<t tx="jonathanhudson.20201006084250.5" __bookmarks="7d7100580700000069735f6475706571014930300a732e">yum install ansible

ansible all -i "localhost," -c local -m shell -a 'echo hello world'

cd

mkdir ansible_dir

cd ansible_dir/

vi helloworld.yml

---
- hosts: all
  tasks:
    - shell: echo "hello world"


ansible-playbook -i "localhost," -c local helloworld.yml

vi /etc/ansible/hosts

localhost ansible_connection=local

ansible-playbook helloworld.yml

ansible all -m shell -a 'echo hello world'



ansible-playbook release.yml --extra-vars "version=1.23.45 other_variable=foo"</t>
<t tx="jonathanhudson.20201006084250.50"></t>
<t tx="jonathanhudson.20201006084250.51"></t>
<t tx="jonathanhudson.20201006084250.52">@language md
Example Spain
```
ansible-playbook l-es-install-d3s-server.yml -e d3sVersion="5.1.0.13-RELEASE"
```
</t>
<t tx="jonathanhudson.20201006084250.53" __bookmarks="7d7100580700000069735f6475706571014930300a732e">#!/bin/sh
#set -x

username=`logname`
group=d3s
hostname=`hostname -s`
userhome=/home/${username}
prikey=${userhome}/.ssh/id_${hostname}
pubkey=${userhome}/.ssh/id_${hostname}.pub
config=${userhome}/.ssh/config

mkdir ${userhome}/.ssh
chown -R ${username}:adusers ${userhome}/.ssh 

su - ${username} -c "ssh-keygen -t rsa -f ${prikey}"

cat ${pubkey} &gt;&gt; /data/home/d3s-dk/.ssh/authorized_keys

cat &lt;&lt;EOT &gt;&gt; ${config}
Host ${hostname}
   IdentityFile ${prikey}
EOT

ssh d3s-dk@${hostname}</t>
<t tx="jonathanhudson.20201006084250.54" __bookmarks="7d7100580700000069735f6475706571014930300a732e">{
    "addresses": {
        "private_ext": [
            {
                "type": "fixed",
                "addr": "172.16.2.100"
            }
        ],
        "private_man": [
            {
                "type": "fixed",
                "addr": "172.16.1.100"
            },
            {
                "type": "floating",
                "addr": "10.90.80.10"
            }
        ]
    }
}


To filter a list of dicts you can use the selectattr filter together with the equalto filter:

network.addresses.private_man | selectattr("type", "equalto", "fixed")


While this in theory should work and actually does for most people, it never did for me. Any Ansible version I've ever used (up to 2.1) did not know the equalto filter. I've never figured out why, but got a solution: Ansible also has the filters match and search, which take regular expressions:


match will require a complete match in the string, while search will require a match inside of the string.
network.addresses.private_man | selectattr("type", "match", "^fixed$")


To reduce the list of dicts to a list of strings, so you only get a list of the addr fields, you can use the map filter:
... | map(attribute='addr') | list


Or if you want a comma separated string:
... | map(attribute='addr') | join(',')


Combined, it would look like this.
- debug: msg={{ network.addresses.private_man | selectattr("type", "equalto", "fixed") | map(attribute='addr') | join(',') }}
</t>
<t tx="jonathanhudson.20201006084250.55" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: linux
  vars:
    users:
      - { name: 'd3s-es', delivery: 'dictao-d3s-jel-setup-server', module: 'dictao-d3s-jel-setup-server', symlink: 'd3s', component: 'd3s', target: 'OB1-001122' }
      - { name: 'd3s-bg-nraproxy', delivery: 'dictao-d3s-jel-setup-bg-nraproxy', module: 'dictao-d3s-jel-setup-bg-nraproxy', symlink: 'd3s-nraproxy', component: 'nraproxy', target: 'XYZ-001122' }
      - { name: 'd3s-bg-pushsrv', delivery: 'dictao-d3s-jel-setup-bg-pushsrv', module: 'dictao-d3s-jel-setup-bg-pushsrv', symlink: 'd3s-pushsrv', component: 'pushsrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-nramocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-nramocksrv', component: 'nramocksrv', target: 'XYZ-001122' }
      - { name: 'd3s-bg-opermocksrv', delivery: 'dictao-d3s-jel-setup-bg-mocksrv', module: 'dictao-d3s-jel-setup-bg-mocksrv', symlink: 'd3s-opermocksrv', component: 'opermocksrv', target: 'XYZ-001122' }      
    jurisdiction: 'es'
    configFile: []
  tasks:
  - name: check configure.properties file
    local_action: stat path=/cygdrive/c/data/code/safe-configs/Spain/LCL/{{jurisdiction}}-d3s-configure.properties    
    register: configCheck
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: show configCheck
    debug: msg={{ configCheck }}
    
  - name: filter users
    debug: msg={{ users | selectattr( 'symlink', 'equalto', 'd3s' ) | list }}
#    with_items: '{{ users | map( attribute='target' ) | list | unique }}'

  - name: filter configCheck with stat that exists
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}  

  - name: filter configCheck with stat that exists and filter on path
    debug: msg={{ configCheck.results | selectattr( 'stat', 'defined' ) | selectattr( 'stat.path', 'search', 'configure.properties' ) | list }}  
    
  - name: set source config missing
    set_fact: configFile="{{ item }}"
    with_items: "{{ configCheck.results | selectattr( 'stat', 'defined' ) | list }}"
    
  - name: dump configFile
    debug: msg={{ configFile }}
    
#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties-1 dest=/cygdrive/c/temp/es-d3s-configure.properties-1
#    when: not configFile.results[0].stat.exists and item.target in ansible_hostname

#  - name: copy configure.properties for component to remote symlink
#    local_action: copy src=/cygdrive/c/data/code/safe-configs/Spain/LCL/es-d3s-configure.properties dest=/cygdrive/c/temp/es-d3s-configure.properties
#    when: configFile.results[0].stat.exists and item.target in ansible_hostname

</t>
<t tx="jonathanhudson.20201006084250.56" __bookmarks="7d7100580700000069735f6475706571014930300a732e">@nocolor
change the default behavior by setting the 

dzdo.always_set_home or 
dzdo.set_home configuration parameters in the centrifydc.conf configuration 


cd /etc/centrifydc/

cp centrifydc.conf centrifydc.conf.backup
vi centrifydc.conf
systemctl restart centrifydc.service
adinfo
systemctl status centrifydc.service
adflush
adreload

dzdo.always_set_home: True
dzdo.set_home: True

</t>
<t tx="jonathanhudson.20201006084250.57" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: linux 
  vars:
    delivery: '5.0.35.5-SNAPSHOT'  
    users:
      - { name: 'safe-mon-test', groups: 'd3s,safe-mon-test', gid: '1222', uid: '1222', symlink: 'd3s' }
  tasks:
  - name: Test Priv elevation and create user 
    user: name=safe-mon-test comment="safe-mon-test account" state=present uid=1222 group=d3s groups=d3s home=/data/home/safe-mon-test
    become: yes
    become_user: root
    become_method: dzdo
    
  - name: Switch to safe-mon-test user orig
    shell: ". /data/home/{{ item.name }}/.bashrc &amp;&amp; whoami &amp;&amp; echo ' ' &amp;&amp; id {{ item.name }} &amp;&amp; echo ' delivery' {{delivery}}"    
    register: response
    become: yes
    become_user: safe-mon-test
    become_method: dzdo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"
</t>
<t tx="jonathanhudson.20201006084250.58" __bookmarks="7d7100580700000069735f6475706571014930300a732e">#Tech:1--&gt;Linux:4</t>
<t tx="jonathanhudson.20201006084250.59" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: dev_d3s
  vars:
    env: 'dev'
    sourceDirectory: '/data/code'
    configFolder: '{{sourceDrive}}{{sourceDirectory}}/safe-configs/Denmark/DEV'
    commonTaskLocation: '{{sourceDirectory}}/safe-deployments/ansible/common/tasks'   
    tamperTokenBackup: '~/backup/backup-tamper-token-safdk-232.zip' 
    serviceState: []
    users:
      - { name: 'd3s-dk', oldTamperTokenFolder: '/mnt/emergency/d3s-dk/tampertokens/', newTamperTokenFolder: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/', component: 'd3s', port: '25080', context: 'dk', target: 'mn2regcap0001d1' }
    configs:
      - { name: 'configure.properties', user: 'd3s-dk', type: '', prefix: 'dk-d3s-', suffix: '-1', dstLocation: '~/d3s', srcLocation: '{{configFolder}}', target: 'mn2regcap0001d1' }      

  tasks:  
    
    # Stop the running service 
  - name: check service is running on {{inventory_hostname}}   
    uri: url=http://{{inventory_hostname}}:{{ item.port }}/d3s/jel/{{ item.context }}/monitoring.xml return_content=yes
    register: result
    ignore_errors: yes
    with_items: "{{users}}"
    when: item.target in ansible_hostname

  - name: set service status
    set_fact: serviceState="{{ serviceState + [item.status|default(0)] }}"
    with_items: "{{ result.results }}"
    
  - name: disable activator for running service {{inventory_hostname}}   
    uri: url=http://{{inventory_hostname}}:{{ item.0.port }}/activator/state?force=down return_content=yes
    register: result
    ignore_errors: yes
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) and item.0.component == 'd3s' )
  
  - name: wait for queues to empty before proceeding d3s
    uri: url=http://{{inventory_hostname}}:{{ item.0.port }}/d3s/jel/{{ item.0.context }}/monitoring.xml return_content=yes
    register: result
    ignore_errors: yes
    until: ( result.content.find( "Level1QueueSize&gt;0&lt;" ) != -1 and result.content.find( "Level2QueueSize&gt;0&lt;" ) != -1 )
    retries: 10
    delay: 120   
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) and item.0.component == 'd3s' )
  
  - name: check queues are empty before proceeding
    fail: msg="Queues are not empty unable to proceed"
    with_items: "{{ result.results }}"
    when: item.failed is defined and item.attempts == 10
   
  - name: run the d3sadmin stop command
    shell: ". /data/home/{{ item.0.name }}/.bashrc &amp;&amp; ~/d3s/bin/d3sadmin stop"
    become: yes
    become_user: "{{ item.0.name }}"
    become_method: dzdo
    with_together: 
     - "{{ users }}"
     - "{{ serviceState }}"
    when: item.0.target in ansible_hostname and ( item.1 in ( 200, 503 ) )
    
    # Backup tamper token files
  - name: create tamper token backup folder
    file: path="~/backup" state=directory mode=0770
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: backup tamper token source files
    archive: path={{item.oldTamperTokenFolder}} dest={{tamperTokenBackup}} remove=false format=zip
    register: zipTamperTokenFiles
    become: yes
    become_user: '{{ item.name }}'
    become_method: dzdo
    with_items: '{{users}}'
    when: item.target in ansible_hostname

  - name: check backup ok
    debug: msg="Backup tamper tokens successfull"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and zipTamperTokenFiles.results[0] is defined and zipTamperTokenFiles.results[0].state == 'file'

  - name: move tamper token source files to new location
    shell: "/usr/bin/rsync -a --remove-source-files /{{item.0.oldTamperTokenFolder}}/ /{{item.0.newTamperTokenFolder}}/"
    no_log: True
    register: moveTamperTokenFiles
    become: yes
    become_user: '{{ item.0.name }}'
    become_method: dzdo
    with_together:
      - "{{users}}"
      - "{{zipTamperTokenFiles.results }}"
    when: item.0.target in ansible_hostname and (item.1.state is defined and item.1.state == 'file')
    delegate_to: "{{item.0.target}}"

  - name: check move ok
    debug: msg="Move tamper tokens successfull"
    with_items: '{{users}}'
    when: item.target in ansible_hostname and moveTamperTokenFiles.results[0] is defined and moveTamperTokenFiles.results[0].rc == 0
    
  - name: remove Fempty tamper token folders
    shell: "find /{{item.0.oldTamperTokenFolder}}/ -type d -empty -delete"
    no_log: True
    become: yes
    become_user: '{{ item.0.name }}'
    become_method: dzdo
    with_together:
      - "{{users}}"
      - "{{moveTamperTokenFiles.results}}"
    when: item.0.target in ansible_hostname and (item.1.rc is defined and item.1.rc == 0)
    delegate_to: "{{item.0.target}}"

    # Backup and update configure.properties
  - name: backup the configure.properties file
    copy: remote_src=true src=~/d3s/configure.properties dest=~/d3s/configure.properties.tt.bkp mode=775
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname  
  
  - name: push configs to remote folder
    copy: src={{ item.srcLocation }}/{{ item.type }}/{{ item.prefix }}{{ item.name }}{{ item.suffix }} dest={{ item.dstLocation }}/{{ item.name }} force=yes
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{ configs }}"
    when: item.target in ansible_hostname
    
    # Configure to use new tamper token location
  - name: run the d3sadmin configure script
    shell: ". /data/home/{{ item.user }}/.bashrc &amp;&amp; ~/d3s/bin/d3sadmin configure -Dforce=true"
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{configs}}"    
    when: item.target in ansible_hostname and item.name == 'configure.properties' 

    # Start the service
  - name: start d3s managed services
    service: name={{ item.name }} state=started enabled=yes
    become: yes
    become_user: root
    become_method: dzdo  
    with_items: "{{users}}"
    when: item.target in ansible_hostname

  - name: reset service variables
    set_fact:
      serviceState: []
    
  - file: path=/var/log/ansible state=touch
    become: yes
    become_user: root
    become_method: dzdo
    
  - name: log execution
    lineinfile: dest=/var/log/ansible line='{{ ansible_date_time.iso8601 }} relocate tamper token files' insertafter='EOF'
    become: yes
    become_user: root
    become_method: dzdo
</t>
<t tx="jonathanhudson.20201006084250.6" __bookmarks="7d7100580700000069735f6475706571014930300a732e">
# On mn2regcap0001d1
[jonathanhudson@mn2regcap0001d1 ~]$  dzdo -u safe-mon /bin/sh
sh-4.2$ echo ~
/data/home/safe-mon

# On OB1-001122
[15:36 jonathanhudson@OB1-001122 ~] &gt; dzdo -u safe-mon /bin/sh
sh-4.2$ echo ~
/home/jonathanhudson




# Works from OB1-001122
dzdo -Hu safe-mon /bin/sh</t>
<t tx="jonathanhudson.20201006084250.60" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: dev_d3s
  vars:
    files:
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/Dictao1/inuse/', target: 'mn2regcap0001d1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/Dictao2/inuse/', target: 'mn2regcap0001d1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-2/Dictao1/inuse/', target: 'mn2regcap0001d1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-2/Dictao2/inuse/', target: 'mn2regcap0001d1' }
    copyToFolder: '/tmp/jono'
    outputFile: 'tt-close-times'
    wildcard: '*.xml'
    sourceFiles: []
  tasks:
  - name: remove copyTo folder
    local_action: file path={{copyToFolder}} state=absent
    run_once: true
    ignore_errors: yes
  
  - name: create copyTo folder
    local_action: file path={{copyToFolder}} state=directory
    run_once: true
  
  - name: find files to move
    find: paths={{ item.srcLocation }} patterns={{wildcard}} recurse=true
    register: findGlob
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{ files }}"
    when: item.target in ansible_hostname

#  - name: check copy ok
#    debug: var=findGlob
#    with_together: 
#      - "{{ files }}"
#      - "{{ findGlob.results }}"
#    when: item.0.target in ansible_hostname and item.1.files is defined and item.1.examined &gt; 0

  - name: set files
    set_fact: sourceFiles="{{ sourceFiles + [ item.1.files ]}}"
    with_together: 
      - "{{ files }}"
      - "{{ findGlob.results }}"
    when: item.0.target in ansible_hostname and item.1.files is defined and item.1.examined &gt; 0

#  - name: check copy ok
#    debug: var=sourceFiles

#  - name: file names
#    debug: msg="Name "{{ item.path }}
#    with_items: "{{ sourceFiles }}"

  - name: copy files from target
    fetch: src={{ item.path }} dest={{copyToFolder}}/ flat=yes force=yes
    become: yes
    become_user: "root"
    become_method: dzdo
    with_items: "{{ sourceFiles }}"

  - name: list copied files
    local_action: find paths={{copyToFolder}}/ patterns={{wildcard}} recurse=true
    run_once: true
    register: retrievedGlob

#  - name: check copy ok
#    debug: var=retrievedGlob

#  - name: convert to list remote
#    debug: var=item
#    with_items: "{{ sourceFiles | map( attribute='path' ) | list | unique }}"

#  - name: convert to list local
#    debug: var=item
#    with_items: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

#  - name: locate value
#    debug: msg="the value of file is {{ item | map( 'regex_search', '(.*)expirationDateTime&gt;(.*)&lt;expirationDateTime(.*)' ) | select('string') | list }}"
#    with_file: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

  - name: extract expiry date
    local_action: xml path={{item}} xpath='/mutableTamperToken/expirationDateTime' content=text
    register: expiryDates
    with_items: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

#  - name: dump extracted dates
#    debug: var=expiryDates    

#  - name: convert to list local
#    debug: msg="Expiry "{{ item }}
#    with_items: "{{ expiryDates.results | map( attribute='matches' ) | list | unique }}"

  - name: create log file
    local_action: file path={{copyToFolder}}/{{outputFile}} state=touch
    run_once: true

  - name: log expiry dates
    local_action: lineinfile dest={{copyToFolder}}/{{outputFile}} line='{{ item }}' insertafter='EOF'
    run_once: true
    with_items: "{{ expiryDates.results | map( attribute='matches' ) | list | unique }}"
    
  - name: display output file
    debug: var=item
    run_once: true
    with_file: "{{copyToFolder}}/{{outputFile}}"
    
  - name: cat output file
    local_action: command cat {{copyToFolder}}/{{outputFile}}
    run_once: true
    register: response
</t>
<t tx="jonathanhudson.20201006084250.61" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: prd_dk_d3s
  vars:
    files:
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/b365/inuse/', target: 'ir1regcap0002p1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-1/b365sports/inuse/', target: 'ir1regcap0002p1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-2/b365/inuse/', target: 'ir1regcap0002p1' }
    - { folder: 'fstab', user: 'd3s-dk', type: 'tamper-token', prefix: '', suffix: '.xml', dstLocation: '/tmp/', srcLocation: '/mnt/SAFE_DATA/d3s-dk/tampertokens-2/b365sports/inuse/', target: 'ir1regcap0002p1' }
    copyToFolder: '/tmp/jono'
    outputFile: 'tt-close-times'
    wildcard: '*.xml'
    sourceFiles: []
  tasks:
  - name: remove copyTo folder
    local_action: file path={{copyToFolder}} state=absent
    run_once: true
    ignore_errors: yes
  
  - name: create copyTo folder
    local_action: file path={{copyToFolder}} state=directory
    run_once: true
  
  - name: find files to move
    find: paths={{ item.srcLocation }} patterns={{wildcard}} recurse=true
    register: findGlob
    become: yes
    become_user: "{{ item.user }}"
    become_method: dzdo
    with_items: "{{ files }}"
    when: item.target in ansible_hostname

#  - name: check copy ok
#    debug: var=findGlob
#    with_together: 
#      - "{{ files }}"
#      - "{{ findGlob.results }}"
#    when: item.0.target in ansible_hostname and item.1.files is defined and item.1.examined &gt; 0

  - name: set files
    set_fact: sourceFiles="{{ sourceFiles + [ item.1.files ]}}"
    with_together: 
      - "{{ files }}"
      - "{{ findGlob.results }}"
    when: item.0.target in ansible_hostname and item.1.files is defined and item.1.examined &gt; 0

#  - name: check copy ok
#    debug: var=sourceFiles

  - name: file names
    debug: msg="Name "{{ item.path }}
    with_items: "{{ sourceFiles }}"

  - name: copy files from target
    fetch: src={{ item.path }} dest={{copyToFolder}}/ flat=yes force=yes
    become: yes
    become_user: "root"
    become_method: dzdo
    with_items: "{{ sourceFiles }}"

  - name: list copied files
    local_action: find paths={{copyToFolder}}/ patterns={{wildcard}} recurse=true
    run_once: true
    register: retrievedGlob

#  - name: check copy ok
#    debug: var=retrievedGlob

#  - name: convert to list remote
#    debug: var=item
#    with_items: "{{ sourceFiles | map( attribute='path' ) | list | unique }}"

#  - name: convert to list local
#    debug: var=item
#    with_items: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

#  - name: locate value
#    debug: msg="the value of file is {{ item | map( 'regex_search', '(.*)expirationDateTime&gt;(.*)&lt;expirationDateTime(.*)' ) | select('string') | list }}"
#    with_file: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

  - name: extract expiry date
    local_action: xml path={{item}} xpath='/mutableTamperToken/expirationDateTime' content=text
    register: expiryDates
    with_items: "{{ retrievedGlob.files | map( attribute='path' ) | list | unique }}"

#  - name: dump extracted dates
#    debug: var=expiryDates    

#  - name: convert to list local
#    debug: msg="Expiry "{{ item }}
#    with_items: "{{ expiryDates.results | map( attribute='matches' ) | list | unique }}"

  - name: create log file
    local_action: file path={{copyToFolder}}/{{outputFile}} state=touch
    run_once: true

  - name: log expiry dates
    local_action: lineinfile dest={{copyToFolder}}/{{outputFile}} line='{{ item }}' insertafter='EOF'
    run_once: true
    with_items: "{{ expiryDates.results | map( attribute='matches' ) | list | unique }}"
    
  - name: display output file
    debug: var=item
    run_once: true
    with_file: "{{copyToFolder}}/{{outputFile}}"
    
  - name: cat output file
    local_action: command cat {{copyToFolder}}/{{outputFile}}
    run_once: true
    register: response
</t>
<t tx="jonathanhudson.20201006084250.62" __bookmarks="7d7100580700000069735f6475706571014930300a732e">rename u-bg- p-bg- *

</t>
<t tx="jonathanhudson.20201006084250.63" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># Running ansible playbooks
 
1. Create ssh key pair with passphrase
 
ssh-keygen -t rsa -f .ssh/id_jonathanhudson 
 
2. Create config file for all hosts

Host ob1-001122
    IdentityFile ~/.ssh/id_jonathanhudson
	
3. Copy ssh public key to remote hosts root authorized_keys

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcKkDQT7Ls/EqqEnONlUC218wdwX8xm0Qw/J2RxtPgdeP26NOjI4JDT/vvh0QNkJYMstgBtKqcIbEulaqjmXQEAWYtyRjoOLJqSMQbrR2hDpvM5VQ2SdqpS7/ZfRxvMa+7gd/D/4qKx8xQ85jmS4Q9gF7CeBHbosTpnbjalKXGfh1TcTpHEbgTdi//9+akp+ucHlGSqk8PWyP4ml20Lq6ZO4WSWoc51ihGlnGbQcrZqNaepxqs/BdhPd0OB9Hmz+Fb+D3sRLbeDFtvD69SQnpLMb6Ngz6+Mk9Rk2sYh5BjnNCQqZKO1AGg6pFaP0lBA8QA5hCZwyJSGIS59SzlRxf1 jonathanhudson@HLS-F10299 

4. Add the ssh-agent to the cygwin .bash_profile
 
SSHAGENT=/usr/bin/ssh-agent
SSHAGENTARGS="-s"

if [ -z "$SSH_AUTH_SOCK" -a -x "$SSHAGENT" ]; then
	eval `$SSHAGENT $SSHAGENTARGS`
	trap "kill $SSH_AGENT_PID" 0
fi

5. Then add the ssh key to the agent which will prompt for password

ssh-add ~/.ssh/id_jonathanhudson
 
6. Now change to the ansible script folder to test
 
cd /cygdrive/c/data/code/safe-deploy/ansible/bulgaria/LCL

7. Finally run the a playbook on host

ansible linux -m ping

ansible-playbook l-bg-install-d3s-server.yml</t>
<t tx="jonathanhudson.20201006084250.64" __bookmarks="7d7100580700000069735f6475706571014930300a732e">ansible-playbook test-priv-echo-linux.yml --ask-pass --ask-become-pass

vi /etc/sudoers

## Next comes the main part: which users can run what software on
## which machines (the sudoers file can be shared between multiple
## systems).
## Syntax:
##
##      user    MACHINE=COMMANDS
##
## The COMMANDS section may have other options added to it.
##
## Allow root to run any commands anywhere
root    ALL=(ALL)       ALL
jonathanhudson ALL=(ALL) ALL









rps_jhudson ALL=(ALL) ALL
</t>
<t tx="jonathanhudson.20201006084250.65" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># SUDOERS

vi /etc/sudoers

## Next comes the main part: which users can run what software on
## which machines (the sudoers file can be shared between multiple
## systems).
## Syntax:
##
##      user    MACHINE=COMMANDS
##
## The COMMANDS section may have other options added to it.
##
## Allow root to run any commands anywhere
root    ALL=(ALL)       ALL
jonathanhudson ALL=(ALL) ALL


# PLAYBOOK


---
- hosts: linux 
  vars:
    users:
      - { name: 'safe-mon' }
      - { name: 'postgres' }
  tasks:
  - name: Switch to safe-mon user orig
    shell: "whoami &amp;&amp; id {{ item.name }}"    
    register: response
    become: yes
    become_user: "{{item.name}}"
    become_method: sudo
    with_items: "{{users}}"    

  - name: debug response to print list of stdout_lines
    debug: msg="{{ item }}"
    with_items: "{{ response.results|map(attribute='stdout_lines')|list }}"  
	
	
# COMMAND
	
ansible-playbook test-priv-echo-linux.yml --ask-pass --ask-become-pass
	</t>
<t tx="jonathanhudson.20201006084250.66" __bookmarks="7d7100580700000069735f6475706571014930300a732e">- name: Do something
  shell: /usr/bin/something | grep -c foo || true
  register: shell_output

- name: Catch some fish (there are at least 5)
  shell: /usr/bin/somethingelse 
  when: shell_output.stdout &gt; "5"
  
  </t>
<t tx="jonathanhudson.20201006084250.67" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: linux
  vars:
    env: 'dev'
    jurisdiction: 'es'
    sourceDirectory: '/data/code'
    configFolder: '{{sourceDrive}}{{sourceDirectory}}/safe-configs/Denmark/LCL'
    commonTaskLocation: '{{sourceDirectory}}/safe-deployments/ansible/common/tasks'  
    executionDate: "{{ lookup('pipe', 'date +%Y%m%d-%H%M') }}"
    tamperTokenBackup: '~/backup/backup-tamper-token-safdk-232-{{ansible_date_time.date}}.zip'
    serviceState: []
    users:
      - { name: 'd3s-es', component: 'd3s', port: '25080', context: 'es', target: '{{myhost}}' }
    configs:
      - { name: 'configure.properties', user: 'd3s-dk', type: '', prefix: 'dk-d3s-', suffix: '-1', dstLocation: '~/d3s', srcLocation: '{{configFolder}}', target: '{{myhost}}' }      
  tasks:  
    # Perform smoke tests
  - name: run the testone smoke test
    shell: ". /data/home/{{item.name}}/.bashrc &amp;&amp; ~/d3s/bin/smoke-test-{{item.context}} testone Dictao1"
    register: smokeTestResult
    failed_when: smokeTestResult.rc != 0
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname
  
  - name: run the testall smoke tests
    shell: ". /data/home/{{item.name}}/.bashrc &amp;&amp; ~/d3s/bin/smoke-test-{{item.context}} testall Dictao1"
    register: smokeTestResult
    failed_when: smokeTestResult.rc != 0
    become: yes
    become_user: "{{ item.name }}"
    become_method: dzdo
    with_items: "{{users}}"
    when: item.target in ansible_hostname
  
</t>
<t tx="jonathanhudson.20201006084250.68" __bookmarks="7d7100580700000069735f6475706571014930300a732e">- name: Simple select query to acme db
  postgresql_query:
    db: acme
    query: SELECT version()

- name: Select query to db acme with positional arguments and non-default credentials
  postgresql_query:
    db: acme
    login_user: django
    login_password: mysecretpass
    query: SELECT * FROM acme WHERE id = %s AND story = %s
    positional_args:
    - 1
    - test

- name: Select query to test_db with named_args
  postgresql_query:
    db: test_db
    query: SELECT * FROM test WHERE id = %(id_val)s AND story = %(story_val)s
    named_args:
      id_val: 1
      story_val: test

- name: Insert query to db test_db
  postgresql_query:
    db: test_db
    query: INSERT INTO test_db (id, story) VALUES (2, 'my_long_story')

- name: Run queries from SQL script
  postgresql_query:
    db: test_db
    path_to_script: /var/lib/pgsql/test.sql
    positional_args:
    - 1
</t>
<t tx="jonathanhudson.20201006084250.69" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: localhost 
  vars:
    configs:
      - { name: 'conf-1', target: 'system-a' }
      - { name: 'conf-2', target: 'system-a' }
      - { name: 'conf-3', target: 'system-a' }
      - { name: 'conf-4', target: 'system-b' }
      - { name: 'conf-5', target: 'system-b' }
      - { name: 'conf-6', target: 'system-b' }
      - { name: 'conf-7', target: 'system-c' }
  tasks:
  - name: create folders
    shell: "echo create folder on {{ item }}"    
    with_items: "{{ configs | map( attribute='target' ) | list | unique }}"

  - name: copy files
    shell: "echo copy file {{ item }}"    
    with_items: "{{ configs }}"
    </t>
<t tx="jonathanhudson.20201006084250.7" __bookmarks="7d7100580700000069735f6475706571014930300a732e">---
- hosts: linux
  tasks:
  - name: simulate inherited facts
    set_fact: 
      env: dev
      jurisdiction: dk
      configFolder: '/safe-configs/Denmark/BCM'

  - name: set facts
    set_fact: 
      jurisdictionName: {
        dk: 'Denmark',
        es: 'Spain',
        bg: 'Bulgaria',
        gi: 'Gibraltar',
        gr: 'Greece' }
      envType: "{{ env|upper }}"
      envName: "{{ configFolder[-3:] }}"
    
  - name: show env lookup
    debug: msg="{{ jurisdictionName[ jurisdiction ] }}/{{ envName }}"
</t>
<t tx="jonathanhudson.20201006084250.70" __bookmarks="7d7100580700000069735f6475706571014930300a732e">In the example above, my_var is the variable name, while the value is "example text". password-file.txt is the path to the file containing password. 
Alternatively, you can make it prompt for a password value by replacing password-file.txt with @prompt.

# One password 
ansible-vault encrypt_string --vault-id password-file.txt 'example text' --name 'my_var'

# Using a label allows multiple password per file
ansible-vault encrypt_string --vault-id label1@password-file.txt 'example text' --name 'my_var'
ansible-vault encrypt_string --vault-id label1@prompt 'example text' --name 'my_var'

# As an alternative, you can echo variable value:

echo 'some text' | ansible-vault encrypt_string --vault-id label1@password-file.txt --stdin-name 'my_var'

# Or if you prefer to get a prompt to input the value:

ansible-vault encrypt_string --vault-id label1@password-file.txt --stdin-name 'my_var'


## Running

Running Playbook with Encrypted Files and Variables
Of course you want to store all files and variables containing sensitive information safely encrypted. When you run a playbook with ansible-playbook, there may be multiple encrypted files and variabes. You don't need to run the decrypt command first and encrypt again later after it finishes. Since version 2.4, you can use --vault-id flag while running ansible-playbook command, followed by either path to the password file or @prompt (to be prompted for a password). All encrypted files and variables will be decrypted if you give the correct passwords. If you use multiple labels, , you can use multiple --vault-id flags like this one:

ansible-playbook &lt;playbook-name&gt;  --vault-id label1@password-file.txt --vault-id label2@prompt


ansible-vault encrypt_string --vault-password-file a_password_file 'foobar' --name 'the_secret'


ansible-vault encrypt_string --stdin-name 'sft_config_password'
</t>
<t tx="jonathanhudson.20201006084250.71">@language md

#### Contacts

Team|Contact|Number
:---|:---|:---
&lt;img width=200 height=0/&gt;|&lt;img width=300 height=0/&gt;|&lt;img width=100 height=0/&gt;
Inrastructure Systems|Doug Mitchie|N/A
Inrastructure Systems|Anthony Balkwill|N/A
Inrastructure Systems|Ian Abrahams|N/A
Inrastructure Systems|Martin Goldsmith|N/A
Inrastructure Systems|Graham ??|N/A</t>
<t tx="jonathanhudson.20201006084250.72"></t>
<t tx="jonathanhudson.20201006084250.73">@language md
### General
```
git remote show origin

git tag -a v5.0.35.0 e63ff32c622f162ad2d-&gt; aa454a68b309d52b9671a -f

git push --tags --force
```
on other machines 
```
git pull --tags
git pull -v       ( forces git to prompt for authentication )
```
### Update a branch with Master (after Master has been updated) [ensures branch delta is minimal]
```
git checkout &lt;branch&gt;
git merge master
git push
```
### Force git merge commit to maintain history
```
git merge --no-ff --no-edit &lt;from-branch&gt;
```
### Merge from Master with merge conflicts such as version POM files
```
git merge --abort
git merge master -Xtheirs
git push
```
### Git Diff file with previous version
```
git diff HEAD^^ HEAD b365.xml
git difftool -y HEAD^..HEAD
```
### Revert file to previous commit 
Note: Doesn't remove newer commits so can't reapply any pushes user reset --hard &lt;commt&gt; instead
```
git show HEAD^^:./b365.xml       ( or path  from start of repo )
gitk ./b365.xml
git checkout 8a7facd -- b365.xml &lt;file&gt; &lt;file&gt;
```
### Revert branch to a previous commit
```
git log
git reset --hard &lt;commit&gt;
git push -f
```
### Revert branch remove all local changes
```
git reset --hard 
git clean -fd

git reset --hard origin/master
```
### To overwrite locally changed file
```
git checkout -f ....	# To overwrite local file
git checkout . 			# To overwrite all locally changed
```
### Resolve Binary Merge Conflicts
```
git checkout --theirs -- \*.p12
git reset -- \*.exe
```
### Merge safe-monitoring
```
git checkout master
git merge safe-monitoring-wip
git push 
```
### Branch based on Jira Issue

### pt (patch), ch (chore), bg (bug), tk (task), sp (scratchpad), rl (release) 
```
git checkout -b tk/SAFDK-232/relocate-tamper-tokens
```
### Push new branch to remote repository
```
git push --set-upstream origin tk/SAFDK-232/relocate-tamper-tokens
```
### To merge back and tidy up 
```
git checkout master
git checkout dictao-d3s-jel-wip
git checkout dictao-dss-wip

git merge pt/TSTRPS-45/idemp-base64-eol

git push
```
### Stash local before merge (master out of date)
```
git stash
git pull origin master 
git stash pop
```
### Delete Branch
```
git push origin --delete pt/TSTRPS-57/bgclient-smoketest-agr

git branch -d pt/TSTRPS-57/bgclient-smoketest-agr
```
### Code Version
```
git checkout . (revert branch back if version wrong)
cd /cygdrive/c/data/code/dictao-d3s-jel/
mvn clean
mvn --batch-mode release:update-versions -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.6-SNAPSHOT
mvn --batch-mode release:update-versions -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.7-SNAPSHOT
mvn --batch-mode release:update-versions -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.10-SNAPSHOT
# mvn --batch-mode release:update-versions -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.5-SNAPSHOT -DreleaseVersion=5.0.35.5-RELEASE

# mvn --batch-mode release:prepare -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.5-SNAPSHOT -DreleaseVersion=5.0.35.5-RELEASE
mvn --batch-mode release:prepare-with-pom -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.5-SNAPSHOT -DreleaseVersion=5.0.35.5-RELEASE
mvn --batch-mode release:prepare-with-pom -DautoVersionSubmodules=true -DdevelopmentVersion=5.0.35.6-SNAPSHOT -DreleaseVersion=5.0.35.6-RELEASE
mvn --batch-mode release:prepare-with-pom -DautoVersionSubmodules=true -DdevelopmentVersion=1.0.2-SNAPSHOT -DreleaseVersion=1.0.2-RELEASE
mvn --batch-mode release:prepare-with-pom -DautoVersionSubmodules=true -DdevelopmentVersion=1.0.3-SNAPSHOT -DreleaseVersion=1.0.3-RELEASE
mvn release:clean
```
### Tags

# New repo annotated tag
```
git tag -a 0.0.0.0 -m "0.0.0.0"
git push origin --tags
```

```
git tag -d &lt;tag_name&gt;
git push --delete origin &lt;tag_name&gt;
git tag -l
 
git tag -a 1.0.2-RELEASE -m "Release 1.0.2-RELEASE"

git tag -l
git push origin --tags
git push origin 1.0.2-RELEASE

# Annotated tag for specific commit

git tag -a 0.0.0.0 11594648cc23279c7afba181ad8598d0ef5f5446 -m "0.0.0.0"
git push origin --tags

# Lightweight tag for specific commit

git tag 0.0.0.0 11594648cc23279c7afba181ad8598d0ef5f5446
git push origin --tags
```
### Move a tag to latest commit
```
git push origin :refs/tags/&lt;tagname&gt;
git tag -fa &lt;tagname&gt;
git push origin arma	
```
### Get commit id for a tag
```
git rev-list -n 1 1.0.0.1
```
### List tags by date
```
git log --tags --simplify-by-decoration --pretty="format:%ai %d" | grep '2021-02-24'
```
### Check Branch before Merge
```
git checkout branch
git checkout master
git diff master..branch
git diff master..branch
git diff --name-only master..branch
```
### Gen patch file
```
git format-patch -1 HEAD --stdout &gt; 0001-last-10-commits.patch 
```
### Git logs
```
git log --pretty=format:"%h - %an, %ar : %s"
git log --merges
```
### Undo staged file (example delete)
```
git checkout -- &lt;filename&gt;
```
### show tree
```
git log --graph --oneline --all 
gitk -- show UI
```
### Resolve binary merge conflicts
```
git checkout --theirs -- procedures/documents/bet365\ RPS_Java\ Coding\ Standards.docx
git add .
git commit -m "Resolved merge conflict"
```
### Git LFS

### Git Rename Branch
```
git branch -m ＜new-branch＞ ; Rename the current branch to ＜new-branch＞
git branch -m &lt;old-branch&gt; ＜new-branch＞
git push origin :&lt;old-branch&gt; ＜new-branch＞
git push origin -u ＜new-branch＞
```

Install LFS client
git lfs install
git lfs uninstall

git lfs track "*.zip"

git config lfs.contenttype false
git lfs push --all origin master

git push

### Clone from Bare Git Repository
Export folder with .git files
Example name : /data/c-code/kognitio/kognitio-compenv.git
```
git clone /data/c-code/kognitio/kognitio-compenv.git kognitio-source

```

### Change rename limit

```
git config merge.renameLimit 999999
```
</t>
<t tx="jonathanhudson.20201006084250.74">@language md

#### Migrate a repo and keep history

git clone --mirror &lt;url_of_old_repo&gt;
cd &lt;name_of_old_repo&gt;
git remote add new-origin &lt;url_of_new_repo&gt;
git push new-origin --mirror</t>
<t tx="jonathanhudson.20201006084250.75">@language md

#### Tags
```
git tag --sort=committerdate

git tag --sort=creatordate

git tag | xargs -I@ git log --format=format:"%ai @%n" -1 @ | sort | awk '{print $4}'

git log &lt;tag&gt;..HEAD --oneline

git log --pretty=oneline
```
</t>
<t tx="jonathanhudson.20201006084250.76">@language md

### Commands
[Docs](https://git-scm.com/book/en/v2/Git-Tools-Submodules)
#### Adding an existing Project as a Submodule
```
$ git submodule add https://github.com/chaconinc/DbConnector
Cloning into 'DbConnector'...
remote: Counting objects: 11, done.
remote: Compressing objects: 100% (10/10), done.
remote: Total 11 (delta 0), reused 11 (delta 0)
Unpacking objects: 100% (11/11), done.
Checking connectivity... done.
```
This creates a folder inside the project with the folder name of the submodule
Notice the new `.gitmodules` folder

#### Committing
Just like any other commit
```
git commit -am 'added DbConnector module'
git push origin master
```

#### Cloning a Project with Submodules
```
git clone https://github.com/chaconinc/MainProject
cd MainProject
```
Submodules are cloned empty you have to initialise your local copy
```
git submodule init
git submodule update
```
Now your DbConnector subdirectory is at the exact state it was in when you committed earlier.
Another way
```
git clone --recurse-submodules https://github.com/chaconinc/MainProject
```
If you already cloned the project and forgot `--recurse-submodules`, you can combine the `git submodule init` and `git submodule update` steps by running 
```
git submodule update --init
```
To also initialize, fetch and checkout any nested submodules, you can use the foolproof 
```
git submodule update --init --recursive
```

#### Pulling in upstream changes from Submodule Remote

If you want to check for new work in a submodule, you can **go into the directory** and run git fetch and git merge the upstream branch to update the local code.
```
git fetch
git merge origin/master
```
an easier way ( assumes master branch )
```
git submodule update --remote DbConnector
```
switching the submodule branch to user `stable` rather than `master`
```
git config -f .gitmodules submodule.DbConnector.branch stable
git config status.submodulesummary 1
git status
git submodule update --remote
```
*Note : By default, the git pull command recursively fetches submodules changes, as we can see in the output of the first command above. However, it does not update the submodules. *
```
git pull
git submodule update --init --recursive
```
#### Working on Project and Submodule at the same time
First of all, let’s go into our submodule directory and check out a branch.
```
cd DbConnector/
git checkout stable
Switched to branch 'stable'
```
Let’s try updating our submodule with the “merge” option. To specify it manually, we can just add the --merge option to our update call. Here we’ll see that there was a change on the server for this submodule and it gets merged in.
```
cd ..
git submodule update --remote --merge
```
If we go into the DbConnector directory, we have the new changes already merged into our local stable branch. Now let’s see what happens when we make our own local change to the library and someone else pushes another change upstream at the same time.
```
cd DbConnector/
vim src/db.c
git commit -am 'unicode support'
[stable f906e16] unicode support
 1 file changed, 1 insertion(+)
```
Now if we update our submodule we can see what happens when we have made a local change and upstream also has a change we need to incorporate.
```
$ cd ..
$ git submodule update --remote --merge
First, rewinding head to replay your work on top of it...
Applying: unicode support
Submodule path 'DbConnector': rebased into '5d60ef9bbebf5a0c1c1050f242ceeb54ad58da9
```
If you forget the --merge, Git will just update the submodule to whatever is on the server and reset your project to a detached HEAD state.
```
$ git submodule update --remote
Submodule path 'DbConnector': checked out '5d60ef9bbebf5a0c1c1050f242ceeb54ad58da94'
```
#### Publishing Submodule Changes
If we commit in the main project and push it up without pushing the submodule changes up as well, other people who try to check out our changes are going to be in trouble since they will have no way to get the submodule changes that are depended on. Those changes will only exist on our local copy.

In order to make sure this doesn’t happen, you can ask Git to check that all your submodules have been pushed properly before pushing the main project. The git push command takes the --recurse-submodules argument which can be set to either “check” or “on-demand”. The “check” option will make push simply fail if any of the committed submodule changes haven’t been pushed.
```
git push --recurse-submodules=on-demand
```
#### Merging Submodule Changes
So, we will go into our submodule directory, create a branch based on that second SHA-1 from git diff and manually merge.
```
$ git diff
diff --cc DbConnector
index eb41d76,c771610..0000000
--- a/DbConnector
+++ b/DbConnector
```
So, in this case, eb41d76 is the commit in our submodule that we had and c771610 is the commit that upstream had. If we go into our submodule directory, it should already be on eb41d76 as the merge would not have touched it. If for whatever reason it’s not, you can simply create and checkout a branch pointing to it.
```
$ cd DbConnector

$ git rev-parse HEAD
eb41d764bccf88be77aced643c13a7fa86714135

$ git branch try-merge c771610
(DbConnector) $ git merge try-merge
Auto-merging src/main.c
CONFLICT (content): Merge conflict in src/main.c
Recorded preimage for 'src/main.c'
Automatic merge failed; fix conflicts and then commit the result.
```
We got an actual merge conflict here, so if we resolve that and commit it, then we can simply update the main project with the result.
```
$ vim src/main.c (1)
$ git add src/main.c
$ git commit -am 'merged our changes'
Recorded resolution for 'src/main.c'.
[master 9fd905e] merged our changes

$ cd .. (2)
$ git diff (3)
diff --cc DbConnector
index eb41d76,c771610..0000000
--- a/DbConnector
+++ b/DbConnector
@@@ -1,1 -1,1 +1,1 @@@
- Subproject commit eb41d764bccf88be77aced643c13a7fa86714135
 -Subproject commit c77161012afbbe1f58b5053316ead08f4b7e6d1d
++Subproject commit 9fd905e5d7f45a0d4cbc43d1ee550f16a30e825a
$ git add DbConnector (4)

$ git commit -m "Merge Others's Changes" (5)
[master 10d2c60] Merge Others's Changes
```
- First we resolve the conflict
- Then we go back to the main project directory
- We can check the SHA-1s again
- Resolve the conflicted submodule entry
- Commit our merge









</t>
<t tx="jonathanhudson.20201006084250.77"></t>
<t tx="jonathanhudson.20201006084250.78">@language md
## Installation GlusterFS

- [ ] Request firewall opening between FSH and REP / CAP servers ( TCP/UDP 24007, 24008, TCP 49152 - 49156 )
- [ ] Check servers have valid RedHat subscription FSH, REP, CAP
- [ ] Install GlusterFS on FSH servers
- [ ] Install Fuse client on REP / CAP servers
- [ ] Update the fstab file on REP / CAP servers ( add GFS_DATA_xx mounts )
- [ ] Create mount folders on REP / CAP servers ( GFS_DATA_xx )
- [ ] Create zero permission files in mount points for each jurisdiction for protection
- [ ] Create brick folders with permissions user:d3s 775
- [ ] Mount GFS volumes on REP / CAP servers
- [ ] Create jurisdiction folders on GFS_DATA_xx mounts

- [ ] Create the sudoers files on REP servers
- [ ] Deploy updated Safe-Mon with GFS monitors

- [ ] Update configure.properties to add GFS to FileCopy for each jurisdiction in NO_OPERATION mode
- [ ] Update git version of xx-configuration.xml in git

</t>
<t tx="jonathanhudson.20201006084250.79">@language md
### Install GlusterFS Dev
#### Packages
```
rpm -qa | grep gluster

cat /etc/sysconfig/rhn/systemid

yum install glusterfs-libs-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-client-xlators-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-api-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-cli-4.1.7-1.el7.x86_64.rpm
yum install userspace-rcu-0.7.16-1.el7.x86_64.rpm
yum install userspace-rcu-0.10.0-3.el7.x86_64.rpm
yum install glusterfs-fuse-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-server-4.1.7-1.el7.x86_64.rpm
```
and Ganesha
```
yum install libntirpc-1.7.1-1.el7.x86_64.rpm
yum install nfs-ganesha-2.7.1-1.el7.x86_64.rpm
yum install nfs-ganesha-gluster-2.7.1-1.el7.x86_64.rpm
```
and client Fuse
requires RedHat subscription for `psmisc`
```
yum install glusterfs-libs-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-client-xlators-4.1.7-1.el7.x86_64.rpm
yum install glusterfs-fuse-4.1.7-1.el7.x86_64.rpm
```

#### Enable Service
```
systemctl enable glusterd.service
systemctl start glusterd.service
```
#### Connect GlusterFS Nodes
From `mn2regrep0001d1` ensure ports open between nodes ( see below )
```
gluster peer probe mn2regrep0001d2
peer probe: success.
```
### Verify cluster Peers
```
gluster peer status
gluster pool list
```
#### Create Brick Mount Points
```
mkdir -p /data/bricks/dk-brick
```
#### Create Replicated volume
Run on the `mn2regrep0001d1` only
Note: 'force' is being used to override the error about using root ( this is a test )
```
gluster volume create dk-gluster-vol1 replica 2 transport tcp mn2regcap0001d1:/data/bricks/dk-brick mn2regcap0001d2:/data/bricks/dk-brick force
volume create: glustervol1: success: please start the volume to access data
```
#### Adding an arbiter
Since the arbiter brick does not store file data, its disk usage will be considerably less than the other bricks of the replica. The sizing of the brick will depend on how many files you plan to store in the volume. A good estimate will be 4kb times the number of files in the replica.
```
gluster volume add-brick dk-gluster-vol1 replica 3 arbiter 1 mn2regrep0001d1:/data/bricks/dk-brick mn2regrep0001d2:/data/bricks/dk-brick mn2regmok0001d0:/data/bricks/dk-brick
```
#### Create Replicated Volume with Arbiter
```
gluster volume create dk-gluster-vol1 replica 3 arbiter 1 transport tcp mn2regrep0001d1:/data/bricks/dk-brick mn2regrep0001d2:/data/bricks/dk-brick mn2regmok0001d0:/data/bricks/dk-brick
```

### Delete Replicated volume
```
gluster volume stop dk-gluster-vol1
gluster volume delete dk-gluster-vol1
rm /data/bricks/dk-brick -Rf
rm /data/bricks/dk-brick/* -Rf
rm /data/bricks/dk-brick/.glusterfs/ -Rf

setfattr -x trusted.glusterfs.volume-id /data/bricks/dk-brick
setfattr -x trusted.gfid /data/bricks/dk-brick
rm /data/bricks/dk-brick/.glusterfs -Rf

rm /data/bricks/dk-brick -Rf
```
#### Start the Replicated volume
```
gluster volume start dk-gluster-vol1
volume start: dk-gluster-vol1: success
```
#### Check Started
```
gluster volume info all
```
#### Check connected clients
```
gluster volume status dk-gluster-vol1 clients

watch -n 5 'gluster volume status dk-gluster-vol1 clients'
```

#### Troubleshooting Split-Brain
```
gluster volume heal dk-gluster-vol1 info split-brain
gluster volume heal dk-gluster-vol1 split-brain bigger-file /SAFE_DATA/d3s-bg/storage/Dictao2/data/2019/03/12/jono-text-file.txt
gluster volume heal dk-gluster-vol1 split-brain latest-mtime /SAFE_DATA/d3s-bg/storage/Dictao2/data/2019/03/12/jono-text-file.txt
gluster volume heal dk-gluster-vol1 split-brain source-brick mn2regcap0002u1:/bricks/brick1/brick /SAFE_DATA/d3s-bg/storage/Dictao2/data/2019/03/12/jono-text-file.txt
```
Show attribute of file on brick1
```
getfattr -d -m . -e hex /bricks/brick1/brick/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2019-09-24/Dictao2-7617886935752598859.zip
```
Then perform a manual heal
```
gluster volume heal dk-gluster-vol1
gluster volume heal dk-gluster-vol1 info
```
#### Unable to heal one brick is offline

Check the clients are all connected
```
gluster volume start dk-gluster-vol1 force
```
If any are not log onto the brick server and start
```
gluster volume start dk-gluster-vol1 force
```
Then check all connected and force manual heal
```
gluster volume heal dk-gluster-vol1
gluster volume heal dk-gluster-vol1 info
```

#### Enable Ganesha NFS
```
gluster volume set dk-gluster-vol1 nfs.disable on
gluster volume set dk-gluster-vol1 nfs.rpc-auth-allow 10.224.36.38
gluster volume set dk-gluster-vol1 auth.allow 10.224.36.38
gluster volume set dk-gluster-vol1 server.root-squash off
gluster volume set dk-gluster-vol1 features.cache-invalidation on
gluster volume set dk-gluster-vol1 cluster.min-free-disk 2GB
gluster volume set dk-gluster-vol1 storage.reserve 20
gluster volume set all cluster.enable-shared-storage enable
gluster volume stop dk-gluster-vol1
gluster volume start dk-gluster-vol1
```
**Stop the NFS Services**
```
systemctl stop nfs
systemctl stop nfs-lock.service
service rpcbind start
service glusterd restart
service nfs-ganesha restart
```
**Make sure the NFS server in GlusterFS is running**
```
gluster volume status dk-gluster-vol1
```
### Client Configuration
Open ports, create a mount point and update fstab

#### Ports required for GlusterFS

Ensure that TCP and UDP ports
**24007**,
**24008**

One port for each brick starting from port
**49152** - **49156**

#### Add dk-gluster-vol1 to /etc/fstab to access remotely

```
mn2regrep0001d1:/dk-gluster-vol1 /mnt/GLUSTER_FS glusterfs defaults,_netdev 0 0
```</t>
<t tx="jonathanhudson.20201006084250.8" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># Setup Ansible for Environment using non-root login

# Create .ssh keys
ssh-keygen -t rsa -f .ssh/id_jonathanhudson

# Create ssh-agent in .bash_profile
SSHAGENT=/usr/bin/ssh-agent
SSHAGENTARGS="-s"

if [ -z "$SSH_AUTH_SOCK" -a -x "$SSHAGENT" ]; then
        eval `$SSHAGENT $SSHAGENTARGS`
        trap "kill $SSH_AGENT_PID" 0
fi

# Publish pub key to all nodes

mkdir ~/.ssh
chmod 750 ~/.ssh
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDcKkDQT7Ls/EqqEnONlUC218wdwX8xm0Qw/J2RxtPgdeP26NOjI4JDT/vvh0QNkJYMstgBtKqcIbEulaqjmXQEAWYtyRjoOLJqSMQbrR2hDpvM5VQ2SdqpS7/ZfRxvMa+7gd/D/4qKx8xQ85jmS4Q9gF7CeBHbosTpnbjalKXGfh1TcTpHEbgTdi//9+akp+ucHlGSqk8PWyP4ml20Lq6ZO4WSWoc51ihGlnGbQcrZqNaepxqs/BdhPd0OB9Hmz+Fb+D3sRLbeDFtvD69SQnpLMb6Ngz6+Mk9Rk2sYh5BjnNCQqZKO1AGg6pFaP0lBA8QA5hCZwyJSGIS59SzlRxf1 jonathanhudson@HLS-F10299" &gt;&gt; ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
cat ~/.ssh/authorized_keys

# Enable ident lookup in local ~/.ssh/config file

Host mn2rproxy0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2rproxy0001d2
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regcap0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regcap0001d2
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regrep0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regrep0001d2
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2rrpush0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2rrpush0001d2
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regsig0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regsig0001d2
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regpog0001d1
    IdentityFile ~/.ssh/id_jonathanhudson
Host mn2regpog0001d2
    IdentityFile ~/.ssh/id_jonathanhudson

# ideally but not possible
Edit your sshd_config file. Edit parameter "PermitRootLogin" to yes.
PermitRootLogin=yes

# Configure /etc/ansible/hosts for nodes

[dev_all]
mn2rproxy0001d1 ansible_user=jonathanhudson
mn2rproxy0001d2 ansible_user=jonathanhudson
mn2rproxy0001d1 ansible_user=jonathanhudson
mn2rproxy0001d2 ansible_user=jonathanhudson
mn2regcap0001d1 ansible_user=jonathanhudson
mn2regcap0001d2 ansible_user=jonathanhudson
mn2regrep0001d1 ansible_user=jonathanhudson
mn2regrep0001d2 ansible_user=jonathanhudson
mn2rrpush0001d1 ansible_user=jonathanhudson
mn2rrpush0001d2 ansible_user=jonathanhudson
mn2regsig0001d1 ansible_user=jonathanhudson
mn2regsig0001d2 ansible_user=jonathanhudson

# Test ansible connectivity

ansible -m ping test -k # prompts for ssh password
ansible -m ping test    # uses keys
ansible -m ping dev_all

# Create a test script that uses dzdo to raise privs

---
- hosts: test
  tasks:
  - name: Switch and check username
    shell: ". /data/home/d3s-dk/.bashrc &amp;&amp; echo 'I am user : ' &amp;&amp; whoami &amp;&amp; echo 'delivery' $delivery"
    register: hello
    become: yes
    become_user: d3s-dk
    become_method: dzdo
  - debug: msg="{{ hello.stdout }}"
  - debug: msg="{{ hello.stderr }}"

# Run test script to check users ( requires --ask-become-pass )

ansible-playbook test-priv-who.yml --ask-become-pass
</t>
<t tx="jonathanhudson.20201006084250.80">@language md
## Dev Testing

### JMeter

```
cd /opt/apache-jmeter-3.1/bin
./jmeter -n -t /data/code/safe-performance-testing/jmeter/denmark/safe-test-plan-normal-dev-54k-linux.jmx
```

### Monitor Heal status
```
watch -n 5 "gluster v heal dk-gluster-vol1 info"
```

### Monitor Files On brick
```
watch -n 5 "ls -al /data/bricks/dk-brick/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2019-09-25/ | wc -l"
```

### Disable GFS via Firewall
From root
```
cd ~/scripts
./disable-gfs-network.sh
```

### Enable GFS via Firewall
From root
```
cd ~/scripts
./enable-gfs-network.sh
```

### Monitor Bricks using RSync
```
rsync -avnc /data/bricks/dk-brick/SAFE_DATA/ mn2regrep0001d1:/data/bricks/dk-brick/SAFE_DATA/

rsync -avnc /mnt/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2019-10-01/ jonathanhudson@mn2regrep0001d1:/data/bricks/dk-brick/SAFE_DATA/d3s-dk/storage/Dictao2/folderstruktur-spilsystem/Zip/2019-10-01/ --exclude="*.xml"
```

### Run the Check GFS Ansible Scripts
```
ansible-playbook d-dk-check-gluster-volume.yml
```

### Checking the performance
```
gluster volume status [all| []] [detail|clients|mem|inode|fd|callpool]
```
#### Profile
```
gluster vol profile dk-gluster-vol1 start
gluster vol profile dk-gluster-vol1 info | more
gluster vol profile dk-gluster-vol1 stop
```
#### Top
list-cnt limits the count to top n entries
```
gluster vol top dk-gluster-vol1 read list-cnt 10
gluster vol top dk-gluster-vol1 write list-cnt 10
gluster vol top dk-gluster-vol1 open list-cnt 10
gluster vol top dk-gluster-vol1 opendir list-cnt 10
gluster vol top dk-gluster-vol1 read-perf bs 256 count 1 list-cnt 10
gluster vol top dk-gluster-vol1 write-perf bs 256 count 1 list-cnt 10
```

### Tests Performed

B1           = mn2regrep0001d1 ( brick 1 )
B2           = mn2regrep0001d2 ( brick 2 )
A1           = mn2regmok0001d0 ( arbiter 1 )
BvB          = Brick compared to Brick
BvS          = Brick compared to Source
Brick Status = Heal status of bricks maximum entries to heal

| Date        | Load               | Test                                                                   | Brick Status           | Heal Status            | BvB  | BvS  | Perf Mon  | Recovery              |
|-------------|--------------------|------------------------------------------------------------------------|------------------------|------------------------|------|------|-----------|-----------------------|
| 01/10 15:20 | 45.1/s avg for 30m | Split brain test disable Rep 1 (10m) and Rep 2 (10m) in isolation      | B1: 12, B2: 10, A1: 21 | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |
| 01/10 16:09 | 45.1/s avg for 30m | Reboot brick 2 server after 5m                                         | B1: 2, B2: 0, A1: 2    | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |
| 02/10 07:05 | 45.1/s avg for 30m | Disable rep 2 (20m)                                                    | B1: 26, B2: 0, A1: 26  | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |
| 02/10 07:45 | 45.1/s avg for 30m | Disable rep 1 (20m)                                                    | B1: 25, B2: 0, A1: 25  | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |
| 02/10 13:00 | 45.1/s avg for 30m | Run service using gluster volume as primary storage                    | B1: 0, B2: 0, A1: 0    | No healing required    | Same | Same | No impact | No recovery required  |
| 02/10 15:40 | 45.1/s avg for 30m | Run service using gluster volume as activemq storage for cap1          | B1: 0, B2: 0, A1: 0    | No healing required    | Same | Same | No impact | No recovery required  |
| 03/10 08:00 | 45.1/s avg for 30m | Run service using gluster volume as activemq storage for cap1 / cap2   | B1: 0, B2: 0, A1: 0    | No healing required    | Same | Same | No impact | No recovery required  |
| 04/10 08:00 | 45.1/s avg for 30m | Run service using gluster volume as primary storage disable cap2 20m   | B1: 16, B2: 0, A1: 16  | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |
| 04/10 08:00 | 45.1/s avg for 30m | Run service using gluster volume as primary storage disable cap1 20m   | B1: 0, B2: 16, A1: 16  | Heal entries cleared   | Same | Same | No impact | Success heal in 1m    |

#### Create files tests

10240 x 100K
1024 x 1M
100 x 10M
10 * 100M
1 x 1G

On local linux HD
```
time ./test-create-files.sh
real 360.798
user 4.906
sys 365.700
pcpu 102.72
```
On GlusterFS
```
time ./test-create-files.sh
real	3m48.782s
user	0m10.746s
sys	0m35.926s
```
</t>
<t tx="jonathanhudson.20201006084250.81">@language md
# GlusterFS Commands

### Peer Connections
Checking connections between GFS nodes
```
gluster peer status
gluster pool list
gluster peer probe &lt;server&gt;
```
### Start / Stopping
Starting and stopping GFS volumes
```
gluster volume start xx-gluster-vol1
gluster volume start xx-gluster-vol1 force  # If a brick is showing as Offline use force
gluster volume stop xx-gluster-vol1
```
### Create / Delete / Add / Remove / Replace
Modifying bricks within volumes
```
gluster volume delete xx-gluster-vol1
gluster volume create xx-gluster-vol1 replica 3 arbiter 1 transport tcp &lt;server-1&gt;:/data/bricks/xx-brick &lt;server-2:/data/bricks/xx-brick &lt;server-arbiter&gt;:/data/bricks/xx-brick
gluster volume add-brick xx-gluster-vol1 replica 4 &lt;server-3&gt;:/data/bricks/xx-brick
gluster volume remove-brick xx-gluster-vol1 replica 3 &lt;server-3&gt;:/data/bricks/xx-brick
gluster volume replace-brick xx-gluster-vol1 &lt;server-x&gt;:/data/bricks/xx-brick &lt;server-x&gt;:/data/bricks/yy-brick commit force
```
### Info
Retrieve information on the volume including parameters and configured bricks
```
gluster volume info all
gluster volume info xx-gluster-vol1
```
### Status
Get the current status of the bricks within the volume
```
gluster volume status
gluster volume status all detail
gluster volume status xx-gluster-vol1
gluster volume status xx-gluster-vol1 detail        # Good for looking at space available on volume
gluster volume status xx-gluster-vol1 clients
```
### Heal
Check the heal status and perform a manual heal
```
gluster volume heal xx-gluster-vol1
gluster volume heal xx-gluster-vol1 info
gluster volume heal xx-gluster-vol1 info summary

gluster volume heal xx-gluster-vol1 info split-brain
gluster volume heal xx-gluster-vol1 split-brain bigger-file /d3s-xx/.../&lt;file&gt;
gluster volume heal xx-gluster-vol1 split-brain latest-mtime /d3s-xx/.../&lt;file&gt;
gluster volume heal xx-gluster-vol1 split-brain source-brick &lt;server-1&gt;:/data/bricks/xx-brick /d3s-xx/.../&lt;file&gt;
```
### Adding Arbiter Brick
```
gluster volume add-brick xx-gluster-vol1 replica 3 arbiter 1 &lt;server-1&gt;:/data/bricks/xx-brick &lt;server-2&gt;:/data/bricks/xx-brick &lt;server-arbiter&gt;:/data/bricks/xx-brick
```
### Remove Brick
When removing a brick you start then check the status. Start causes the data to migrate to other bricks
When the data migration shown in the status command is complete, you can run the commit command to finalize the brick removal
```
gluster volume remove-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick start
gluster volume remove-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick status
gluster volume remove-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick commit
```
### Reset Brick
If a brick won't heal and it can't be resolved for any reason then you can reset the brick
Calling start will start the migration of data so you can check the status and then commit ( force only if want to do regardless of status )
```
gluster volume reset-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick start
gluster volume reset-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick status
gluster volume reset-brick xx-gluster-vol1 &lt;server&gt;:/data/bricks/xx-brick &lt;server&gt;:/data/bricks/xx-brick commit force 
```
### Setting Parameters
Setting the volume parameters
```
gluster volume set xx-gluster-vol1 nfs.disable on
gluster volume set xx-gluster-vol1 nfs.rpc-auth-allow 10.224.36.38
gluster volume set xx-gluster-vol1 auth.allow 10.224.36.38
gluster volume set xx-gluster-vol1 server.root-squash off
gluster volume set xx-gluster-vol1 features.cache-invalidation on
gluster volume set xx-gluster-vol1 cluster.min-free-disk 2GB
gluster volume set xx-gluster-vol1 cluster.min-free-disk 2GB
gluster volume set xx-gluster-vol1 storage.reserve 20
```
</t>
<t tx="jonathanhudson.20201006084250.82">@language md

## Knowledge Base

### Overview

A GlusterFS volume is made up of bricks ( also referred to as nodes in this article as each node has only one brick ). Each GFS volume has two data bricks and one arbiter brick. The arbiter brick does not store files only meta-data.

At the moment GlusterFS is only used for jurisdictions that have satellite systems with remote file share servers ( d3s-dk, d3s-es and d3s-bg )

***Important*** File operations that are performed on the GFS data should always be performed from the **mounted** volume and never from the raw data brick directly. 

To access files you can connect to a client server such as one of the capture servers and then access the mount *( fuse mounts use glusterfs not nfs )* which will be named `/mnt/GFS_DATA_XX` with XX being the two letters for the jurisdiction. i.e ES, DK, BG.

Each mount point connects to its own GFS volume only, so unlike SAFE_DATA where you can see all jurisdictions, GFS_DATA_DK will only show you data for jurisdiction d3s-dk ( same for d3s-es and d3s-bg ).

### Checking Data

If you want to check that the data on the **GFS_DATA_XX** volume is the same as the data on the **SAFE_DATA** volume then you can perform a quick rsync check for a specific date using examples below. 

Note the rsync commands below use the **-n** dry run option so no data will be transferred. Also depending on when you perform the rsync check you may see a difference especially if *File Copy* is in the process of copying ZIP files between SAFE_DATA and the GFS volume

If File Copy was running at the time you perform the rsync check you may depending on the jurisdiction see some difference i.e. DK you may see many XML files and perhaps a single zip, this is because "File Copy" only copies ZIP files not XML files which are only copied at timed intervals.

**For DK**
```
rsync -avcn /mnt/SAFE_DATA/d3s-dk/storage/&lt;Tenant&gt;/folderstruktur-spilsystem/Zip/YYYY-MM-DD/ /mnt/GFS_DATA_DK/d3s-dk/storage/&lt;Tenant&gt;/folderstruktur-spilsystem/Zip/YYYY-MM-DD/
```
**For ES**
```
rsync -avcn /mnt/SAFE_DATA/d3s-es/storage/&lt;Tenant&gt;/CNJ/&lt;Vault&gt;/JU/YYYYMMDD/ /mnt/GFS_DATA_ES/d3s-es/storage/&lt;Tenant&gt;/CNJ/&lt;Vault&gt;/JU/YYYYMMDD/ 
```
**For BG**
```
rsync -avcn /mnt/SAFE_DATA/d3s-bg/storage/b365sports/data/YYYY/MM/DD/ /mnt/GFS_DATA_BG/d3s-bg/storage/b365sports/data/YYYY/MM/DD/ 
```

### Heal Alerts

In the below examples I'm using the DK volume but each jurisdiction has its own volume listed below
```
DK      dk-gluster-vol1
ES      es-gluster-vol1
BG      bg-gluster-vol1
```
Each volume contains only data that relates to the jurisdiction so the DK volume will only contain d3s-dk so you would only expect to see **`/mnt/GFS_DATA_DK/d3s-dk`**

#### Safe-Mon Alerts

If **safe-mon** reports an alert stating that there are outstanding heal entries on the GFS volume you would need to check the *disk usage* and the *node connection status*. 
Heal entries are files that are yet to be copied to all nodes within the GFS volume.

Having outstanding heal entries on only one node in the cluster does not mean that the data is not accessible in the cluster. The client will still see the correct data on the GFS mount though the healing process is ongoing.

First take a look at the volume and check the bricks that belong to the volume
```
gluster volume info dk-gluster-vol1
```
This will show you the bricks that make up the volume. Take a note that one of the bricks is marked as an **arbiter** brick.
```
Bricks:
Brick1: mn2regrep0001u1:/data/bricks/dk-brick
Brick2: mn2regrep0001u2:/data/bricks/dk-brick
Brick3: mn2rstats0001u0:/data/bricks/dk-brick (arbiter)
```
Here you can see that `Brick3` is the **arbiter** which means that it stores meta-data about the files in the GFS index but *NOT* the actual files themselves. The purpose of the arbiter brick is to prevent **split brain** which can happen when two nodes are running in isolation and data is changed resulting in two different sets of data with no clear indication which copy is correct.

Now check the heal status using the command
```
gluster volume heal dk-gluster-vol1 info
```
you should see something like this
```
Brick mn2regrep0001u1:/data/bricks/dk-brick
Status: Connected
Number of entries: 0

Brick mn2regrep0001u2:/data/bricks/dk-brick
Status: Connected
Number of entries: 0

Brick mn2rstats0001u0:/data/bricks/dk-brick
Status: Connected
Number of entries: 0
```
Notice each node is listed along with its connection status and number of outstanding heal entries.

If all is OK each node will show a `Status: Connected` and `Number of entries: 0`

If there are heal entries you will see the files listed and the number of entries that need to be healed

### Heal Entry Causes
#### Disk Utilization

Check if the disk utilization is at 80% use ( this wouldn't normally happen as the data we retain should not exceed 80% of 1TB )

If the disk utilization is at 80% then the GFS volume will no longer be accessible. This is intentional and is designed to prevent the volume from reaching 100% capacity. GlusterFS will automatically unmount volumes that reach capacity and then prevents you from mounting the GFS volume. At this point you can't mount the volume and therefore you can't easily remove data from the volume to free up space.
    
Two options exist either get more disk space or remove files from the volume to free up space.

Under extreme circumstances you can increase the 80% threshold using the command below but this should be reverted as soon as possible.
Notice that this property controls the percentage of storage to reserve before limiting write access to the volume

```
gluster volume set dk-gluster-vol1 storage.reserve 20
```
then check the property using
```
gluster volume info dk-gluster-vol1
```
looking for the updated **`storage.reserve`** value
You can then see if the volume is accessible again by switching to the folder **`/mnt/GFS_DATA_DK`** and attempting to create a file

#### Disconnection Nodes

Check if any of the nodes are disconnected ( this can happen due to network errors )

To check if a peer is disconnected you need to log on to one of the nodes in the cluster and view the peer status;

```
gluster peer status
```
also check the volume info with
```
gluster volume info dk-gluster-vol1
```
Check the clients are all connected. If they are **not** it will show as 'Disconnected' when you run the peer status command

If any nodes are not connected you will need to log on to the disconnected server and start the volume using the command below
```
gluster volume start dk-gluster-vol1 force
```
Then check all nodes are connected using the peer status command. If they are all connected you can check the heal status
```
gluster volume heal dk-gluster-vol1 info
```
And if any require healing you can perform the heal manually using
```
gluster volume heal dk-gluster-vol1
```
#### Ports Used by GlusterFS

Ports tcp **24007**-**24008** are required for communication between GlusterFS nodes and each brick requires another tcp port starting at 49152.

If for some reason nodes are disconnected from the gluster you should check connectivity between each peer in the volume and also each client connecting to the volume

You can see the ports being used for each brick
```
gluster volume status dk-gluster-vol1
```
You can check the status of clients using the following command
```
gluster volume status dk-gluster-vol1 clients
```
Ensure that TCP and UDP ports are accessible
**24007**,
**24008**

One port for each brick starting from port
**49152** - **49156**
</t>
<t tx="jonathanhudson.20201006084250.83"></t>
<t tx="jonathanhudson.20201006084250.84">@language md

### Notes

#### Looks like files have been removed from the brick and the symlinks / indices have been left in the lurch
```
gluster volume heal dk-gluster-vol1 info

Brick mn2regrep0001u1:/data/bricks/dk-brick
&lt;gfid:9bfe4f68-2d7c-4c4c-8aa2-06e06d35bbc6&gt;
Status: Connected
Number of entries: 1

Brick mn2regrep0001u2:/data/bricks/dk-brick
Status: Connected
Number of entries: 0

Brick mn2rstats0001u0:/data/bricks/dk-brick
&lt;gfid:d72a6f27-1681-4881-99b9-51343d79187a&gt;
&lt;gfid:65c87777-3fc4-4f96-9c49-1ac1fb9447c4&gt;
&lt;gfid:bf65d9a1-491f-4601-b6e5-127c16269977&gt;
&lt;gfid:dba87681-2545-48fe-a4b2-afa46ac6e1ca&gt;
&lt;gfid:bb753db5-b905-4660-9497-3f3d54cba47b&gt;
&lt;gfid:0e5bfa8f-b743-49ba-9ec5-bf9fdf23de47&gt;
&lt;gfid:5605b3fe-7de3-4c90-b38a-3730873911aa&gt;
```
#### For each of the gfids we need to check the symlink
```
find .glusterfs/ -name '65c87777-3fc4-4f96-9c49-1ac1fb9447c4'
-&gt; .glusterfs/indices/xattrop/65c87777-3fc4-4f96-9c49-1ac1fb9447c4
-&gt; .glusterfs/65/c8/65c87777-3fc4-4f96-9c49-1ac1fb9447c4         &lt;---- symlink broken

getfattr -d -e hex -m . /data/bricks/dk-brick/.glusterfs/indices/xattrop/d72a6f27-1681-4881-99b9-51343d79187a
getfattr -d -e hex -m . /data/bricks/dk-brick/.glusterfs/d7/2a/d72a6f27-1681-4881-99b9-51343d79187a

#Error too many levels of symbolic links
```
#### For each scenario where the brick file has been deleted we need to unlink and remove the indices to tidy up
```
ls -al .glusterfs/65/c8/65c87777-3fc4-4f96-9c49-1ac1fb9447c4 &lt;--- was linked to a missing file so need to remove the indices and unlink
lrwxrwxrwx 1 root root 59 Mar 14 03:10 .glusterfs/65/c8/65c87777-3fc4-4f96-9c49-1ac1fb9447c4 -&gt; ../../ce/c5/cec51d30-838b-4330-9af1-304289a2aca1/2020-03-14

unlink .glusterfs/65/c8/65c87777-3fc4-4f96-9c49-1ac1fb9447c4
rm .glusterfs/indices/xattrop/65c87777-3fc4-4f96-9c49-1ac1fb9447c4
```
</t>
<t tx="jonathanhudson.20201006084250.85">@language md
### Checking the performance
```
gluster volume status [all| []] [detail|clients|mem|inode|fd|callpool]
```
#### Profile
```
gluster vol profile dk-gluster-vol1 start
gluster vol profile dk-gluster-vol1 info | more
gluster vol profile dk-gluster-vol1 stop
```
#### Top
list-cnt limits the count to top n entries
```
gluster vol top dk-gluster-vol1 read list-cnt 10
gluster vol top dk-gluster-vol1 write list-cnt 10
gluster vol top dk-gluster-vol1 open list-cnt 10
gluster vol top dk-gluster-vol1 opendir list-cnt 10
gluster vol top dk-gluster-vol1 read-perf bs 256 count 1 list-cnt 10
gluster vol top dk-gluster-vol1 write-perf bs 256 count 1 list-cnt 10
```

## Performance FOPs File Operations

    ACCESS - ?
    CREATE - C - create a file
    DISCARD - support for trim?
    ENTRYLK - lock a directory given its pathname?
    FALLOCATE - allocate space for file without actually writing to it
    FENTRYLK - lock a file given its handle
    FGETXATTR - C - get named extended attribute value for a file (handle)
    FINODELK - C - lock a file/directory for write/read
    FLUSH - ensure all written data is persistently stored
    FREMOVEXATTR - remove a named extended attribute from a file handle
    FSETATTR - set value of metadata field (which ones?) for a file (handle)
    FSETXATTR - C - set value of a named extended attribute for a file handle
    FSTAT - get standard metadata about a file given its file handle
    FSYNC - C - ensure all written data for a file is persistently stored
    FSYNCDIR - ensure all directory entries in directory are persistently stored
    FTRUNCATE - set file size to specified value, deallocating data beyond this point
    FXATTROP - C - used by AFR replication?
    GETXATTR - get value of named extended attribute
    INODELK - lock a directory for write or for read
    LINK - create a hard link
    LK - lock?
    LOOKUP - C - lookup file within directory
    MKDIR - C - create directory
    MKNOD - create device special file
    OPEN - C - open a file
    OPENDIR - C - open a directory (in preparation for READDIR)
    RCHECKSUM - ?
    READ - C - read data from a file
    READDIR - C - read directory entries from a directory
    READDIRP - C - read directory entries with standard metadata for each file (readdirplus)
    READLINK - get the pathname of a file that a symlink is pointing to
    RELEASE - C - let go of file handle (similar to close)
    RELEASEDIR - let go of directory handle (similar to close)
    REMOVEXATTR - remove a named extended attribute from a pathname?
    RENAME - C - rename a file
    RMDIR - C - remove a directory (assumes it is already empty)
    SETATTR - set field in standard file metadata for pathname
    SETXATTR - C - set named extended attribute value for file given pathname
    STAT - C - get standard metadata for file given pathname
    STATFS - get metadata for the filesystem
    SYMLINK - create a softlink to specified pathname
    TRUNCATE - truncate file at pathname to specified size
    UNLINK - C - delete file
    WRITE - C - write data to file
    XATTROP - ?
    ZEROFILL - write zeroes to the file in specified offset range

## File IO scripts
generate-files.sh
```
for i in $(seq 1 $NUMBER);
do
 dd if=/dev/urandom of=$TARGET/file_$i bs=$SIZE count=$COUNT 2&gt;&amp;1 | grep -v records
done
```
```
# Creating 10240 files of 100k

export NUMBER=10240
export TARGET=`pwd`/100k
export SIZE=100K
./generate-files.sh &gt; 100k.log
```
```
# Creating 1024 files of 1M

export NUMBER=1024
export TARGET=`pwd`/1M
export SIZE=1M
./generate-files.sh &gt; 1M.log
```
```
# Creating 100 files of 10M

export NUMBER=100
export TARGET=`pwd`/10M
export SIZE=10M
./generate-files.sh &gt; 10M.log
```
```
# Creating 10 files of 100M

export NUMBER=10
export COUNT=100
export TARGET=`pwd`/100M
export SIZE=1M
./generate-files.sh &gt; 100M.log
```
```
# Creating 1 file of 1G

export NUMBER=1
export TARGET=`pwd`/1G
export SIZE=1M
export COUNT=1024

./generate-files.sh &gt; 1G.log
```
</t>
<t tx="jonathanhudson.20201006084250.86">c.frame.log.selectTab('Log')
c.frame.log.clearLog()
current = c.p
print("exclusive subtree of %s" % (current.h))
for p in current.subtree():
#    print(" - [ ] "+p.h)
    g.es(" - [ ] "+p.h)</t>
<t tx="jonathanhudson.20201006084250.87" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.88" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.89" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.9" __bookmarks="7d7100580700000069735f6475706571014930300a732e"># Edit sudores file

vi /etc/sudoers

jonathanhudson ALL=(root) NOPASSWD: /bin/su
jonathanhudson ALL=(safe-mon) NOPASSWD: /bin/sh

# Then you can 

sudo su - safe-mon

sudo -u safe-mon /bin/sh echo -c 'Hello'</t>
<t tx="jonathanhudson.20201006084250.90" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.91" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.92" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.93" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.94" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.95" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.96" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.97" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.98" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006084250.99" annotate="7d71002858080000007072696f7269747971014b13580a000000707269736574646174657102580a000000323032302d30312d32387103752e" icons="5d71007d7101285804000000747970657102580400000066696c657103680358340000002f646174612f702d636f64652f6c656f2d656469746f722f6c656f2f49636f6e732f636c656f2f63686b626f78626c6b2e706e677104580700000072656c5061746871055812000000636c656f2f63686b626f78626c6b2e706e677106580500000077686572657107580e0000006265666f7265486561646c696e6571085807000000796f666673657471094b005807000000786f6666736574710a4b02580400000078706164710b4b0158020000006f6e710c5805000000766e6f6465710d5808000000636c656f49636f6e710e580100000031710f75612e"></t>
<t tx="jonathanhudson.20201006092251.1">@language md

#### Notes

I would suggest setting up httpd to proxy sft requests for you. 

This way the HSTS - and other security headers -  are lost at the apache level ( same as in production ). 

Should you have HSTS’d yourself though you can remove the domain from the browser with the below steps. Note that this is only possible because our LCL boxes aren’t ( shouldn’t be ) in the preload list. You’ll need to do this each time you log in to sft directly ( i.e on port 60443 ), for whatever reason no security headers are returned at all on the login page.

Steps to remove a domain from the HSTS cache in chrome:

•	Go to chrome://net-internals/#hsts
•	Check that HSTS is causing the issue by entering your hostname in the “Query HSTS/PKP domain"
•	If you get something like this returned then HSTS is causing the issue, if it’s empty then HSTS is not what troubles you
</t>
<t tx="jonathanhudson.20201006132701.1">@language md

#### System Management Daemon (SMD)

All day-to-day WX2 administrative tasks are performed through the SMD

#### RAMStore (RS)

The RAM Store manipulates data in RAM

#### DiskStore (DS)

The Disk Store manipulates data on disk

#### Compiler (Comp) ( CG )

The Compiler compiles a SQL query for subsequent execution

#### Interpreter (Int)

The Interpreter runs previously compiled SQL queries

#### Input/Output (IO)

The IO process handles communication with the user

#### Miscellaneous (Misc)

The Misc process performs a variety of other database tasks
</t>
<t tx="jonathanhudson.20201006150145.1">@language md

The WX2 system table ‘ipe_command’ records all submitted SQL statements and is persistent across database restarts. It contains one row per statement.
FAQ: What does the ‘status’ column on ‘ipe_command’ signify?


- status=0 – Statement executed successfully;
- status&lt;&gt;0 – Statement did not execute successfully, for example, there was a syntax error.

More generally, use the SQL 'explain &lt;table or view&gt;' to see a description of what each column in a table means. 
FAQ: What does the ‘operation’ column on ‘ipe_transaction’ signify?

- operation=2 – Transaction started;
- operation=3 – Transaction finished and committed (i.e. completed successfully);
- operation=4 – Transaction finished and rolled back(i.e. transaction cancelled, with any data changes reverted)

More generally, use the SQL 'explain &lt;table or view&gt;' to see a description of what each column in a table means. 
FAQ: What does the ‘command_running’ column on ‘ipe_cursessions’ signify?

It signifies the status of the most recent SQL run in that session. Statuses are:

0: Session idle (command completed)
1: Command running
2: Statement rollback
3: Transaction rollback
4: Command queued, waiting for a queue slot to open
5: Command being compiled
6: Waiting for interpreter
</t>
<t tx="jonathanhudson.20201006150454.1">@language md

All faq solutions in one page
FAQ: What is an MPP database system?

FAQ: What is the WX2 database system?

FAQ: What is so special about WX2 being an ‘in-memory’ database? Don’t all database systems have a RAM-based data cache?

FAQ: I don’t want to be at the bleeding edge of technology. Aren’t MPP systems highly exotic, difficult to configure and expensive to administer?

FAQ: I’ve heard it all before when it comes to ‘linear scalability’. Why should I believe that WX2 actually delivers on this promise where others have failed?

FAQ: I don’t have the time or the resources to plan and manually distribute my database across multiple servers as required by the WX2 system?

FAQ: The WX2 system, and MPP technology in general, sounds very advanced, but shouldn’t I wait a bit for the technology to mature before I invest in it?

FAQ: All my OLTP systems run on Oracle and I have a large pool of internal Oracle resource and expertise. How can I justify to my Board using WX2, a niche, non-Oracle warehousing solution, when I have all this Oracle resource already available?

FAQ: Are the terms ‘server’, ‘blade’ and ‘node’ interchangeable when referring to WX2?

FAQ: Parallel query processing in WX2 is all very well, but are other aspects of database operation, maintenance and administration, such as joins, inserts, updates and backups, fully parallel as well?

FAQ: What happens to the data stored on a server that fails in operation? Does that portion of the database become inaccessible?

FAQ: I think I’ve heard of the WhiteCross/WX DES/WX2 system before, but didn’t it run on proprietary hardware?

FAQ: Does WX2 only support servers from particular manufacturers?

FAQ: We have racks of Intel x86 servers, but they aren’t blades. Can we run the WX2 system on these servers and still realise the claimed performance benefits?

FAQ: Can servers of differing specifications be used to create a single WX2 database?

FAQ: Can hardware from more than one supplier be used to create a single WX2 database?

FAQ: Is the number of servers assigned to a WX2 database fixed, or can it be varied in response to fluctuating demand?

FAQ: Is a WX2 node in any way ‘special’, or can it be re-deployed for any purpose?

FAQ: Our corporate standards dictate that a shared SAN infrastructure must be used for all significant data storage. Will the WX2 system work in a shared SAN environment?

FAQ: How easy is it to add one or more new servers to an existing WX2 system?

FAQ: I like the idea of the WX2 system, but I don’t know much about Linux or blades. Do you offer pre-configured hardware/software bundles that don’t require detailed Linux or blade knowledge?

FAQ: We already have enough problems administering a handful of database servers. How can we possibly administer a WX2 system potentially composed of dozens or hundreds of servers?

FAQ: Can data be loaded directly into memory?

FAQ: How many network interface ports should each WX2 server have?

FAQ: Does WX2 require a specialised interconnect to allow all the nodes to communicate with each other at high speed?

FAQ: Can a WX2 node be run without any disk storage?

FAQ: Which Linux distributions does WX2 run on?

FAQ: Can I use the special Red Hat Enterprise Linux For HPC Compute Nodes subscription to run WX2?

FAQ: Do any of the Linux vendors offer special licencing deals for large numbers of blade servers?

FAQ: Does WX2 comply with the LSB (Linux Standard Base) specification?

FAQ: Is WX2 a 32-bit or 64-bit application?

FAQ: What types of disk storage does WX2 support?

FAQ: What type of Linux partition does WX2 use?

FAQ: Along with many other organisations, we have standardised on Microsoft Windows and do not plan to use Linux. Why do you not offer a Windows version of WX2?

FAQ: The WX2 system seems to create a very large number of processes in Linux when it is running. Is this normal?

FAQ: The Linux load average seems very high when WX2 is running. Is this a cause for concern?

FAQ: Does WX2 require a Linux cluster file system to operate?

FAQ: Does WX2 use the industry-standard MPI (Message Passing Interface) for communication amongst nodes?

FAQ: Do all nodes in a WX2 system run an identical set of WX2 processes?

FAQ: Which versions of ANSI SQL does WX2 support?

FAQ: Which version of ODBC does WX2 support?

FAQ: How does WX2 cope with Oracle SQL statements?

FAQ: What is the difference between ‘create table…as select’ and ‘create table…for select’?

FAQ: What is the WX2 equivalent to Oracle’s SQL*Plus command line utility?

FAQ: If WX2 does not have indices, how can it support constraint checks such as ‘primary key’ and ‘unique’?

FAQ: What are the WX2 architectural limits?

FAQ: What features of WX2 minimise administrative overhead?

FAQ: How do I archive and restore a WX2 system?

FAQ: I understand that WX2 minimises administrative overhead, but what tasks still need to be undertaken?

FAQ: How would I typically go about tuning a WX2 system?

FAQ: What is a reclaim and how long does it take?

FAQ: What WX2 system events can be monitored?

FAQ: How does WX2 interface with Linux security?

FAQ: What database security features are present in WX2?

FAQ: What is the WX2 'embedded client'?

FAQ: What is connection 'bouncing’ and connection forwarding?

FAQ: Why does the WX2 ODBC driver have space for multiple server IP addresses?

FAQ: Does WX2 keep a record of all SQL statements submitted for processing?

FAQ: What does the ‘status’ column on ‘ipe_command’ signify?

FAQ: What does the ‘operation’ column on ‘ipe_transaction’ signify?

FAQ: What does the ‘command_running’ column on ‘ipe_cursessions’ signify?

FAQ: What is the structure of the WX2 version numbering scheme?

FAQ: Where is the WX2 root installation directory on Linux/Solaris?

FAQ: Where are the WX2 configuration files stored on Linux/Solaris?

FAQ: Where is the WX2 root log directory on Linux?

FAQ: What are the outline steps required to create a working WX2 system from an empty set of servers?

FAQ: Is WX2 a ‘non-stop’ database system?

FAQ: Why would I use WX2 rather than a column-based database?

FAQ: The terms column-based and tokenisation are often used together. Do they mean the same thing?

FAQ: Why is ‘in-memory’ different to using a conventional disk-based database with a very large cache?

FAQ: What are database indices?

FAQ: How does WX2 compare with Sybase IQ?

FAQ: How does WX2 compare with Netezza?

FAQ: How does WX2 compare with Teradata?

FAQ: What date formats do wxloader/wxunloader/wximport/wxexport support?

FAQ: What data formats are supported by wximport and wxexport?

FAQ: Which query queue is used by SYS?

FAQ: How does the -d parameter in wximport / wxloader empty the contents of the table?

FAQ: Are concurrent parallel import and export streams subject to query queueing?

FAQ: Does the choice of network interfaces made during installation affect subsequent external connectivity to the WX2 server?

FAQ: What are the byte alignment rules for WX2?

FAQ: Is a create image performed after a reclaim?

FAQ: Is there a way of updating the definition of a view without having to drop all the objects that depend on that view?

FAQ: Why do I sometimes receive an 'Invalid date value' error when I add a 1 month interval to the last day of a month?

FAQ: What is the range of valid exit codes for wxsubmit?

FAQ: Is it possible to have subqueries inside case statements with WX2?

FAQ: Will the whole WX2 server crash if an AP node crashes?

FAQ: Can I shut down one of the AP nodes without stopping the rest of the WX2 server?

FAQ: Why is 'wxprobe -H' reporting the wrong MHz values for my CPUs?

FAQ: How many variables can be defined in wxsubmit?

FAQ: Is a table or view image replicated per node or per RAMStore on each node?

FAQ: Does WX2 support disk quotas?

FAQ: What are sm_time and tm_time in ipe_command measured in?

FAQ: Is there a suggested strategy for performing reclaims on a slab-by-slab basis?

FAQ: Is logging produced on all nodes or just master nodes and is there any difference between the log files on each node?

FAQ: Can you update the statistics on ROTTs?

FAQ: Do Kognitio alter the 'standard' Linux kernel in any way in order to run WX2?

FAQ: Is it possible to identify reclaimable space created by TRUNCATE TABLE?

FAQ: Does WX2 support synonyms?

FAQ: What is the scope of ALL privileges in its various security domains?

FAQ: Why do I sometimes see one node using much more RAM than all the others when running SQL windowing queries?

FAQ: Does the -z option in wxbackup perform the compression on the server running the wxbackup command or on the DB node performing the extraction?

FAQ: What size is a Cluster Unit (CU)?

FAQ: What are the performance benefits to using truncate table instead of delete from to empty a table?

FAQ: How do compressed indices work?

FAQ: Does WX2 support BLOB/CLOB large binary objects?

FAQ: What predicates or expressions will trigger the use of an applicable Compressed Index (CI)?

FAQ: What protocols are used by WX2 to discover new nodes and how is this discovery restricted to an appropriate domain?

FAQ: Does WX2 support consistent reads?

FAQ: What level of Oracle compatibility does WX2 offer?

FAQ: What is a disk store slab?

FAQ: Can I commit and rollback DDL commands when running in transactional mode?

FAQ: Can I rollback a truncate table statement?

FAQ: Can I edit SQL commands in wxsubmit using my favourite text editor?

FAQ: Does WX2 offer any facilities to help profile data?

FAQ: Does WX2 support multi-row inserts?

FAQ: Why does WX2 not make use of OLAP technology to speed up queries?

FAQ: What Linux/Solaris swap setting should be used with WX2?

FAQ: What do the timings from wxsubmit mean

FAQ: How does WX2 licencing work?

FAQ: What is a reasonably level of RAM occupancy

FAQ: What is the precision of REAL/FLOAT/DOUBLE?

FAQ: How can I use virtual tables to find out what WX2 is doing?

FAQ: How can I use the ipe_allcursessions virtual table?

FAQ: How can I use the ipe_allram_access virtual table?

FAQ: How can I used the ipe_allram_images virtual table?

FAQ: How can I used the ipe_disk_access virtual table?

FAQ: How can I use the ipe_disk_slab virtual table?

FAQ: How can I used the ipe_ftable virtual table?

FAQ: How can I used the ipe_query_queues virtual table?

FAQ: How can I used the ipe_query_queue_stats virtual table?

FAQ: Create or replace view [image]

FAQ: What can be put in the configuration file?

FAQ: Problems caused by security extension SELinux.

FAQ: troubleshooting WX2 install problems

FAQ: How do I recover if Kognitio is down?

FAQ: How do I access the RAM and Disk monitoring facilities in Control Tower

FAQ: How do I give an AP node full DB capabilities?

FAQ: Investigating performance issues (locking, queueing, poor queries, ...)

FAQ: What is the optimal number of streams for a parallel import

FAQ: Why are user parameters within WX2 not documented?

FAQ: Common problems with non-appliance installs

FAQ: How does WX2 message-passing work?

FAQ: Kognitio troubleshooting

FAQ: ODBC issues

FAQ: How can I use the ipe_alllocks virtual table?

FAQ: wxpimport troubleshooting

FAQ: What to do if the Linux OOM killer kills WX2 processes

FAQ: WX2 disk usage, and how to tell when disks or slabs fill up

FAQ: Why does the wxsmd process appear to be busy when viewed through wxtop?

FAQ: Why should each link used by the MPK exist on a separate subnet?

FAQ: can I abort or interrupt a reclaim?

FAQ: why am I getting variable results for a query?

FAQ: How does streaming affect query performance?

FAQ: what is the maximum length for a system_id?

FAQ: How do I list out tables that are skewed on disk

FAQ: License key issues

FAQ: Commissioning issues

FAQ: What to do if "Initialising disks" is slow / hanging

FAQ: How do I use the import/export API

FAQ: Understanding ODBC AutoCommit behaviour

FAQ: How to upgrade versions

FAQ: Performance figures

FAQ: Will ALTER TABLE generate a new copy of my table?

FAQ: My backup/data load/other admin task fails when run from cron

FAQ: CREATE TABLE IMAGE is slow - what should I check?

FAQ: How does locking work in Kognitio?

FAQ: All about CSV

FAQ: Kognitio Console failed to read configuration file - how can I resolve this?

FAQ: Deskew does not result in disks being balanced

FAQ: External Scripts FAQ

FAQ: How big can Kognitio disk resources be?

FAQ: NUMA awareness with KAP, NUMA skew, and resulting OOM issues

FAQ: what options should I consider to maximise resilience?

FAQ: Debugging WX2 crashes

FAQ: What is an MPP database system?
Solution 00000018 by Stuart Watt at 2008-01-29T13:48:33.000+0000
An MPP (Massively Parallel Processing) database system is one in which a number of identically-configured, discrete servers are combined to form a single system. The data in the database is evenly distributed across the servers and each server is then able to work independently and in parallel on its own portion of the data to satisfy a query. The major advantage to this approach is the unrestricted parallelism that can be brought to bear to the analysis of massive volumes of data, allied to true linear scalability, wherein each new server added contributes materially to the overall power of the environment.
FAQ: What is the WX2 database system?
Solution 00000019 by Stuart Watt at 2008-01-29T13:49:52.000+0000
WX2 is an ANSI SQL-compliant, memory-based, MPP relational database system specifically designed to meet the particular challenges of extremely high-volume, complex data analytics. By running on arrays of low-cost, industry-standard x86 servers, WX2 has an industry-leading price/performance capability, in a space where competing solutions rely on costly and inflexible proprietary hardware platforms.
FAQ: What is so special about WX2 being an ‘in-memory’ database? Don’t all database systems have a RAM-based data cache?
Solution 00000020 by Stuart Watt at 2008-01-29T13:50:56.000+0000
Most database systems assume that data will be essentially disk-resident, with search algorithms, buffer management and indexing techniques all biased towards this fundamental assumption. Even when these systems are configured to hold cached copies of data in memory, their performance is badly affected by the deeply-woven assumptions of disk-based residency.

In sharp contrast, WX2 explicitly copies all or part of the database into RAM and was designed from scratch on this assumption. It can therefore take direct routes to memory, simplifying algorithms and typically reducing code path lengths by a factor of ten. WX2 is also explicitly designed to effectively manage hundreds of gigabytes, or even terabytes of RAM, enabling entire databases to be made memory-resident. This means queries can run up to ten times faster than in more traditional disk-oriented systems, whose RAM caches are normally measured in tens of gigabytes at best.

Please see "Why is ‘in-memory’ different to using a conventional disk-based database with a very large cache?" for a fuller discussion of this topic.
FAQ: I don’t want to be at the bleeding edge of technology. Aren’t MPP systems highly exotic, difficult to configure and expensive to administer?
Solution 00000021 by Stuart Watt at 2008-06-23T14:51:45.000+0000
In the past, MPP database systems were rare and many of the practical difficulties associated with their administration had not been fully resolved. However, the WX2 MPP database system has been continuously developed and revised over a period of 20 years and, as a consequence, the unique challenges involved in creating a scaleable and reliable MPP environment have been comprehensively resolved.
FAQ: I’ve heard it all before when it comes to ‘linear scalability’. Why should I believe that WX2 actually delivers on this promise where others have failed?
Solution 00000022 by Stuart Watt at 2008-06-25T14:10:57.000+0000
The WX2 database system is today a software-only product running on industry-standard x86 servers. However, in the early days of the software’s development, commodity hardware was not yet sufficiently powerful to support the demands of very large MPP environments and so a combined hardware/software appliance was offered. The limited nature of hardware available back then dictated that a very large number of nodes would be required to achieve good performance, with 1,000 node systems being not uncommon. While WX2 no longer requires the proprietary hardware, the expertise of being able to sensibly scale to thousands of nodes lives on in the software, ensuring linear scalability on modern platforms.
FAQ: I don’t have the time or the resources to plan and manually distribute my database across multiple servers as required by the WX2 system?
Solution 00000023 by Stuart Watt at 2008-01-29T13:54:32.000+0000
The distribution of data across the server nodes in a WX2 system is entirely automatic and requires no intervention whatsoever on the part of administrators. As data is loaded into WX2 using its import utility, the data is automatically distributed in a round-robin fashion across the available server nodes.
FAQ: The WX2 system, and MPP technology in general, sounds very advanced, but shouldn’t I wait a bit for the technology to mature before I invest in it?
Solution 00000024 by Stuart Watt at 2008-06-25T12:04:23.000+0000
One of the unique strengths of the WX2 system is that although it has been available on industry-standard blades for the last four years, the core software components that underpin it have been intensively developed over the last 20 years on similar, proprietary, hardware environments. This means that all the unique challenges involved in creating a scaleable and reliable MPP environment have already been solved, allowing the potential of the new industry-standard blade platforms to be exploited to the full. MPP is also much better understood within the computer industry today, with many more vendors exploiting the architecture to produce powerful computer systems.
FAQ: All my OLTP systems run on Oracle and I have a large pool of internal Oracle resource and expertise. How can I justify to my Board using WX2, a niche, non-Oracle warehousing solution, when I have all this Oracle resource already available?
Solution 00000025 by Stuart Watt at 2015-11-04T17:04:35.000+0000
WX2 can co-exist with your current Oracle environment.  WX2 simply runs those aspects of the analysis that require very high performance, leaving the OLTP elements to Oracle where it is most suited. The aim is to enhance Oracle, not replace it.


The WX2 system supports a wide variety of Oracle compatibility features to enable successful co-existence.
FAQ: Are the terms ‘server’, ‘blade’ and ‘node’ interchangeable when referring to WX2?
Solution 00000026 by Stuart Watt at 2015-11-04T17:09:59.000+0000
A WX2 system is composed of a number of discrete x86 servers, each running their own copy of the Linux operating system. If these servers have been packaged into an ultra-dense configuration, then they are normally referred to as ‘blades’, otherwise they are typically rack-mount servers.  A ‘node’ in WX2 refers to one of its constituent x86 servers. Therefore, the terms ‘server’, ‘blade’ and ‘node’ can be used interchangeably within WX2, although ‘node’ is the most generic.
FAQ: Parallel query processing in WX2 is all very well, but are other aspects of database operation, maintenance and administration, such as joins, inserts, updates and backups, fully parallel as well?
Solution 00000027 by Stuart Watt at 2008-01-29T14:00:43.000+0000
WX2 was designed for total parallelism from day one; it is not an adaptation or conversion of another product. All key database operations are performed in parallel, including joins, inserts, updates, deletes and major administrative functions.
FAQ: What happens to the data stored on a server that fails in operation? Does that portion of the database become inaccessible?
Solution 00000028 by Stuart Watt at 2008-06-25T12:22:39.000+0000
The WX2 system offers inter-server software RAID, which ensures full access to the data even in the event of a total server failure. Only the CPU and RAM processing power of that particular server is lost, with the remainder of the system, and all of the data, still available for use. This software RAID capability is complementary to any hardware RAID functionality offered by the underlying server platform.


Please see "Is WX2 a ‘non-stop’ database system?" for a more detailed discussion of the fault-tolerant features of WX2.
FAQ: I think I’ve heard of the WhiteCross/WX DES/WX2 system before, but didn’t it run on proprietary hardware?
Solution 00000029 by Stuart Watt at 2008-06-25T12:31:31.000+0000
The WX2 system was originally developed by a company called WhiteCross and ran on proprietary hardware. In 2003, the WX2 server was ported to industry-standard x86 servers and proprietary hardware development ceased. In August 2005, WhiteCross merged with Kognitio.

A brief timeline of WX2 development is:
1988 – Development of MPP database started;
1992 – First-generation proprietary hardware/software product released (9000 Series) using Inmos Transputer-based ‘blade’ architecture;
1996 – Database ported to Intel x86 processor;
1998 – Second-generation proprietary hardware/software product (9800 Series) released using Intel x86-based ‘blade’ architecture (WX/DES);
2003 – Database ported to industry-standard x86 ‘blade’ platform;
2004 – Third-generation software-only product released using industry-standard x86 servers, and, in particular, x86 blade servers;
2005 – Merger with Kognitio and database renamed to WX2;
2007 - DaaS (Data Warehousing as a Service) launched based on WX2;
2008 - Kognitio USA operation established.
FAQ: Does WX2 only support servers from particular manufacturers?
Solution 00000030 by Stuart Watt at 2008-06-25T12:55:38.000+0000
WX2 has been tested on servers from HP, IBM, Dell and Sun. However, any standard x86 server platform will work.
FAQ: We have racks of Intel x86 servers, but they aren’t blades. Can we run the WX2 system on these servers and still realise the claimed performance benefits?
Solution 00000031 by Stuart Watt at 2008-06-25T14:04:41.000+0000
Yes. WX2 will run on any suitably connected array of x86-based servers. These arrays do not necessarily have to be blade servers, although they do provide the most convenient way of building systems with the highest density and bandwidth. For larger systems, rack-mount servers will tend to be more appropriate by allowing the most amount of RAM and disk to be configured into fewer, higher capacity servers, at the expense of slightly reduced parallelism and bandwidth.
FAQ: Can servers of differing specifications be used to create a single WX2 database?
Solution 00000032 by Stuart Watt at 2008-06-25T14:05:56.000+0000
Yes, it is possible to have a WX2 system composed of servers of differing specifications and the system will endeavour to balance the workload relative to the performance characteristics of each server. This offers great flexibility for incremental growth over time without introducing unnecessary redundancy due to technological progression.


For optimal throughput, though, it is important that the performance of all servers is either similar or scaled in terms of RAM and CPU power, i.e. if a server has twice the CPU power of another, it ideally needs twice the RAM as well.
FAQ: Can hardware from more than one supplier be used to create a single WX2 database?
Solution 00000033 by Stuart Watt at 2008-01-29T14:27:34.000+0000
Yes, it is possible to create a WX2 system using hardware from multiple suppliers. Amongst other things, this allows customers to obtain platform upgrades from a number of competing suppliers as a way of securing the best possible pricing. It also gives customers more flexibility for long-term installations where vendor preferences may change over time.
FAQ: Is the number of servers assigned to a WX2 database fixed, or can it be varied in response to fluctuating demand?
Solution 00000034 by Stuart Watt at 2008-06-25T14:08:36.000+0000
WX2 supports capacity and time-based licencing, allowing a baseline platform to be expanded with additional nodes on-demand during times of special need or peak workload.

The architecture of WX2 on standard servers gives organisations an extremely flexible operating model, whereby the resources available to the database system are not set in stone and can be altered in response to changing circumstances.

Additional capacity can be added at any time, incrementally, starting with a single server. Linear scalability ensures that each new server materially adds to the overall power of the WX2 system.

Servers can be added or removed from a WX2 system after a software restart. If these servers are part of a blade ‘farm’ connected to a common network infrastructure, it becomes possible to move them ‘virtually’ from one WX2 system to another without physical unplugging. The VLAN capabilities of modern switching systems can be used to create secure, isolated subsets of servers, which can then be automatically configured as separate WX2 systems. Servers can be moved virtually between VLANs in response to fluctuating demand and the recipient WX2 system reconfigured to use these new servers. When the peak has past, the servers can be re-deployed back to the original WX2 system.
FAQ: Is a WX2 node in any way ‘special’, or can it be re-deployed for any purpose?
Solution 00000035 by Stuart Watt at 2016-05-23T08:52:44.000+0000
Being based on industry-standard x86 servers, the nodes in a WX2 system can be re-used for any purpose, not just in another WX2 environment. They can be configured to run standard operating systems and applications on Windows or Linux, then optionally re-configured and added back to WX2 as required at any time in the future.


Other Linux based applications may be run on the database nodes while WX2 is running, although this is not recommended due to the potential for resource contention. If the database is stopped, the nodes can be immediately and fully used for other applications, for example, to provide computing resource to other enterprise applications during peak workloads. This is part of the flexibility of a virtual appliance and is one of the unique strengths of WX2.
FAQ: Our corporate standards dictate that a shared SAN infrastructure must be used for all significant data storage. Will the WX2 system work in a shared SAN environment?
Solution 00000036 by Stuart Watt at 2008-06-25T14:38:01.000+0000
SAN-based storage will, in most circumstances, be slower than directly attached storage, because of the large number of server nodes performing significant amounts of simultaneous I/O to a shared disk infrastructure. An optimal WX2 installation will therefore prefer the use of local disk storage.


However, for users who want high-performance RAM-based data analysis, WX2 is uniquely placed to exploit SAN-based storage, as once the memory images have been created, the lower performance of the SAN-based disks will not affect subsequent query performance.
FAQ: How easy is it to add one or more new servers to an existing WX2 system?
Solution 00000038 by Stuart Watt at 2016-06-29T11:14:19.000+0000
The normal procedure for adding servers to a WX2 system is as follows (note that if you get as far as step (7) in the steps below, but do not reach step (8) this MAY prevent parallel import and export operations working, as the new nodes will have smds running but no WXDB processes, and so connections to localhost on those nodes will FAIL):

1. Make a copy of the old configuration file in a safe place (copy from /opt/kognitio/wx2/etc/config to another location), and check the current pointer by doing 'ls -ltr /opt/kognitio/wx2'.

2. Install the new server hardware into the existing environment

3. Using the server vendor’s deployment tool, image all new servers with an identical image of an existing server. If in a cloud environment where the IP addresses of all nodes need to be specified in the config file, ensure this is done for the config file on all nodes.

4. Start the new servers.

5. Use "wxprobe -HNF" to confirm that the new nodes are identical to the existing ones. Also, ensure that all drivers and firmware are identical (e.g. use 'ethtool -i eth2' to check drivers/firmware for eth2, and similarly for other interfaces).

6. Double-check that the current pointer on every node is as expected - i.e. the same as at step 1. You can verify this with "wxtool -S 'ls -ltr /opt/kognitio/wx2'".

7. Use wxviconf to ensure the config file is as in step 1 (subject to any modifications for a cloud environment as mentioned in step 3). The new smds will attempt to synchronise items such as the current pointer and config file to the latest version (determined by timestamp), so we want to ensure that nothing has gone awry here.

8. Instruct WX2 to re-start. WX2 will auto-identify all new nodes and configure them into the system. Extra processor and memory resource will be immediately available, disk resource will be visible but not used

9. Consider running a shape test on the new system to ensure no problems have been introduced by adding the node(s) (this has been seen before on e.g. ica01)

10. A reconfigure process can then be started at a convenient time to make the new disk capacity available to the system and to re-distribute the existing data. Note that reconfigure should never be run via SQL - always use the wxadmin front-end, choosing options 'server' followed by 'reconfigure'

Note that the reconfigure 'quickadd' option should always be used now, rather than the deprecated 'fulladd' option. Quickadd has the following advantages:
+ the deskew can easily be started by setting redist_threshold to a suitable value (e.g. 20), and stopped by getting a global lock and setting redist_threshold back to its default value of 101
+ the deskew process does not interleave data from different tables on disk - this was an issue with fulladd which could lead to significantly worse disk scanning performance subsequently.
+ fulladd causes history to be lost which means that no incremental backups can then be run until another full backup has been taken.

Note that quickadd will only move copies of some data from old to new disks, it will not remove them from the old disks. This will speed up disk scans as the copied data will never be rescanned on the old disks when querying tables, but to get the disk space back on the old disks a repack or reclaim operation will be required.
FAQ: I like the idea of the WX2 system, but I don’t know much about Linux or blades. Do you offer pre-configured hardware/software bundles that don’t require detailed Linux or blade knowledge?
Solution 00000039 by Stuart Watt at 2016-05-20T10:54:44.000+0000
Kognitio optionally offer a complete ‘virtual data appliance’ service, whereby agreed hardware and software packages are built, configured and tested by Kognitio on behalf of customers for shipping as a ‘plug-and-go’ unit anywhere in the world.
FAQ: We already have enough problems administering a handful of database servers. How can we possibly administer a WX2 system potentially composed of dozens or hundreds of servers?
Solution 00000040 by Stuart Watt at 2008-01-29T14:39:17.000+0000
One of the key strengths of the blade server platforms from the major manufacturers is the deployment software supplied with them that allows identical OS and application configurations to be installed on dozens or hundreds of blades simultaneously with one command. This allows a group of discrete servers to be essentially treated as a single entity, significantly reducing administrative overhead. Each node is consistently configured (a ‘clone’) and the nodes are managed consistently as a whole.


The WX2 database system was itself designed for MPP operation from day one and includes many options for performing operations on all nodes using just one command, for example, software is only installed on one node and then automatically distributed to all other nodes. Database processes are automatically placed and replicated on all available nodes for optimal configuration.
FAQ: Can data be loaded directly into memory?
Solution 00000043 by Stuart Watt at 2015-11-04T17:22:42.000+0000
WX2 supports the concept of RAM only tables. These tables exist only in memory, and as such have faster loading times, and faster data manipulation. For more info, please see the "RAM ONLY tables" section in the official Kognitio Guide.

FAQ: How many network interface ports should each WX2 server have?
Solution 00000044 by Stuart Watt at 2008-06-26T11:53:39.000+0000
Each x86 server in a WX2 system should have at least two network interface ports, one for internal communication with the other WX2 nodes and the other for communication with the outside world. Clients wishing to connect to the WX2 server will use the ports assigned to the external network.


If the servers have more than two network interface ports available, WX2 is able to use these additional ports to enhance inter-node network bandwidth.
FAQ: Does WX2 require a specialised interconnect to allow all the nodes to communicate with each other at high speed?
Solution 00000045 by Stuart Watt at 2008-06-26T11:55:29.000+0000
The WX2 system uses an industry-standard Ethernet network, normally switched Gigabit or 10Gb Ethernet, to connect all the nodes together. No other specialist interconnects are required.


However, WX2 can run over interconnects such as InfiniBand where these connections have been configured to emulate a normal Ethernet interface.
FAQ: Can a WX2 node be run without any disk storage?
Solution 00000046 by Stuart Watt at 2016-05-20T10:55:17.000+0000
All WX2 nodes require enough disk space to hold and run their own copy of Linux, including swap space, log space, etc. However, not all WX2 nodes need to contribute disk space towards the total disk storage capacity of the database, instead operating as a ‘compute node’ offering only CPU and RAM resource. This is particularly useful when a new node is added to an existing WX2 system, as it allows that node to be immediately useful without the need to redistribute all the disk-based data.


See also "FAQ: Our corporate standards dictate that a shared SAN infrastructure must be used for all significant data storage. Will the WX2 system work in a shared SAN environment?" for SAN storage considerations.
FAQ: Which Linux distributions does WX2 run on?
Solution 00000047 by Stuart Watt at 2008-01-30T16:39:46.000+0000
The WX2 system is currently supported on Red Hat Enterprise Linux and Novell SUSE Linux Enterprise Server. Other Linux distributions may work, but are not officially supported by Kognitio.


The currently supported versions are:

1. Red Hat - Enterprise Linux 4 and 5;

2. Novell - SUSE Linux Enterprise Server 9 and 10.


The following distributions should also be able to run WX2, although they are not officially supported:


  - Red Hat Fedora;

  - openSUSE;

  - Ubuntu;

  - CentOS;

  - Debian.
FAQ: Can I use the special Red Hat Enterprise Linux For HPC Compute Nodes subscription to run WX2?
Solution 00000051 by Stuart Watt at 2008-01-29T16:02:02.000+0000
The special Red Hat Enterprise Linux For HPC Compute Nodes subscription can in technical terms be used to run WX2. However, as there is no concept of a communications ‘head node’ in WX2, with each node instead being an equal participant in communication with the outside world, it is not clear that the specific licencing terms of the licence would be satisfied in these circumstances.


In addition, the Red Hat HPC FAQ states the following:


"Many IT infrastructure services are not considered to be HPC workloads. For example, the following would not be HPC workloads: database, web application, file/print, backup, firewalls, DNS, Samba, Squid, DHCP and directory servers. High availability clusters or load balancer machines are also not considered HPC clusters."


This statement would appear to explicitly disallow the use of the HPC subscription for database systems such as WX2.
FAQ: Do any of the Linux vendors offer special licencing deals for large numbers of blade servers?
Solution 00000052 by Stuart Watt at 2008-01-29T16:09:37.000+0000
Red Hat and SUSE offer bundled licence deals for certain blade system configurations:


1. Red Hat – Bundle for HP BladeSystem, consisting of 8 Enterprise Linux subscriptions and supporting software, covering one HP blade enclosure;

2. SUSE – Bundle for IBM eServer BladeCenter, consisting of 14 Linux Enterprise Server subscriptions, covering one IBM blade chassis, or bundle for HP BladeSystem, consisting of 8 Enterprise Server subscriptions, covering one HP blade enclosure


Further details can be found on the respective vendor’s web site.
FAQ: Does WX2 comply with the LSB (Linux Standard Base) specification?
Solution 00000053 by Stuart Watt at 2008-01-30T09:16:17.000+0000
WX2 uses all LSB function calls in the proper way, but also makes use of functions that are not part of LSB to deliver the most fully featured product. Given the relative ubiquity of commercial Linux distributions such as Red Hat and SUSE with which WX2 is fully compatible, Kognitio does not anticipate re-engineering WX2 to be fully LSB compliant.
FAQ: Is WX2 a 32-bit or 64-bit application?
Solution 00000054 by Stuart Watt at 2012-03-13T13:18:04.000+0000
WX2 is a 32-bit application but runs on both 32-bit and 64-bit x86 versions of Linux.


When installing on a 64-bit version of Linux ensure the 32-bit runtime is installed, as WX2 has dependencies on 32-bit libraries such as libgcc and libz.

As servers with 64-bit enabled processors and large RAM configurations become more widely available, along with suitable versions of Linux to exploit them, WX2 will progressively be updated to exploit the enhanced capabilities provided.
FAQ: What types of disk storage does WX2 support?
Solution 00000055 by Stuart Watt at 2008-01-29T16:18:19.000+0000
WX2 can be configured to use the following disk storage methods:


1. file – A normal file on a Linux filesystem;

2. sparsefile – This is similar to a ‘file’, but data is not written to the unused locations within the file;

3. partition – A disk partition;

4. raw – An entire disk as a raw device.


Kognitio normally recommend the ‘partition’ option for production use, as it is the most efficient, having no file system overhead. Sparse files are quicker to set up than normal files, as the entire data area does not need to be initialised beforehand. However, sparse files are not recommended for production use, as there is always the possibility that the disk does not have sufficient room to completely represent the file, something that would only be discovered when an attempt was made to write the data.
FAQ: What type of Linux partition does WX2 use?
Solution 00000056 by Stuart Watt at 2008-01-29T16:24:27.000+0000
WX2 will look for disk partitions where the partition type has been set to 60 Hex (0x60). Any partition that it finds of this type will be automatically used by WX2 for data storage.


Type 0x60 partitions can cause problems during blade cloning. To avoid this, the type 0x60 partitions can be created as standard Linux partitions and mounted on dummy mount points (/dummy1, /dummy2, etc). This can make cloning/installation simpler and during the installation of WX2, they can be converted to type 0x60 partitions using partition tools such as fdisk.
FAQ: Along with many other organisations, we have standardised on Microsoft Windows and do not plan to use Linux. Why do you not offer a Windows version of WX2?
Solution 00000058 by Stuart Watt at 2015-11-04T17:05:05.000+0000
The WX2 database system shares many similarities with other HPC (High Performance Computing) environments in the industry, which almost always run a variant of Linux due to its performance, flexibility and configurability in these demanding environments. Microsoft Windows, by contrast, is a very feature-rich but expensive environment that is overkill for what WX2 requires in a large-scale MPP platform.  WX2 therefore runs on Linux, requiring only the basic kernel plus some drivers, thereby allowing customers to construct the most cost-effective MPP database system.
FAQ: The WX2 system seems to create a very large number of processes in Linux when it is running. Is this normal?
Solution 00000059 by Stuart Watt at 2008-01-29T16:40:18.000+0000
A Linux node in a large, busy, WX2 system can have over 3,000 listed 'processes', although these are actually threads. This is normal for WX2 systems.
FAQ: The Linux load average seems very high when WX2 is running. Is this a cause for concern?
Solution 00000060 by Stuart Watt at 2008-01-29T16:41:45.000+0000
With WX2, the aim is to make CPUs 100% busy all the time if possible and if there are a number of queries waiting their turn to run on the CPUs, then the load averages for the blades will go up. Linux load averages of 30+ on a node are normal for WX2.
FAQ: Does WX2 require a Linux cluster file system to operate?
Solution 00000061 by Stuart Watt at 2008-01-29T16:43:44.000+0000
WX2 runs on independent instances of standard Linux on each node and does not require a shared clustered file system to operate.
FAQ: Does WX2 use the industry-standard MPI (Message Passing Interface) for communication amongst nodes?
Solution 00000062 by Stuart Watt at 2010-07-13T13:18:38.000+0000
The WX2 system incorporates its own, extremely efficient, UDP-based MPK (Message Passing Kernel) interface to maximise performance across industry-standard x86 server nodes and does not therefore require MPI.
FAQ: Do all nodes in a WX2 system run an identical set of WX2 processes?
Solution 00000063 by Stuart Watt at 2008-01-29T16:51:02.000+0000
The nodes in a WX2 system do not run an identical set of processes, instead being configured to run various subsets, giving nodes certain combinations of capabilities. The primary capabilities are:

1. System Management Daemon (SMD) – All day-to-day WX2 administrative tasks are performed through the SMD;
2. RAMStore (RS) – The RAM Store manipulates data in RAM;
3. DiskStore (DS) – The Disk Store manipulates data on disk;
4. Compiler (Comp) – The Compiler compiles a SQL query for subsequent execution;
5. Interpreter (Int) – The Interpreter runs previously compiled SQL queries;
6. Input/Output (IO) – The IO process handles communication with the user;
7. Miscellaneous (Misc) – The Misc process performs a variety of other database tasks.

Some of these capabilities must exist on all nodes to have a functioning WX2 system, for example, the SMD process, with some of the others being configurable to support particular environments, for example, the number of compilers. Each node may also have multiple occurrences of a particular capability, for example, the total RAM on a node may be split into a number of smaller RAM Stores for efficiency.
FAQ: Which versions of ANSI SQL does WX2 support?
Solution 00000065 by Stuart Watt at 2011-08-02T14:25:51.000+0000
WX2 is fully compliant with the ANSI SQL-89 standard, with full certification by NIST to FIPS 127-1. WX2 also incorporates functionality from the ANSI SQL-92, SQL:1999, SQL:2003, SQL:2006, SQL:2008 and draft SQL:2012 standards.

With respect to the most recently published ANSI SQL:2008 standard, WX2 fully supports the Core features of SQL:2008 (with certain exceptions) and a large number of the Optional features of SQL:2008. Some of the optional features of draft SQL:2012 are also supported:

* SQL:2008 Core Features
E011, Numeric data types - This feature is fully supported by WX2.
E021, Character data types - This feature is fully supported by WX2.
E031, Identifiers - WX2 supports this feature, save for: a non-quoted identifier may not be equivalent to a non-standard WX2 reserved word.
E051, Basic query specification - This feature is fully supported by WX2.
E061, Basic predicates and search conditions - This feature is fully supported by WX2.
E071, Basic query expressions - This feature is fully supported by WX2.
E081, Basic privileges - WX2 fully supports all subfeatures of this feature, except E081-09, USAGE privileges, and E081-10, EXECUTE privileges.
E091, Set functions - This feature is fully supported by WX2.
E101, Basic data manipulation - This feature is fully supported by WX2.
E111, Single row SELECT statement - This feature is fully supported by WX2.
E121, Basic cursor support - WX2 fully supports the following subfeatures: E121-01, DECLARE CURSOR, E121-02, ORDER BY columns need not be in SELECT list, E121-03, Value expressions in ORDER BY clause, E121-04, OPEN statement, E121-08, CLOSE statement, E121-10, FETCH statement, implicit NEXT
E131, Null value support - This feature is fully supported by WX2.
E141, Basic integrity constraints - This feature is fully supported by WX2.
E151, Transaction support - This feature is fully supported by WX2.
E152, Basic SET TRANSACTION statement - This feature is not supported by WX2.
E153, Updatable queries with subqueries - This feature is supported by WX2 in certain cases.
E161, SQL comments using leading double minus - This feature is fully supported by WX2.
E171, SQLSTATE support - This feature is not supported by WX2.
E182, Module language - WX2 supports this feature for C only.
F031, Basic schema manipulation - This feature is fully supported by WX2.
F041, Basic joined table - This feature is fully supported by WX2.
F051, Basic date and time - This feature is fully supported by WX2, save for: F051-07, LOCALTIME, F051-08, LOCALTIMESTAMP
F081, UNION and EXCEPT in views - This feature is fully supported by WX2.
F131, Grouped operations - This feature is fully supported by WX2.
F181, Multiple module support - This feature is not supported by WX2.
F201, CAST function - This feature is fully supported by WX2.
F221, Explicit defaults - This feature is fully supported by WX2.
F261, CASE expressions - This feature is fully supported by WX2.
F311, Schema definition statement - This feature is fully supported by WX2.
F471, Scalar subquery values - This feature is fully supported by WX2.
F481, Expanded null predicate - This feature is fully supported by WX2.
F501, Feature and conformance views - WX2 does not support this feature.
F812, Basic flagging - WX2 does not support this feature.
S011, Distinct types - WX2 does not support this feature.
T321, Basic SQL-invoked routines - WX2 does not support this feature.
T631, IN predicate with one list element - This feature is fully supported by WX2.

* SQL:2008 Optional Features Supported
B021    Direct SQL
F032    CASCADE drop behavior
F033    ALTER TABLE statement: DROP COLUMN clause
F034-02 REVOKE statement: GRANT OPTION FOR clause
F034-03 REVOKE statement to revoke a privilege that the grantee has WITH GRANT OPTION
F052    Intervals and datetime arithmetic
F053    OVERLAPS predicate
F171    Multiple schemas per user
F191    Referential delete actions
F200    TRUNCATE TABLE statement
F222    INSERT statement: DEFAULT VALUES clause
F262    Extended CASE expression
F263    Comma-separated predicates in simple CASE expression
F271    Compound character literals (v7.2)
F291    UNIQUE predicate
F301    CORRESPONDING in query expressions
F302    INTERSECT table operator
F302-01 INTERSECT DISTINCT table operator
F302-02 INTERSECT ALL table operator
F304    EXCEPT ALL table operator
F312    MERGE statement
F313    Enhanced MERGE statement
F381    Extended schema manipulation
F381-01 ALTER TABLE statement: ALTER COLUMN clause
F382    Alter column data type
F391    Long identifiers
F392    Unicode escapes in identifiers
F393    Unicode escapes in literals
F401    Extended joined table
F401-01 NATURAL JOIN
F401-02 FULL OUTER JOIN
F401-04 CROSS JOIN
F421    National character (v7.2)
F451    Character set definition
F461    Named character sets
F502    Enhanced documentation tables
F502-01 SQL_SIZING_PROFILES view
F502-02 SQL_IMPLEMENTATION_INFO view
F502-03 SQL_PACKAGES view
F531    Temporary tables
F561    Full value expressions
F571    Truth value tests
F591    Derived tables
F641    Row and table constructors
F661    Simple tables
F731    INSERT column privileges
F763    CURRENT_SCHEMA
F771    Connection management
F781    Self-referencing operations
F801    Full set function
F857    Top-level &lt;fetch first clause&gt; in &lt;query expression&gt; (v7.2)
F858    &lt;fetch first clause&gt; in subqueries (v7.2)
T021    BINARY and VARBINARY data types
T023    Compound binary literals (v7.2)
T024    Spaces in binary literals (v7.2)
T053    Explicit aliases for all-fields reference (v7.2)
T061    UCS support
T071    BIGINT data type
T101    Enhanced nullability determination
T121    WITH (excluding RECURSIVE) in query expression
T122    WITH (excluding RECURSIVE) in subquery
T141    SIMILAR predicate
T151    DISTINCT predicate
T152    DISTINCT predicate with negation
T171    LIKE clause in table definition (v7.2)
T172    AS subquery clause in table definition (v7.2)
T191    Referential action RESTRICT
T285    Enhanced derived column names
T312    OVERLAY function
T351    Bracketed comments
T431    Extended grouping capabilities
T432    Nested and concatenated GROUPING SETS
T433    Multiargument GROUPING function
T434    GROUP BY DISTINCT
T441    ABS and MOD functions
T461    Symmetric BETWEEN predicate
T501    Enhanced EXISTS predicate
T551    Optional key words for default syntax
T611    Elementary OLAP operations
T612    Advanced OLAP operations
T613    Sampling (v7.2)
T621    Enhanced numeric functions
T641    Multiple column assignment

* Draft SQL:2012 Optional Features Supported
T614    NTILE function
T615    LEAD and LAG functions
T617    FIRST_VALUE and LAST_VALUE functions (v7.2)

FAQ: Which version of ODBC does WX2 support?
Solution 00000066 by Stuart Watt at 2008-01-30T09:45:54.000+0000
A v3.51 WX2 ODBC driver is currently available on Linux, Solaris and Windows. It supports all the functions for core level compliance, along with some level 1 and 2 functions:


* Level 1 functions supported:

   - SQLMoreResults

   - SQLPrimaryKeys


* Level 2 functions supported:

   - SQLColumnPrivileges

   - SQLDescribeParam

   - SQLTablePrivileges
FAQ: How does WX2 cope with Oracle SQL statements?
Solution 00000068 by Stuart Watt at 2008-01-30T09:50:21.000+0000
WX2 offers a high level of Oracle syntactic compatibility that allows it to successfully execute SQL statements containing Oracle-specific functions and syntax such as Oracle (+) outer join constructs.
FAQ: What is the difference between ‘create table…as select’ and ‘create table…for select’?
Solution 00000070 by Stuart Watt at 2008-01-30T09:53:21.000+0000
The only difference between ‘create table…as select’ and ‘create table…for select’ is that the latter merely creates the target table structure without inserting any of the rows represented by the ‘select’ portion of the statement.
FAQ: What is the WX2 equivalent to Oracle’s SQL*Plus command line utility?
Solution 00000071 by Stuart Watt at 2008-01-30T09:54:52.000+0000
The WX2 equivalent to Oracle’s SQL*Plus utility is ‘wxsubmit’.
FAQ: If WX2 does not have indices, how can it support constraint checks such as ‘primary key’ and ‘unique’?
Solution 00000077 by Stuart Watt at 2008-01-30T10:56:24.000+0000
WX2 can perform constraint checking on tables which are memory-resident, using its considerable in-memory scan rates to allow the checking to be performed without the need for traditional indices.
FAQ: What are the WX2 architectural limits?
Solution 00000078 by Stuart Watt at 2016-06-14T11:17:47.000+0000
The WX2 architectural limits are:

  - Maximum number of columns in a table – 1,000;

  - Maximum number of tables/views in a schema – Unrestricted;

  - Maximum column length –32,000 bytes;

  - Maximum row length – 32,764 bytes;

  - Maximum length of table/view/column/schema identifier – 128 characters;

  - Maximum system ID length – 11 characters; 
FAQ: What features of WX2 minimise administrative overhead?
Solution 00000079 by Stuart Watt at 2008-01-30T11:01:15.000+0000
WX2 was designed as an analytical database and comes out of the box highly tuned for that role. It is designed to run on an MPP platform employing the enormous raw processing power of arrays of low-cost, industry-standard servers to literally crunch through huge volumes of data incredibly quickly. This ability to look at all the data in very short periods of time eliminates the need for any sort of tuning, making the database very simple to install and manage:

  - Indices do not have to be designed and built;
  - The data does not need to be partitioned;
  - Queries do not need hints added to ensure that the database knows how to run them.

WX2 allows data to be loaded very quickly with no pre-planning. Once loaded, the data is immediately available for querying. New data can be added and is immediately available for use; indices do not need to be rebuilt, nor the system re-tuned. Overall, the DBA requirements of a WX2 installation will be a small fraction of that required for a generic database system. WX2 initial installation is very simple, regardless of system size. The quick install guide amounts to two sides of A4. WX2 automatically configures itself to run optimally on any size of system, seamlessly identifying and configuring all processing resources made available to it.
FAQ: How do I archive and restore a WX2 system?
Solution 00000080 by Stuart Watt at 2008-01-30T11:11:01.000+0000
The WX2 database has in-built mechanisms for loading and unloading data very quickly. These mechanisms are easily called from scripts to set up a production archive process.


Additionally, backup and restore tools that allow the user to easily set up an archive process and restore data when necessary are available.
FAQ: I understand that WX2 minimises administrative overhead, but what tasks still need to be undertaken?
Solution 00000082 by Stuart Watt at 2008-01-30T11:15:49.000+0000
Typical WX2 administrative activities might include:

  - Create user accounts and schemas and associated user access rights (minutes of effort using admin tools or SQL);
  - Check error log and hardware status (typically managed by auto scheduled scripts sending out email reports);
  - Check disk space. If disk is filling to capacity, then schedule a reclaim to run. Reclaim completion time depends on many factors such as the initial fill, the data to be reclaimed and the distribution of reclaimed data within the data space. A typical run time for offline reclaim would be 2-4 hours;
  - Check tables/views in RAM, their distribution, and overall RAM utilisation. Review with users and modify as required (recreating table/view images depends on their size and complexity - typically minutes to tens of minutes). Users can self-manage this if so authorised;
  - Review query access to very large disk-based tables.

All of the above can be achieved with less than one hour of DBA time per day.
FAQ: How would I typically go about tuning a WX2 system?
Solution 00000083 by Stuart Watt at 2008-01-30T11:18:45.000+0000
The primary mechanism for improving the speed of access to data is to place the data into memory by creating a table or view image. Data can be placed into RAM using one of three distribution methods:

  - Random – equal volume on each node, which is the default;
  - Hashed on a value – fully, or partially where value skewing is causing significant imbalances;
  - Replicated across all nodes – ideal for lookup or dimension tables used in joins to a fact table.

Arranging the data in different ways provides for more ‘processor local’ join strategies, with the minimum of data redistribution between the nodes. The database optimiser is smart enough to be able to use any of the distribution methods for intermediate query steps based on gathered statistics and redistribute as necessary.

Access to data on disk can only be tuned by adding additional nodes, i.e. less data per node and thus a faster scan rate, or by adding compressed bit mapped indices to suitably selective attributes (note that these are a memory based object that use a reserved space within a database process and thus do not utilise any user disk or RAM). Compressed indices do not provide the level of performance for single row lookup provided by traditional indices, but do significantly reduce I/O when selecting specific subsets of dates, stores, products, etc. from a large table.

Choosing between disk and RAM-based objects is a function of the size of the data, the available RAM and the expected repeatability or iterations of queries against a given subset of data. If a table holds twelve months of data and the next phase of work will focus on the most recent month, then placing that month’s worth of data in RAM (utilising a CI on month) will dramatically speed up the subsequent analysis. If a complex view is to be repeatedly used, then instantiating it into RAM will dramatically speed up subsequent analysis.

Objects placed into RAM can be dropped at any time in seconds to free-off space for other uses. The goal is to meet the current context of data analysis needs by placing objects into RAM and then when finished, changing this to meet the next context of analysis need. This is a dynamic approach that requires a little forethought, but is fully able to support the wide variation of query needs in an analytical database with minimal overhead.
FAQ: What is a reclaim and how long does it take?
Solution 00000084 by Stuart Watt at 2016-05-23T10:48:51.000+0000
WX2 uses a linear file system for storing data. This means that any time a row is updated or deleted, that row is marked as invalid and a new row is written to the end of the file. Row updates are never performed in-situ. This has the advantage of maximising performance for large data volumes and providing consistent read-only database scans, but it does mean that ‘dead’ space gradually builds up in the database that occasionally needs to be released for re-use. The process of releasing this dead space is called a ‘reclaim’. Reclaims require that all users are locked out of the system for the duration of the reclaim, to ensure disk consistency. This could be a number of hours for a large database.
FAQ: What WX2 system events can be monitored?
Solution 00000092 by Stuart Watt at 2008-01-30T11:48:36.000+0000
Internal monitoring and alerting within WX2 is the responsibility of the SMD (System Manager Daemon) process, which is also responsible for other system maintenance activities including:

  - Starting and stopping the WX2 database;
  - Monitoring the system for problems;
  - Installing or uninstalling WX2 software packages;
  - Software upgrades;
  - Synchronizing installed software versions and configuration details;
  - Expiring old log files and core dumps to prevent disks from filling up;
  - Alerting the administrator when problems occur;
  - Performing debug dumps if WX2 crashes.

The SMD monitor process maintains a log file in its logging directory and when a problem is detected, information is written out to this log file that describes the problem. The monitor can also raise alerts and initiate user-defined actions when certain events occur. The events that can be handled in this way include:

  - Informational event;
  - Security-related event;
  - Warnings;
  - Errors;
  - Fatal errors;
  - Automated recovery failures;
  - Daemon events, including starting, stopping, restarting and spontaneous exiting;
  - Server events, including server start and stop, server crash, server reset and server initialisation;
  - Connection or permission events, including user connection or disconnection, refused connections and required permissions available or denied events.

When one of these events occurs, as well as an entry being written to the log file, a Linux program or script can be run to take any desired action. It could, for example, e-mail a help desk when any fatal errors occur, or automatically restart the server in the event of a crash.
FAQ: How does WX2 interface with Linux security?
Solution 00000093 by Stuart Watt at 2016-05-23T08:46:07.000+0000
The WX2 database system runs on top of standard installations of Linux, normally Red Hat or SUSE, and does not alter the kernel or the normal security model of the operating system it runs on. It does, however, require root-level privileges in order to function.


The owner of the WX2 system files is normally the Linux ‘root’ user. Two additional WX2 OS administrative identities are created as follows:

  - wxroot: The identity normally used for WX2 ‘destructive’ operations, for example, performing a ‘new system’ initialisation, or for altering rights and permissions;

  - wxadmin: The identity used for all other day-to-day operations and administration of WX2 at the operating system level.


These are the administrative identities used to control and administer WX2 at the operating system level. These identities are distinct from the database user identities that are set up and maintained by the database administrator, who has the ‘SYS’ database identity.
FAQ: What database security features are present in WX2?
Solution 00000094 by Stuart Watt at 2008-01-30T11:57:52.000+0000
Within WX2, the database administrator has access to a comprehensive range of access control SQL statements:
 - create/alter/drop user;
  - create/alter/drop group;
  - create/drop/set default schema;
  - grant/revoke.

When creating users, a database administrator can set up a ‘security class’ for each user that specifies:
  - How often a password must be changed;
  - The length and style of password;
  - How many passwords must be used before the same one can be re-used;
  - How many consecutive login failures can occur before a user is suspended;
  - What period must elapse before an inactive user ID is suspended;
  - What period must elapse before an inactive session is forcibly terminated.

Several different classes of user may be set up, depending on the requirements and the sensitivity of the data being held on the WX2 system.

The database administrator can also restrict the clients or hosts that a particular user or class of users can connect from by, for example, specifying a particular network protocol, interface or address to be allowed or denied.

Each username has an associated password that is encrypted and stored in a system database table that only the SYS database administrator user has access to.

WX2 also supports SSL on client connections and the management of SSH keys.
FAQ: What is the WX2 'embedded client'?
Solution 00000095 by Stuart Watt at 2008-01-30T11:59:55.000+0000
The ‘embedded client’ is a mechanism used in the WX2 server to perform certain tasks, typically when it would be easier to do something by running SQL rather than actually writing some code to do it. For example, as part of a ‘drop table’ statement, WX2 runs a query along the lines of ‘delete from ipe_alltable where id = xxxx’, which will delete the rows from ‘ipe_alltable’ and use referential integrity to do the deletes from all the tables that reference it. The alternative would have been to reproduce all the referential integrity code for drop table so that WX2 explicitly did all the deletes from each table individually, a much less efficient approach.
FAQ: What is connection 'bouncing’ and connection forwarding?
Solution 00000097 by Stuart Watt at 2012-02-20T13:58:23.000+0000
The WX2 server can ‘bounce connections’ to do I/O load balancing with the outside world. Without this, you would be limited to a maximum of 50 connections if no other parameters have been set and you only have one IP address specified in the ODBC entry.

To enable connection bouncing, edit the global WX2 configuration file to add:

[system]
external_net=eth?

where ‘eth?’ is the interface that is being used by the clients to connect to the WX2 server. If for any reason only a subset of the nodes offer external connectivity, then only the local configuration files of those nodes should have the 'external_net' parameter added.

The idea of connection bouncing is that if you are the only client on the system, the first connection on the server IP address specified in the ODBC entry will work as normal. When the second one connects, the server will send a reply to the ODBC driver asking it to reconnect to a different interface, which it should then do. This will keep happening for new connections until every node has one connection going through it, then the next connection will not be bounced but the N-1 after that will be, assuming N nodes.

Connection bouncing is augmented by connection forwarding. Connection forwarding was introduced in WX2 to allow an I/O node to forward connections to AMs on other nodes if necessary. Previously, the I/O node would only use its own AMs, and fail if it didn't have any free, the maximum number being 58 as controlled by the 'sessions_per_cp' parameter.

An I/O node can handle 80 connections, up to 58 using its own AMs and the remainder forwarded to other I/O nodes. Although the parameter "iomc" (I/O node Max Connections) in the "runtime parameters" section of the configuration file can be made larger than this (it currently defaults to 250), a hard limit in the current server versions means that in practice the 80 connections figure cannot be exceeded.
FAQ: Why does the WX2 ODBC driver have space for multiple server IP addresses?
Solution 00000098 by Stuart Watt at 2010-01-19T15:00:57.000+0000
The WX2 ODBC driver allows more than one node to be referenced when connecting, which helps application availability should one of the referenced nodes fail and be taken out of the WX2 database configuration.

Drivers issued from 2010 onwards support more control over which addresses should be used - setting FirstServerAddress to an integer value in odbc.ini results in that entry being used (e.g. setting to 2 ensures the second address is used initially rather than the first).

If not set, FirstServerAddress defaults to 1.

Setting to 0, or setting RandomServerAddress=Y, makes the ODBC driver select an initial address at random from the list.
FAQ: Does WX2 keep a record of all SQL statements submitted for processing?
Solution 00000099 by Stuart Watt at 2008-01-30T13:22:53.000+0000
The WX2 system table ‘ipe_command’ records all submitted SQL statements and is persistent across database restarts. It contains one row per statement.
FAQ: What does the ‘status’ column on ‘ipe_command’ signify?
Solution 00000100 by Stuart Watt at 2016-05-23T10:39:44.000+0000
- status=0 – Statement executed successfully;

  - status&lt;&gt;0 – Statement did not execute successfully, for example, there was a syntax error.

More generally, use the SQL 'explain &lt;table or view&gt;' to see a description of what each column in a table means. 
FAQ: What does the ‘operation’ column on ‘ipe_transaction’ signify?
Solution 00000102 by Stuart Watt at 2016-05-23T10:27:47.000+0000
- operation=2 – Transaction started;

  - operation=3 – Transaction finished and committed (i.e. completed successfully);

  - operation=4 – Transaction finished and rolled back(i.e. transaction cancelled, with any data changes reverted)

More generally, use the SQL 'explain &lt;table or view&gt;' to see a description of what each column in a table means. 
FAQ: What does the ‘command_running’ column on ‘ipe_cursessions’ signify?
Solution 00000103 by Stuart Watt at 2016-06-01T09:37:02.000+0000
It signifies the status of the most recent SQL run in that session. Statuses are:

0: Session idle (command completed)
1: Command running

2: Statement rollback

3: Transaction rollback

4: Command queued, waiting for a queue slot to open
5: Command being compiled
6: Waiting for interpreter
FAQ: What is the structure of the WX2 version numbering scheme?
Solution 00000104 by Stuart Watt at 2016-05-23T11:09:47.000+0000
Kognitio software versions are usually in one of the following equivalent formats:
ver80100rel150916
8.1.0rel150916

The structure of latter example is:


X.Y.ZrelD,  where:
X = Major release;

Y = Intermediate release;

D = Bug fix or security patch, where D is the date of release
FAQ: Where is the WX2 root installation directory on Linux/Solaris?
Solution 00000105 by Stuart Watt at 2008-01-30T13:38:47.000+0000
The WX2 software is installed on Linux/Solaris under a root directory called ‘/opt/kognitio/wx2’. Multiple versions of the WX2 software can be installed simultaneously under this directory in subdirectories of the form ‘verXYYZZp’ (see "What is the structure of the WX2 version numbering scheme?"), potentially allowing for software downgrades as well as upgrades:


/opt/kognitio/wx2/ver60104g/...

/opt/kognitio/wx2/ver60105/...

/opt/kognitio/wx2/ver60105a/...

/opt/kognitio/wx2/current/…   [Soft link to one of the numbered directories]


A Linux/Solaris soft link is used to point at the ‘current’ software version.
FAQ: Where are the WX2 configuration files stored on Linux/Solaris?
Solution 00000106 by Stuart Watt at 2008-01-30T13:40:02.000+0000
The WX2 configuration files are stored in the ‘/opt/kognitio/wx2/etc’ directory.
FAQ: Where is the WX2 root log directory on Linux?
Solution 00000107 by Stuart Watt at 2016-06-06T13:23:39.000+0000
By default, the WX2 root log directory is ‘/var/log/wx2’, with all log information for a specific WX2 system ID going into the ‘logs-&lt;SystemID&gt;’ subdirectory. For example, for a WX2 system called ‘prodserver’, its log path would be:


/var/log/wx2/logs-prodserver/…


It is possible to specify a different root log directory location by adding the following entry to the WX2 global configuration file:


[paths]

log_dirs=&lt;path to root log directory&gt;


WX2 provides a shortcut mechanism to move quickly to one of the logging subdirectories:


cd $(wxlogd smd)

cd $(wxlogd startup)

etc.
FAQ: What are the outline steps required to create a working WX2 system from an empty set of servers?
Solution 00000109 by Stuart Watt at 2008-06-26T13:21:11.000+0000
1. Install the base Linux/Solaris operating system on each server that is to be part of the WX2 system. Supported Linux/Solaris versions are described elsewhere;
2. Copy the WX2 installation package onto each server;
3. Log onto one of the serversas ‘root’ and run the ‘wxinstaller’ in the self-extracting installation package, supplying values for the system ID, the internal network interface port to be used for communication among the nodes and whether or not the ‘wxroot’ and ‘wxadmin’ accounts should be created;
4. The required directories, users, packages, pointers and files will be created and installed as required. A command line will also be displayed that performs the specified installation without user interaction. This command line can be used to perform the same WX2 install on all the other servers in the system;
5. Depending on the server deployment environment, it may be possible as an alternative to perform steps 1, 2, 3 and 4 on one server, then ‘clone’ this configuration to all other servers automatically;
6. At this end of this process, all the servers are running the WX2 administrative services and are linked together. The next phase of the installation will run on this collection of server to configure them as a parallel RDBMS;
7. Log onto one of the servers as ‘wxroot’ and run the ‘wxadmin’ tool to confirm the available hardware, specify the disk storage to be used for the database and install the WX2 licence key;
8. The new WX2 system will be created and the disk storage initialised. This may take a number of hours for a large system;
9. The new WX2 database system is now available for use.

If you need to download software, or download documentation the relevant material is attached to other solutions - search for "install" and "documentation" respectively to locate them.
FAQ: Is WX2 a ‘non-stop’ database system?
Solution 00000112 by Stuart Watt at 2008-01-30T13:58:09.000+0000
WX2 has high availability features, but is not ‘non-stop’.

The first step of protection is implementing RAID1 within a blade. This gives good protection against one or more disk failures in a blade farm, but would not protect against a whole blade failure. To protect against whole blade failure, WX2 implements a software RAID feature that creates a RAID cluster across a set of blades (effectively RAID5 with a typical cluster size of 4). For extreme protection, it is possible to use both hardware RAID1 in each blade and then put software RAID on top of that.

If one or more disks in different RAID clusters fail, the WX2 database operates continuously without impact. The disk(s) can be replaced and the RAID arrays rebuilt with the database online.

If a blade fails, the database will halt. On restart, any transactions that were open during the event will be rolled back. WX2 can be set to auto-restart (having optionally taken dump information) at which point it will reassess the blade inventory and restart. If hardware RAID1 was being used, the restart will halt, as data storage integrity will be invalidated. If software RAID was used, the failed blade will be ignored and the database reinitialised (with a slightly smaller configuration) and started. The system can cope with one or more blade failures as long they impact different RAID clusters.

Replacement of a failed blade would require that the blade be replaced/fixed. The database would need to be restarted to reintroduce this blade into the configuration. After restart, the software RAID cluster will be rebuilt with the database online.

The blades themselves have many high availability features that are also exploited:
  - Multiple power supplies and power bars;
  - Redundant fans;
  - Multiple network interfaces, redundant switches;
  - ECC memory.
FAQ: Why would I use WX2 rather than a column-based database?
Solution 00000113 by Stuart Watt at 2008-01-30T14:21:34.000+0000
The WX2 BI database is a traditional row-based RDBMS, i.e. it stores data in rows, with the columns or fields that make up the row co-located. Column-based databases store the data by column, also known as ‘vertical partitioning’. This has the advantage that if the query only involves a small number of columns, then only those columns need to be accessed and the rest of the row can be ignored, minimizing disk I/O. This only works for relatively simple queries. Increase the query complexity, or include more rows in the query, and the fact the data has been decomposed into columns starts to work against you. Column-based databases are generally very good with simpler queries, but do not always have the flexibility and query richness of a traditional RDBMS.

Column-based databases were designed to try and minimize disk I/O for simple queries. WX2 is an ‘in-memory’ database and, as such, disk I/O is not so much of an issue. Queries run against data that is stored in very efficient memory structures, rather than simple disk caches. Disk-based copies of all data are maintained and should only a small numbers of columns be needed for a particular query, then only those columns are loaded into memory. Querying data that is held in memory also has the major advantage that no indices of any sort are required to access data at high speed. WX2 can scan data in memory at the rate of 200 million rows per second per system node.

Column-based databases also suffer from poor load speeds as the sources of data are always row based. WX2 has, over the years, developed very high speed parallel data load mechanisms that allow systems to be configured to load very large quantities of data in tightly defined windows of time.

Using traditional row-based storage, but in memory, gives WX2 the query richness and complexity of a full traditional RDBMS, combined with very high performance, and is not constrained by disk I/O or the presence of indices.
FAQ: The terms column-based and tokenisation are often used together. Do they mean the same thing?
Solution 00000114 by Stuart Watt at 2008-01-30T14:24:55.000+0000
Column-based databases often (but not always) use tokenisation as a method of reducing overall data volumes. Tokenisation involves replacing instances of a particular data value with a token which is usually smaller than the data value itself. Provided that there is lots of repetition in the data (e.g. lots of customers based in “Paris”, or called “Jones”), this leads to significant data compression. This in turn can allow relatively large databases to be held in memory, and it also allows other techniques to be used (e.g. bitmap indexes for a table, which can easily be combined in order to AND/OR predicates together without having to reference the underlying data).

Strengths
  - Can reduce data storage requirements and speed up processing as a result of reduced I/O and the possibility of using in-memory techniques;
  - Can sometimes get answers to queries without referring to base data (e.g. if just doing a count of rows which match simple predicates).

Weaknesses
  - Many simple operations will be unable to take advantage of tokens. For example:
     - All inequality predicates, assuming they can only use tokens to compare for equality;
     - Any predicates involving expressions (‘a mod 10 = 7’, ‘a matching '%hello%’’, ‘char_length(a) = 3’, ‘a.b = a.c’, etc).

In such cases, they probably need to de-tokenise all data before applying the predicate, which will be a significant performance hit. The issue of unpredictability will arise here too. A user will see a query run quickly because tokens can be used, then the next one may run slowly because they can't.

Other issues with tokenisation include:
  - Slows down data loading and data retrieval due to added complexity. Data needs to be tokenised as it comes in, and this means each attribute value needs to be checked against all existing tokenised values to determine whether to use an existing token or try to generate a new one. Also, data needs to be de-tokenised on retrieval of results, another overhead;
  - Does not work very well if attributes have lots of distinct values, as you don’t get much compression but you still always have the overhead of tokenising and de-tokenising data. As more data gets added, you need to check if you have already tokenised an attribute value, and this gets more expensive the more tokens you have.
FAQ: Why is ‘in-memory’ different to using a conventional disk-based database with a very large cache?
Solution 00000115 by Stuart Watt at 2008-01-30T14:27:48.000+0000
Disks are relatively slow mechanical devices that require data to be accessed serially. Much of the work done by a disk-based database is done under the assumption that the data is primarily disk-resident. Search algorithms, buffer management and indexing techniques are all weighted towards this fundamental assumption. Even when a database has been configured to hold cached copies of data in memory, its performance is badly affected by these deeply woven assumptions of disk-based residency.

WX2 knows that its data is memory-resident and was designed from scratch based on this assumption. It can therefore take direct routes to memory, simplifying algorithms and typically reducing code path lengths by a factor of ten.

The massive scan speed of in-memory tables also allows WX2 to produce optimal query plans by collecting dynamic statistics that determine the actual number of rows that will be accessed in a query rather than relying on potentially misleading estimates.

The downside of memory-based databases is, of course, the requirement for large quantities of memory. Until recently, this meant that in-memory databases were seen as a very expensive solution only applicable to very niche applications, but this has now changed. Blade hardware allows systems of incredible processing power, coupled with huge quantities of memory, to be built for very low cost. It is now possible to build a blade system with 2TB of memory for less than $0.5M. This would equate to holding all of a 4-6TB traditional disk-based system in memory, as typically 66-75% of any traditional database would be indices. In reality, the memory requirements would be significantly less than this, as not all data would necessarily need to be held in memory at the same time.
FAQ: What are database indices?
Solution 00000116 by Stuart Watt at 2008-01-30T14:29:29.000+0000
Indices are structures designed to help query tables without having to perform full table scans.


WX2 does not need to use indices to obtain performance because it is able to scan all the data at very high speeds.


Index Strengths

  - Some queries run extremely fast.


Index Weaknesses

  - Typically they result in much higher data storage requirements for the database. Indices often consume several times more space than the raw data;

  - Having to update indices slows down data loading and update, as all indices need to be updated as well as the underlying data;

  - Performance can become very unpredictable, as queries that can use indices are fast, those that can't are slow. In extreme cases, this may end up constraining what queries users are allowed to run to only those that can use indices;

  - They typically require expensive DBAs to determine which things to index, as the cost of indexing everything is usually prohibitive.
FAQ: How does WX2 compare with Sybase IQ?
Solution 00000117 by Stuart Watt at 2008-01-30T14:31:44.000+0000
Sybase IQ is a column-based database, which are discussed in "Why would I use WX2 rather than a column-based database?".

Sybase IQ has good user scalability, but little query scalability, whereas WX2 has both.

Sybase IQ uses query multiplexing to achieve good user scalability, but there is no way, other than throwing away the server and buying something more powerful, of scaling the performance of each individual query as the data volumes grow, or to achieve acceptable performance from the outset. 

Multiplexing involves running multiple copies of Sybase IQ on a number of servers. Multiple queries are divided up between the servers according to some predefined rules, but each individual query is handled by one server only. The servers see a single set of disk-based data, so although each query has its chunk of processing power, it will be contending for disk I/O with other servers running against the same data.

Sybase use the assumption that with today’s technologies it is always possible to buy a single server of sufficient power to handle even the most complex of business problems. Even if this were true, organisations would find themselves having to buy ever more expensive servers to maintain performance as data volumes grew.

WX2 is a ‘Massively Parallel Processing’ (MPP) database where each query is split up and executed by all processing nodes in a system. Systems can consist of a minimum of one node and a maximum of 10,000. Performance for any query can be scaled to suit a particular application and maintained as data volumes grow. Using blade servers as the platform means this scaling can take place in small, cost effective, increments that build on, rather than replace, existing infrastructure investments.

The use of MPP and blades means that, unlike Sybase, individual instances can be scaled up or down to get the right level of query performance. As the actual queries will be running against local memory-based data, there is very little contention for disk I/O.
FAQ: How does WX2 compare with Netezza?
Solution 00000118 by Stuart Watt at 2008-01-30T14:33:28.000+0000
Netezza runs on custom proprietary hardware, whereas WX2 runs on industry-standard, low-cost commodity hardware. Of all the technological differences between Netezza and WX2, this one clearly stands out as being the most important. Whilst the IT industry as a whole moves more and more towards the concept of utility computing and the virtualisation of all the key IT infrastructure elements, Netezza is trying to persuade the market to buy a very expensive lump of proprietary hardware that can only ever perform one dedicated task. The fact that they have gained the level of traction they have, clearly demonstrates there is a need for something different to standard database solutions. Even if there was nothing to choose between Netezza and WX2 technically (which there is), the proprietary hardware issue alone will mean that Netezza will find it very difficult to compete with WX2. Proprietary hardware is expensive and potential purchasers will need to make a massive leap of faith to purchase such a system.

Netezza is still a relatively new product and will suffer all the usual early product lifecycle issues. The WX2 software, on the other hand, has been under continual development since 1988 and has had 200 man years of development effort invested in it. It has been proved in the field on many occasions and is today providing reliable, stable operation for a number of organisations. Kognitio has used its own WX2 software to deliver high end analytical services for nearly five years and the experience gained from this has helped make the software a very stable and usable product.

Netezza gives very little away about how their systems actually work and real performance is difficult to judge. What is clear, though, is that at the heart of the system is a single SMP node that handles all the client interfaces, query compilation and optimisation, result aggregation, etc. Even the biggest system only has one of these nodes and so we would expect this to introduce a number of scalability issues. The back end “Snippet Processing Units” (SPUs) provide the MPP bit and perform parallel data filtering as the system scans its many disk drives. It is probable that Netezza has adopted this hybrid architecture because their software is based on the non-parallel open source Postgres database.

WX2 does not have any such point of serialisation as it was designed from scratch to be an MPP application. All database operations are parallelised, systems can be configured with multiple compilers, optimizers, client interfaces, etc. Although this sounds complicated, it is anything but, as all the parallel processes required to build a usable system are automatically and optimally placed and configured when the database is started, with no manual configuration being required.
FAQ: How does WX2 compare with Teradata?
Solution 00000119 by Stuart Watt at 2008-01-30T14:37:00.000+0000
Teradata, previously a $1.2B division of NCR Corporation and now a separate entity focused entirely on data warehousing, is the closest competitor to WX2 at the high end. Teradata’s offerings consist of data warehouse hardware (servers and storage), software (DBMS, data mining and CRM tools and market-specific applications) and services (industry solutions consulting, data warehouse consulting, customization and integration). A typical Teradata deal consists of several (if not all) of the above offerings, and costs several million dollars. Teradata is known as the market leader in large data warehouses (&gt;2TB) and has proved scalability to several hundreds of terabytes. Teradata has more than a decade of success and considerable market presence with hundreds of customers across several verticals and strong customer references. Teradata is strongest in the retail and telecom markets, but it also has significant presence in financial services, government, healthcare and transportation markets. The company is known for a very solid, massively parallel, high performance and scalable data warehouse solution, with particular strengths in large multi-user concurrency. However, Teradata is also known as the high-cost provider, with multi-year deals for tens of millions of dollars, and the lack of a clear pricing list or model. In addition, Teradata is known for having channel conflict with its professional services partners, since a large percentage of Teradata’s revenues come from internally-provided professional services (est. &gt;30%).

Limitations
  - The Teradata DBMS must run on Teradata’s WorldMark servers for tera-scale warehouses. Teradata gets its performance by clustering many server nodes together using proprietary communications switches (‘BYNET’). It takes a lot of hardware and switches to scale, which is expensive and costly to upgrade;
  - Teradata has made several attempts to support commodity platforms, but none of these have really led to anything. There is a Windows NT version of Teradata, but this does not seem to have been used for any significant installations;
  - Teradata uses third-party, generic storage (LSI and EMC), giving them no advantage in storage access over IBM and Oracle;
  - Terdata requires a proprietary version of Unix (MP/RAS) to run its larger systems. This version is not typically used in IT centres;
  - The database and system architecture has not substantially changed over the past 10 years, except for minor enhancements to keep up with Moore’s Law;
  - New Teradata systems are configured and pre-staged at the factory, then re-set up and re-tested on site, thus taking weeks to install;
  - Teradata has recently stopped publishing TPC-H results. This may because the results published by the other vendors, especially DB2 (IBM), have improved so much in past year;
  - Teradata’s rating on Gartner’s ‘Magic Quadrant for Data Warehouse DBMSs, 2004’ fell because of concerns over its inability to resolve its need for proprietary hardware and its continuing association with high pricing.

WX2 Comparison
  - WX2 uses cost effective, commodity hardware that is easy to upgrade and scale;
  - Because WX2 makes extensive use of ‘in-memory’ techniques, it would be expected to be considerably faster than Teradata;
  - WX2 pricing would be closer to Netezza pricing than Teradata’s, despite outperforming them both.
FAQ: What date formats do wxloader/wxunloader/wximport/wxexport support?
Solution 00000124 by Stuart Watt at 2016-05-25T14:07:14.000+0000
Note: wximport and wxexport are deprecated in version 8 in favour of wxloader and wxunloader respectively. The latter should be used instead if possible.

YY - year without the century
YYYY - year with the century
M - month number
MM - month number with leading zero
MMM - month name as a three-letter abbreviation
D - day of month
DD - day of month with leading zero
DDD - Julian day, i.e. day of the year
H - hour
HH - hour with leading zero
N - minute
NN - minute with leading zero
S - second
SS - second with leading zero
T - sub-second: number of Ts indicates the precision (e.g. "HH:NN:SS.TTTTTT")
PP - "AM" or "PM" (apparently for import only)
! - treat the next character as a literal
? - skip this character (for import, allow any character here; for export, fill it in with a space)

All format specifiers are case-insensitive and any characters not present above are treated as literals.
FAQ: What data formats are supported by wximport and wxexport?
Solution 00000131 by Stuart Watt at 2016-05-24T08:35:56.000+0000
Note: wximport has been superceded in version 8 by wxloader, which should be used if available. 

wximport supports:

  - CSV (with continuation) or fixed-format;

  - ASCII or EBCDIC;

  - Little endian or big endian;

  - Packed or unpacked decimals;

  - Implicit or explicit decimal points in numeric fields;

  - User-defined field separator character (per file or per field);

  - User-defined value representing NULL values;

  - User-defined date, time and timestamp formats;

  - User-defined optional century prefixes in date fields;

  - Field mapping;

  - Field exclusion.


wximport can also autoload a CSV file automatically, creating the table and field definitions by forward-sampling the data.


The wxexport utility offers essentially the same functionality and options as wximport but in reverse, taking data from a table and unloading it to a flat file.
FAQ: Which query queue is used by SYS?
Solution 00000132 by Stuart Watt at 2008-01-31T10:37:39.000+0000
SYS queries bypass the queues and are always able to run.
FAQ: How does the -d parameter in wximport / wxloader empty the contents of the table?
Solution 00000133 by Stuart Watt at 2016-05-31T11:38:13.000+0000
note: wximport is deprecated in version 8. Use wxloader instead if available.

The -d parameter in wximport and wxloader always does the equivalent of a 'truncate table'.
FAQ: Are concurrent parallel import and export streams subject to query queueing?
Solution 00000134 by Stuart Watt at 2008-01-31T10:51:43.000+0000
Concurrent parallel import and export streams are not subject to the normal rules of query queueing, so it is possible to have more import/export streams running than are technically allowed by the queue they are executing on. This is to ensure that parallel imports/exports perform as intended. Other query types continue to be constrained by the applicable queue limits.
FAQ: Does the choice of network interfaces made during installation affect subsequent external connectivity to the WX2 server?
Solution 00000138 by Stuart Watt at 2008-01-31T12:58:32.000+0000
The network interfaces specified during install refer to WX2 internal use only, i.e. by the SMDs when finding other nodes in the system, and then by the MPK after that.


The server will listen on all interfaces for external client connections.
FAQ: What are the byte alignment rules for WX2?
Solution 00000144 by Stuart Watt at 2008-01-31T13:32:27.000+0000
In a create table statement, list all aligned data types (decimal, int, bigint, real, float, date, time, timestamp, interval) first, followed by smallint (int2), tinyint (int1) and char to minimise row alignment overhead.


Note that the data portions of varchar fields always follow all fixed-length fields.
FAQ: Is a create image performed after a reclaim?
Solution 00000145 by Stuart Watt at 2016-05-24T08:27:03.000+0000
A reclaim will always run a create image after it completes. The only caveat is that if the system was booted sysimage before the reclaim began, there will be nothing for the create image to do - this is advisable in situations where you have an imaging script which creates images in the most efficient order, as 'create image' can sometimes duplicate a lot of work by creating images in a sub-optimal order.
FAQ: Is there a way of updating the definition of a view without having to drop all the objects that depend on that view?
Solution 00000148 by Stuart Watt at 2008-02-06T15:01:22.000+0000
As of WX2 v6.1.6, it is possible to update the definition of a view without dropping the dependent objects on that view as long as the columns within the view remain the same.
FAQ: Why do I sometimes receive an 'Invalid date value' error when I add a 1 month interval to the last day of a month?
Solution 00000149 by Stuart Watt at 2008-01-31T14:10:44.000+0000
The following SQL will produce an error in WX2:


select date '2008-08-31' + interval '1' month from dual;


Query           2        retcode = -1      ----   0:00.2     ----     ----

22008: [Kognitio][WX2 Driver][myserver] CG0065: Invalid date value


The reason for this is that ANSI SQL date intervals do not take account of the differing number of days in the months of the year. Therefore in the example above, the addition of a 1 month interval has resulted in a date value of '2008-09-31', which is invalid.


The same problem applies to the year interval with leap years:


select date '2008-02-29' + interval '1' year from dual;


Query           5        retcode = -1      ----   0:00.2     ----     ----

22008: [Kognitio][WX2 Driver][myserver] CG0065: Invalid date value


One workaround is to use the Oracle-compatible 'add_months' function instead which correctly adjusts the final day of the resultant month as appropriate.
FAQ: What is the range of valid exit codes for wxsubmit?
Solution 00000153 by Stuart Watt at 2008-02-01T09:31:56.000+0000
The 'quit [&lt;exit_code&gt;]' command in wxsubmit can return the same range of exit codes as all other Unix programs, namely between 0 and 255.
FAQ: Is it possible to have subqueries inside case statements with WX2?
Solution 00000154 by Stuart Watt at 2016-05-23T13:13:25.000+0000
Yes, for example:

select id, 
case when id = (select max(id) from ipe_alltable) then 0 else 1 end 
from ipe_alltable
FAQ: Will the whole WX2 server crash if an AP node crashes?
Solution 00000155 by Stuart Watt at 2008-01-31T16:34:42.000+0000
The whole server will not crash if an AP node crashes, as no WXDB processes are running on that node. Depending on how the AP crashes, you may find that wxprobe commands keep generating output about dead links, as an SMD has died without a clean shutdown, but that does not mean that the server stops working.
FAQ: Can I shut down one of the AP nodes without stopping the rest of the WX2 server?
Solution 00000156 by Stuart Watt at 2015-11-16T15:32:08.000+0000
You can shut down an AP without stopping the rest of the server.  You will want to shut down cleanly if possible so the SMD shuts itself down, which means it notifies the other nodes that it is going away and they don't keep trying to talk to it.
FAQ: Why is 'wxprobe -H' reporting the wrong MHz values for my CPUs?
Solution 00000157 by Stuart Watt at 2008-01-31T15:08:58.000+0000
Many CPUs are able to dynamically alter their clock speed to save power which may result in 'wxprobe -H' reporting a lower than normal value.
FAQ: How many variables can be defined in wxsubmit?
Solution 00000160 by Stuart Watt at 2008-01-31T15:49:44.000+0000
The arrays in which variable information is stored inside wxsubmit are resized dynamically as necessary, so there is no arbitrary limit on how many variables can be defined.
FAQ: Is a table or view image replicated per node or per RAMStore on each node?
Solution 00000161 by Stuart Watt at 2008-02-01T08:49:45.000+0000
When a table or view image is replicated, the contents of that table or view are copied to every RAMStore on each node. This means that replicating a table or view may use more RAM than was expected. 


For example, a node with 16GB of RAM running a 32-bit operating system will normally have 8 RAMStores. If a table or view is replicated to that node, then it will be copied 8 times and not once as might have been expected.
FAQ: Does WX2 support disk quotas?
Solution 00000164 by Stuart Watt at 2008-02-01T09:59:36.000+0000
WX2 does not currently support disk quotas.
FAQ: What are sm_time and tm_time in ipe_command measured in?
Solution 00000165 by Stuart Watt at 2008-02-01T10:11:18.000+0000
sm_time and tm_time in sys.ipe_command are both measured in milliseconds.
FAQ: Is there a suggested strategy for performing reclaims on a slab-by-slab basis?
Solution 00000170 by Stuart Watt at 2008-02-01T11:09:58.000+0000
A good approach is probably to have a regular reclaim scheduled which rotates through the slabs, rather than trying to specify the one with most reclaimable data. This can prove problematic if tables have varchars in them, or have a lot of deleted records as opposed to having been dropped.

The WX2 server is set up to use slab 1 for system table records, slab 2 for logging records (e.g. ipe_login, ipe_transaction, ipe_command), with all other data being spread over the remaining slabs. So one possibility is to reclaim the slab with the most data stored in it, which would mean ignoring slabs with little data in them.

For example, the following query will bring back the slab ids in descending order of space used:

select slab_id, sum(data_stored) from ipe_disk_slab group by 1 order by 2 desc

If there was a lot of resident, unchanging data on some slabs and so they were being reclaimed too often, consider reverting to the first strategy above of just rotating through the slabs regardless of space used.
FAQ: Is logging produced on all nodes or just master nodes and is there any difference between the log files on each node?
Solution 00000173 by Stuart Watt at 2008-02-01T12:05:31.000+0000
Log files should be produced on all nodes as a general rule. Normally directory contents for global operations will be synchronised so e.g. the `wxlogd startup` directory will have identical contents across all nodes, and the `wxlogd smd` directory will have common entries for session, serverdbg and cidebug files. Some log files are generated purely by the local SMD - e.g. the `wxlogd smd` output files, as these represent this individual SMD's output.


The AP is not chosen as a master in preference to other non-APs. The master election is based on IP addresses, so often the AP happens to be the one with the highest priority IP address within a system.
FAQ: Can you update the statistics on ROTTs?
Solution 00000175 by Stuart Watt at 2008-02-01T12:13:28.000+0000
It is not possible to update the statistics on ROTTs (RAM-Only Temporary Tables), as the system table entries for these objects are only added to RAM.
FAQ: Do Kognitio alter the 'standard' Linux kernel in any way in order to run WX2?
Solution 00000176 by Stuart Watt at 2008-02-01T12:17:26.000+0000
WX2 uses a standard Linux kernel, but Kognitio add certain utilities to, for example, set the IP address of nodes based on their physical position in an enclosure.
FAQ: Is it possible to identify reclaimable space created by TRUNCATE TABLE?
Solution 00000177 by Stuart Watt at 2015-01-08T11:08:46.000+0000
If you know the table id and the table has not yet been truncated, you can run the following command to find the number of megabytes that can be reclaimed by truncating that table:



select sum(words)/250000 from ipe_ftable where table_id=&lt;table id&gt;;



If you know the table id and the table has been truncated, run the following:



select sum(trunc_words)/250000 from ipe_ftable where table_id=&lt;table id&gt;;



Note that this space will not be freed up until you run a reclaim command. To find the table id of a table in console, double click on the table, then go to the object view tab, and the Id will be on the &lt;table name&gt; tab

FAQ: Does WX2 support synonyms?
Solution 00000178 by Stuart Watt at 2008-02-01T12:25:51.000+0000
WX2 does not support synonyms.


One workaround specifically for synonyms of tables or views would be the use of a new view with the appropriate synonym name.
FAQ: What is the scope of ALL privileges in its various security domains?
Solution 00000179 by Stuart Watt at 2008-02-01T12:30:01.000+0000
In most cases, ALL means every possible privilege. The main exception is the table domain, where ALL means update, insert, delete, select and references (as is standard for SQL).
FAQ: Why do I sometimes see one node using much more RAM than all the others when running SQL windowing queries?
Solution 00000180 by Stuart Watt at 2008-02-01T12:51:33.000+0000
Any SQL windowing function without partitioning, e.g.  'count(distinct item) over () as total_items') causes the entire result set for the query to serialise onto one RAMStore on one node, as there is no partitioning information available to allow it to be allocated across multiple nodes. This results in that one node greatly increasing its RAM usage, which in the case of extremely large result sets, can exceed the RAM allocation for that RAMStore, resulting in an out-of-memory error.


One workaround would be to use a derived table instead of windowing functions when aggregating large numbers of rows without any partitioning.
FAQ: Does the -z option in wxbackup perform the compression on the server running the wxbackup command or on the DB node performing the extraction?
Solution 00000181 by Stuart Watt at 2016-05-20T09:57:10.000+0000
From version 8 onwards, the -z option is deprecated, and data is compressed regardless of whether -z is specified or not. To turn off compression, use the --unloader-no-compress option. In version 8 compression is always performed on the database nodes, for greater parallelism and speed.

Prior to version 8, data will be compressed only if -z is specified, and it will be compressed on the server running the wxbackup command.
FAQ: What size is a Cluster Unit (CU)?
Solution 00000183 by Stuart Watt at 2008-02-01T13:31:05.000+0000
A CU is a cluster unit (page), which is WX2's base level of data storage on disk. It is 8,192 bytes (8k bytes) in size.
FAQ: What are the performance benefits to using truncate table instead of delete from to empty a table?
Solution 00000184 by Stuart Watt at 2008-02-01T13:43:45.000+0000
When WX2 truncates a table, it just marks it as having zero rows in the metadata, whereas when it deletes rows, it goes through each row one at a time and writes a delete transaction number into the header (hence it takes a lot longer for significant numbers of rows).


Because WX2 updates the table metadata on a truncate, it can't subsequently see the old rows, though they are still present on disk until they are removed at the next reclaim.


The rows removed through truncation are not processed on any table scans, which they would be if they had merely been deleted, which can result in a significant performance improvement for disk-based table scans.
FAQ: How do compressed indices work?
Solution 00000187 by Stuart Watt at 2008-02-01T14:35:36.000+0000
The compressed indexing mechanism is used to reduce the amount of I/O required when doing a table scan, so it generates an optimised table scan rather than direct access to a record of interest.

When an index is created on a column, the column value is reduced to a 32-bit representation before some techniques are used to compress the index further. This means that types with more than 4 bytes (e.g. bigint, strings of more than 4 bytes) lose some information.

For integer types, the least significant bits are used (to make a given tinyint/smallint/int value generate the same 32-bit value for indexing as the corresponding bigint value). For string types, the most significant bits are used as strings are stored backwards within WX2 for efficiency reasons, so the most significant bits are the start of the string, so a short string will now generate the same value as a longer string with trailing spaces.

When performing a scan, if a compressed index is applicable it can reduce the number of disk blocks which need to be read. Once the set of blocks has been determined, each row is processed normally by the scan - so although the compressed index might generate some false positives due to the compression used and the loss of some information when generating the original 32-bit value for indexing, the scan will ensure only relevant rows are qualified and passed through for the rest of the query. Scanning operates in the same way for all data types - i.e. it will look at all bytes of an attribute, rather than a subset of them.
FAQ: Does WX2 support BLOB/CLOB large binary objects?
Solution 00000191 by Stuart Watt at 2016-05-25T15:00:28.000+0000
WX2 does not currently support BLOB/CLOB large binary objects. 


As a workaround, WX2 has a 32KB VARBINARY data type that can be used to store binary data.
FAQ: What predicates or expressions will trigger the use of an applicable Compressed Index (CI)?
Solution 00000192 by Stuart Watt at 2008-02-01T15:52:39.000+0000
The following predicates or expressions will trigger the use of an applicable Compressed Index (CI):

  - =

  - in

  - between


for all discrete attribute types, e.g. integer or date, but not real or float.
FAQ: What protocols are used by WX2 to discover new nodes and how is this discovery restricted to an appropriate domain?
Solution 00000193 by Stuart Watt at 2008-02-01T16:08:33.000+0000
New nodes will typically be commissioned via the server farm deployment tool that would typically apply a disk image to the new server which would include a fully configured Linux and WX2 software package. The administrator would then need to set the WX2 system ID in the WX2 configuration file to associate this server with the relevant WX2 instance. The deployment tool may be able to execute this as a scripted task. The WX2 software may need to be upgraded to the latest operational version for the relevant WX2 instance.

Discovery of new nodes is performed using Kognitio’s own protocol built on top of UDP. A management network is defined and its IP subnet is used for broadcast detection. The WX2  software is designed to have a private network which contains only trusted nodes. Typically this is used for data and management traffic. This network should have no external gateway so untrusted nodes cannot send packets onto it at all, so if a packet arrives on the management network interface it is assumed it came from another WX2 node. Normally, this network is constructed through the use of VLAN tagging but in non-blade environments, it might actually be a physically different set of switches and network cables.

For environments which don't want to use a private subnet or who want extra security, IPSec can be used to provide a similar level of security on a management network with untrusted nodes on it. Using IPSec, one can define a key pair that every WX2 node has and require all management traffic to be signed. For added security, each node can alternatively have a public/private key pair. The private key is used to sign outbound packets and the node's public key is used to verify them.  This stops the trusted nodes from pretending to be each other as well as stopping untrusted nodes.

Detection is done by sending UDP broadcasts to the management subnet's broadcast address. This is done from a UDP socket bound to the management interface's IP address so the broadcast should only be sent on the management network. Replies are received through a socket which is also bound to the management network's IP. Provided that the correct kernel options are enabled (martian/rp filter, etc) this ensures that the discovery process will only receive UDP packets which are sent on the management network. Nodes which are not physically on the management network cannot be detected or inject any packets into the detection process. Additionally, the WX2 software can optionally be configured with SO_BINDTODEVICE setsockopt call on the UDP socket. This restricts the socket so that it will only ever receive packets from the management network even if the kernel is misconfigured.

In addition to the above, WX2 does 'reserved port' authentication on all incoming packets, broadcast or otherwise. Any packet with a UDP source port number &gt;1024 will be rejected. The kernel ensures that only privileged processes can send UDP packets with source ports
FAQ: Does WX2 support consistent reads?
Solution 00000195 by Stuart Watt at 2008-02-01T16:27:21.000+0000
WX2 supports consistent reads ignoring any uncommitted transactions if the suffix AT NOW is added to a select query.
FAQ: What level of Oracle compatibility does WX2 offer?
Solution 00000196 by Stuart Watt at 2008-02-01T16:31:48.000+0000
WX2 has implemented a significant amount of Oracle SQL compatibility:


  - SQL data functions such as: TO_CHAR, TO_DATE, TO_NUMBER, TO_TIMESTAMP, ADD_MONTHS, DECODE, LAST_DAY, MONTHS_BETWEEN, RTRIM, NVL, SYSDATE, TRANSLATE, TRUNC etc.;

  - The Oracle outer join syntax;

  - The Oracle analytical SQL windowing functions: LEAD, LAG, FIRST, LAST, NTILE, RATIO_TO_REPORT;

  - VARCHAR2(), NUMBER() datatypes as synonyms for VARCHAR() and DECIMAL()'

  - CREATE OR REPLACE VIEW syntax for view creation or update'

  - TRUNCATE TABLE syntax for emptying a table.
FAQ: What is a disk store slab?
Solution 00000197 by Stuart Watt at 2016-05-31T12:50:57.000+0000
A disk store slab is a logical division of a disk or disk partition into individually addressable areas, each of which may be customised to different requirements.


Slabs can each contain up to 16GB of addressable user data per disk store. Slabs can be assembled into slab groups, with each slab in the group having identical characteristics. Slab groups allow a user to specify that more than 16GB of storage per disk should be allocated to a particular set of tables.


The first slab is used for system tables, the second for logging tables (primarily to log connections, transactions, and commands), and all other slabs are used for the remaining user data. Operations attempt to remain within one slab for a reasonable period to minimise disk head movement (e.g. when importing data, a slab has a significant chunk added to it before considering use of another slab).


Tables are assigned to a slab group. If more than one slab exists within the group, data will be written to the first available slab until that is almost full, then to the second, and so on. This ensures that data loaded contiguously remains as contiguous as possible on disk – maximising the efficiency of compressed indices, disk scans and reclaims.


WX2 does not allow the user to use SQL to set up or manipulate slabs. System settings are defined when the newsys command is run and these cannot be changed.
FAQ: Can I commit and rollback DDL commands when running in transactional mode?
Solution 00000200 by Stuart Watt at 2015-11-16T15:20:04.000+0000
When WX2 is running in transactional mode, i.e. not autocommit mode, DDL statements can be committed and rolled back in the same way as DML statements. 


Example:

set mode transaction;     

create table t1(c1 char(1));

rollback;

select *  from t1;

42S02: [Kognitio][WX2 Driver][myserver] CI3013: Table TEST.T1 does not exist
FAQ: Can I rollback a truncate table statement?
Solution 00000201 by Stuart Watt at 2008-02-01T17:19:00.000+0000
It is only possible to rollback a truncate table statement if you are running in transactional mode, i.e. not the default autocommit mode.
FAQ: Can I edit SQL commands in wxsubmit using my favourite text editor?
Solution 00000202 by Stuart Watt at 2016-05-23T08:42:35.000+0000
wxsubmit allows SQL commands to be edited using the various Linux text editors. To specify which editor is to be used, either set the EDITOR shell environment variable:


export EDITOR=emacs


or use the 'set editor' command from within 'wxsubmit' itself:


set editor emacs;


To edit the last command using the specified editor, type 'edit'. To edit a specific command, type 'edit n' where n is the number of the query to be edited.
FAQ: Does WX2 offer any facilities to help profile data?
Solution 00000203 by Stuart Watt at 2016-06-01T09:30:16.000+0000
The 'profile' function within the strings plugin can be used to summarise a text attribute:


profile (mode, string)


Example

profile(1,'hi mark')	=&gt;	aaSaaaa

profile(2,'hi mark')	=&gt;	2aS4a

profile(3,'hi mark')	=&gt;	aSa


where 'mode' successively increases the level of summarisation.


Patterns

A=Upper case alpha (A-Z)

a=lowercase alpha (a-z)

N=numeric (0-9)

P=punctuation (.,;'@$ etc.)

S=space (space or tab)

X=non-printable


Usage Examples

* Find any text with non printable chars

  - select * from foo where profile(1,col1) like '%X%'

* Convert text numeric avoiding conversion errors

  - select cast(col1 to integer) from foo where profile(3,col1) = 'N'
FAQ: Does WX2 support multi-row inserts?
Solution 00000210 by Stuart Watt at 2015-11-16T16:23:34.000+0000
WX2 does support the SQL92 multi-row insert feature which allows insert statements of the form:


create  table t1(c1 int,c2 int);

insert into t1 values (1,1), (2,2), (3,3), (4,4);


This can potentially reduce the query overhead by only doing the compilation phase once for multiple rows.
FAQ: Why does WX2 not make use of OLAP technology to speed up queries?
Solution 00000211 by Stuart Watt at 2008-06-25T14:14:14.000+0000
Some warehouse implementations seek to build OLAP cubes from the main data warehouse and then base all subsequent reporting on those cubes. WX2 removes the need for this inflexible and costly (in time to build and refresh) intermediate step. OLAP is well recognised for its performance with smaller scale data sets but does not scale well over time. OLAP came about as an accelerator for reporting against poorly performing relational databases, but this is no longer the case with technologies such as WX2. 

Potential issues with adopting OLAP:
  - OLAP introduces another technology to manage;
  - An OLAP cube is a summary which has inherent limitations which constrains analytical flexibility;
  - Information lag - Cubes have to be rebuilt/updated to introduce new data;
  - Cube design needs prediction of analysis work, this limits ad-hoc usage which can be 'unpredictable';
  - Cubes have practical constraints on:
     * Number of dimensions;
     * Member limits, i.e. number of values in a dimension;
     * Number of measures;
     * Overall size of cube;
  - Cubes become themed or topic based which can create information silos;
  - Cubes can be inflexible if new derived attributes or special measures are required;
  - Cubes scale well for number of users but have little scope to improve individual complex query performance.

WX2 with its mature well proven analytical SQL implementation provides for any degree of complexity with no limits on computations or size of dimensions - unlimited dimensionality and on-the-fly aggregation. The massively parallel architecture of WX2 will scale-up in-line with data growth, user growth and query complexity while providing excellent query performance at a price-point appropriate to business budgets.
FAQ: What Linux/Solaris swap setting should be used with WX2?
Solution 00000214 by Stuart Watt at 2008-02-06T11:55:58.000+0000
Kognitio strongly recommends that swap is turned off on all nodes running WX2 by executing the 'swapoff -a' command . This is because swapping is an undesirable event with a memory-oriented RDBMS such as WX2.
FAQ: What do the timings from wxsubmit mean
Solution 00000215 by Kognitio Support at 2016-06-01T10:30:37.000+0000
When running a query with submit, you can have up to four timings displayed. For example:


...

Query 1   134 Row(s) Retrieved.    -----  0:00.4  0:00.6  1:14.0 


The first timing (----- above) is the time taken to prepare the query, which is not applicable in this case.


The second timing (0:00.4 above) is the time taken to execute the query.


The third timing (0:00.6 above) is the time taken to retrieve the first result row

The fourth timing (1:14.0 above) is the time taken to run the whole query.
FAQ: How does WX2 licencing work?
Solution 00000222 by Kognitio Support at 2013-02-11T11:41:55.000+0000
This solution provides a short overview of licencing. The topic is dealt with in the configuration and maintenance manual, so further details should be sought there.


For a WX2 system, exactly one base licence must be present at all times. Further extension licences can be added to increase the amount of resource licenced. For example, a disk extension licence can be added if the system needs more disk space than is allowed by the existing licence.


Licences must be sufficient to cover all the hardware available for WX2 - a licence for e.g. 100GB RAM on a 120GB system will not allow the system to run with a subset of the RAM - the system will not be bootable until the licence is adequate for the hardware available.

If no licences at all are installed on the system, an implicit licence is used (note this does not include normal Kognitio support, although a support forum is available). Note that this licence is only applicable if no licences are installed, so if "wxlicense -i" or "wxlicense -Xi" show previously existing licences those must be deleted with the "wxlicense -Dn &lt;licence number&gt;" command.
FAQ: What is a reasonably level of RAM occupancy
Solution 00000236 by Kognitio Support at 2008-07-02T13:10:58.000+0000
WX2 is an in-memory database. For efficient operation, the level of RAM usage before any queries are run needs to allow sufficient workspace for buffering of intermediate results and the like during query processing.

A good rule of thumb is to allow 50% of RAM to be used for persistent objects (table and view images) and reserve the rest of RAM for working space. An upper limit of 75% for persistent objects is advised.

Once memory occupancy gets above 75% there is a danger of internal memory fragmentation, leading to performance penalties and occasional errors being returned.

The effect on performance of increasing RAM occupancy is hard to determine without experimentation on the system in question - it depends on the queries being run and the level of concurrency. If more RAM needs to be used, this can be mitigated by reducing the run length for queues to reduce concurrency. 

In addition, using more RAM for view images might simplify queries if the relevant views are often queried, hence improving rather than reducing performance for those queries.

Queries which can use small buffers may run at the same speed as RAM usage increases, but other queries can slow dramatically. If a query can currently put a temporary table into RAM, but then RAM usage increases, this might mean the object now has to be scanned 10 times rather than once in order to e.g. do a sort.

For any given system, the best answer is to see the effects given the current workload. This can be done by creating a RAM only temporary table, inserting some rows, monitoring performance, inserting more rows, monitoring performance again, ...
FAQ: What is the precision of REAL/FLOAT/DOUBLE?
Solution 00000237 by Kognitio Support at 2008-07-03T10:18:51.000+0000
REAL can hold floating point numbers in the range -3.4E38 to 3.4E38, and take up 4 bytes, 24 bits of which are for precision.

FLOAT/DOUBLE can hold floating point numbers in the range -1.7E308 to 1.7E308, and take up 8 bytes. In this format there are 53 bits of precision.
 
The highest precision you can achieve on WX2 with floating point is 53 bits, compared to the 63 bits you get with a maximally-sized decimal, so you will get less precision than with the equivalent decimal value.
 
Also, these floating point representations are subject to the inaccuracies inherent with that representation (e.g. some numbers cannot be represented exactly with FP), and also to varying answers when e.g. summing, depending on the order used to add the numbers (adding a lot of small numbers together, then large numbers gives a different answer than doing the operation the other way round). These are properties of floating point representation and floating point arithmetic, rather than issues specific to WX2.
FAQ: How can I use virtual tables to find out what WX2 is doing?
Solution 00000251 by Kognitio Support at 2012-07-30T14:49:05.000+0000
There are a number of virtual tables in the SYS schema which allow the state of WX2 to be determined via SQL queries. The most useful are:

ipe_allcursessions          shows current sessions and their associated queries
ipe_allram_access          for looking at current RAM accesses
ipe_allram_images           for looking at current RAM images
ipe_disk_access             for looking at current disk accesses
ipe_disk_slab                   how full each disk slab is
ipe_ftable                         for replacing the old disku functionality - can see all the disk metadata broken down by disk and slab
ipe_query_queues           the status of each queue - which queries are queued/running for each one, how long they spent in each state, etc.
ipe_query_queue_stats   historic information for queues, and their current configuration
ipe_alllocks shows information on all the granted and pending locks in the system
ipe_mpk tables which provide information on the network performance between nodes and processes.

For many of these virtual tables there are views in the SYS schema restricting their output to information relevant to an individual user - e.g. ipe_cursessions is a view on ipe_allcursessions.

As well as the links above, it is possible to find out about a virtual table by executing the following SQL from 7.2.0 onwards: "explain &lt;virtual table name&gt;". This shows the comments for the table and every commented column in that table.

For example, "explain SYS.IPE_DISK_SLAB" gives:

Diagnostic on george for 7.02.01-s120714:
VIRTUAL TABLE IPE_DISK_SLAB in schema SYS
ID 480, location RAMONLY, distribution RAND, 20 columns
Owner: "SYS"
Table created: 2012-05-21 08:22:18
Width in RAM:  24 bytes header + 4 null bytes + 96 data bytes = 124 bytes

Comment: 'information on logical disk usage by slab'

Columns:-
MPID        INTEGER        'message passing id of the disk store'
SLAB_ID        INTEGER        'slab number'
DATA_STORED        INTEGER        'logical disk pages stored'
FREE_SPACE        INTEGER        'logical disk pages free'
TIDEMARK        INTEGER        'high water mark of user data'
LOWMARK        INTEGER        'low mark for system informaton stored at top of slab'
LOGICAL_END_ADDR        INTEGER        'The logical address at the end of the slab'
LOGICAL_END_CU        INTEGER        'The final (logical) cache unit of the slab'
LOGICAL_SPACE_LEFT        INTEGER        'How much space is left for data on the slab measured in cache units'
INITIAL_INSERT_THRESHOLD        INTEGER        'The initial disk usage level before other, lower, slabs are considered for inserts'
MAX_INSERT_THRESHOLD        INTEGER        'How full we allow the disk to become before rejecting new inserts'
CURRENT_INSERT_THRESHOLD        INTEGER        'The current disk usage threshold'
PHYS_START_SEC        BIGINT        'The physical disk sector that is the origin point of this slab'
R5_PHYS_START_SEC        BIGINT        'phys_start_sec adjusted to take into account raid 5 stripes, this will be the true value'
R5_PHYS_END_SEC        BIGINT        'The final, physical disk, sector of a slab adjusted to take into account raid 5 stripes'
PHYS_DATA_START_SEC        BIGINT        'The physical disk sector which is the start of user data for this slab'
INITIAL_FTABLE_RESERVE_THRESHOLD        INTEGER        'Initial filetable reserved space (percentage) before considering other slabs'
CURRENT_FTABLE_RESERVE_THRESHOLD        INTEGER        'Current filetable feserved space (percentage) before considering other slabs'
FTABLE_RESERVE_THRESHOLD_INC        INTEGER        'How much to increase the current_ftable_reserve_threshold by when that limit is reached'
FTABLE_USAGE        INTEGER        'How full the file table (in terms of supported table ids) is as a percentage'

No selectivity information

Create text:-
CREATE RAM ONLY TABLE "SYS"."IPE_DISK_SLAB" ("MPID" INTEGER , "SLAB_ID" INTEGER , "DATA_STORED" INTEGER , "FREE_SPACE" INTEGER , "TIDEMARK" INTEGER , "LOWMARK" INTEGER , "LOGICAL_END_ADDR" INTEGER
 , "LOGICAL_END_CU" INTEGER , "LOGICAL_SPACE_LEFT" INTEGER , "INITIAL_INSERT_THRESHOLD" INTEGER , "MAX_INSERT_THRESHOLD" INTEGER , "CURRENT_INSERT_THRESHOLD" INTEGER , "PHYS_START_SEC" BIGINT , "R
5_PHYS_START_SEC" BIGINT , "R5_PHYS_END_SEC" BIGINT , "PHYS_DATA_START_SEC" BIGINT , "INITIAL_FTABLE_RESERVE_THRESHOLD" INTEGER , "CURRENT_FTABLE_RESERVE_THRESHOLD" INTEGER , "FTABLE_RESERVE_THRES
HOLD_INC" INTEGER , "FTABLE_USAGE" INTEGER )

Create image text:-
CREATE TABLE IMAGE "SYS"."IPE_DISK_SLAB"



Query           2              43 rows     ----   0:00.0   0:00.0   0:00.0

In addition, SYS.IPE_DESCRIPTION contains all comments, so can be used to locate relevant virtual tables. For example, "select * from SYS.IPE_DESCRIPTION where COMMENT imatching 'slab' order by 1,2" gives:

SCHEMANAME    TABLENAME    COLUMNNAME    COMMENT
SYS    IPE_ALLCOMP    INDEX_SIZE    how large the index is in bytes PER SLAB
SYS    IPE_ALLSLABS    (null)    information on slab allocation for future inserts
SYS    IPE_ALLSLABS    SLAB_ID    slab id
SYS    IPE_DISK_ACCESS    PARTITION_ID    slab number
SYS    IPE_DISK_COMPRESSION    (null)    information about disk compression by slab
SYS    IPE_DISK_COMPRESSION    SLAB_ID    slab number
SYS    IPE_DISK_SLAB    (null)    information on logical disk usage by slab
SYS    IPE_DISK_SLAB    CURRENT_FTABLE_RESERVE_THRESHOLD    Current filetable feserved space (percentage) before considering other slabs
SYS    IPE_DISK_SLAB    INITIAL_FTABLE_RESERVE_THRESHOLD    Initial filetable reserved space (percentage) before considering other slabs
SYS    IPE_DISK_SLAB    INITIAL_INSERT_THRESHOLD    The initial disk usage level before other, lower, slabs are considered for inserts
SYS    IPE_DISK_SLAB    LOGICAL_END_ADDR    The logical address at the end of the slab
SYS    IPE_DISK_SLAB    LOGICAL_END_CU    The final (logical) cache unit of the slab
SYS    IPE_DISK_SLAB    LOGICAL_SPACE_LEFT    How much space is left for data on the slab measured in cache units
SYS    IPE_DISK_SLAB    LOWMARK    low mark for system informaton stored at top of slab
SYS    IPE_DISK_SLAB    PHYS_DATA_START_SEC    The physical disk sector which is the start of user data for this slab
SYS    IPE_DISK_SLAB    PHYS_START_SEC    The physical disk sector that is the origin point of this slab
SYS    IPE_DISK_SLAB    R5_PHYS_END_SEC    The final, physical disk, sector of a slab adjusted to take into account raid 5 stripes
SYS    IPE_DISK_SLAB    SLAB_ID    slab number
SYS    IPE_DISK_TRAIN    (null)    An overview of each request registered with the disk train on a per-slab basis
SYS    IPE_DISK_TRAIN    BLOCKS_TO_READ    How many blocks the train will have to read for this request on the relevant slab
SYS    IPE_DISK_TRAIN    SLAB_ID    Which slab is this request targeting
SYS    IPE_DISK_TRAIN    SRR_ID    The Slab Read Request id, a unique identifier for a scan of a slab for a partiular read request
SYS    IPE_DISK_TRAIN_PENDING_READS    (null)    Details of each request made to the disk train on a per-slab basis, shows the break down of each requests current state, how many blocks remain to be read, each sub-group of blocks
SYS    IPE_DISK_TRAIN_PENDING_READS    SLAB_ID    Which slab the read pertains to
SYS    IPE_DISK_TRAIN_PENDING_READS    SRR_ID    The Slab Read Request id
SYS    IPE_DISK_TRAIN_QUEUES    SLAB_ID    The slab to which the blocks this element holds belong to
SYS    IPE_FTABLE    BLOCK_COUNT    number of blocks touched by the table on the slab
SYS    IPE_FTABLE    DEL_ROWS    approximate count of the number of deleted rows on the slab
SYS    IPE_FTABLE    DEL_WORDS    approximate size of the inactive portion of the table on the slab
SYS    IPE_FTABLE    FIRST_PTR    disk address of first record for the table on the slab
SYS    IPE_FTABLE    LAST_PTR    disk address of last record for the table on the slab
SYS    IPE_FTABLE    NROWS    approximate count of number of table rows on the slab
SYS    IPE_FTABLE    PARTITION_ID    slab number
SYS    IPE_FTABLE    TRUNC_WORDS    approximate size of any truncated sections of the table on the slab
SYS    IPE_FTABLE    WORDS    approximate size of the active portion of the table on the slab
SYS    IPE_SLAB    (null)    information about slab size and other metadata
SYS    IPE_SLAB    MAX_FILES    maximum number of tables on a single slab instance
SYS    IPE_SLAB    SLAB_ID    slab number
SYS    IPE_SLAB    START_SECTOR    sector the slab starts on
SYS    IPE_SLAB_TABLE    (null)    information about which slabs can contain which tables at commission time
SYS    IPE_SLAB_TABLE    SEQ    sequence number of table number range for the slab
SYS    IPE_SLAB_TABLE    SLAB_ID    slab number
FAQ: How can I use the ipe_allcursessions virtual table?
Solution 00000252 by Kognitio Support at 2011-01-27T14:30:30.000+0000
ipe_allcursessions contains the following columns of interest:


session: this is the session id, which can be used to look up information in the logging tables (ipe_login, ipe_transaction).

user_id: the name of the user for this session.

abort: updating the table and setting abort to 1 results in the current query being aborted. Setting the value to 2 results in the session being aborted.

connect_time: a timestamp indicating when this session connected to WX2.

last_command: the last SQL command run by this session (may still be running).

command_running: indicates the state of the last command as follows:

0: idle (note - may still be holding database locks)

1: running

2: rolling back the last statement

3: rolling back the last transaction

4: queued

5: waiting for the query to be compiled

client_name:a textual name for the client machine involved in this session.

net_if_name: identifies the connection handling process within WX2 that is responsible for this session.

net_address: address of the client machine.
FAQ: How can I use the ipe_allram_access virtual table?
Solution 00000253 by Kognitio Support at 2011-01-27T14:29:29.000+0000
ipe_allram_access contains information about every database access to the ram store processes, with a row for each access on each ram store.

Note that an access involves reading data from WX2, so there will be no entries for e.g. parameterised inserts / imports.

The relevant columns in ipe_allram_access are:

mpid: the message passing id of the ram store concerned.
request_id: a unique identifier for each access.
access_type: textual description of the access.
state: description of the access state - normally if problems are observed it is a good idea to check for any mpids which are in a different state to the others involved in this access.
session: session for this access, can be joined into ipe_allcursessions or the logging tables (ipe_login, ipe_transaction).
transaction: transaction for this access, can be joined into the logging tables (ipe_transaction, ipe_command).
user_id: numeric identifier for the user triggering the access. Can be joined into ipe_alluser or ipe_allcursessions.
table1_id, table2_id: ids of tables involved in the access - can be joined into ipe_allram_access, and other system tables if the ids are &lt; 10,000,000 (ipe_alltable), or 10,000,000 to 20,000,000 (ipe_allview_img).
records_inserted: an indication of how many records have been inserted into the target for this access. This will be reset each time the access is restarted, so most commonly use is to check this field is changing rather than relying on its absolute value.
FAQ: How can I used the ipe_allram_images virtual table?
Solution 00000254 by Kognitio Support at 2011-01-27T14:28:03.000+0000
ipe_allram_images contains a row for every RAM store for every image in RAM (whether a table image, view image, or temporary table).

The relevant columns are:


mpid: the message passing id of the ram store in question.

table_id: up to 10,000,000 for a table image, 10,000,000 to 20,000,000 for view images, and over 20,000,000 for temporary tables / streams.

numrecs: the number of records for this table_id on this ram store.

crtrans: the transaction number which created the table - can be used to join to the logging tables (ipe_transaction, ipe_command).

deltrans: the transaction number which dropped the table.

headertype: indicates how large the header is for each row in the image. FULL is 24 bytes, NONE is 0 bytes, SIZELINK is 8 bytes for variable length records.

recordlen: the record length in bytes (does not include the variable portion for variable length records).

distribution: how the table is distributed in RAM.

free_space: how much space is available in bytes out of the space already allocated to this table.

ram_used: the ram allocated to this table in bytes.

owner: an integer identifying the user who created the image.
FAQ: How can I used the ipe_disk_access virtual table?
Solution 00000255 by Kognitio Support at 2011-01-27T14:20:54.000+0000
ipe_disk_access has a row for each access on each disk store slab. Note that an access involves reading data from WX2, so there will be no entries for e.g. parameterised inserts / imports.


The relevant columns are:


mpid: the message passing id of the disk store in question.

partition_id: the slab id of the slab in question.

operation: a textual description of the operation being performed.

using_ci: an indication of whether the operation is using a compressed index.

table_id: the id of the table upon which the operation is being performed.

first_page: the first page on this slab which is touched by the table.

last_page: the last page on this slab which is touched by the table.

current_page: the page currently being processed.

session: the session id for this operation.

tno: the transaction number for this operation.

active: whether the operation is currently active (alternatively, it could be queued waiting for a table scanning thread to become available).

FAQ: How can I use the ipe_disk_slab virtual table?
Solution 00000256 by Kognitio Support at 2011-01-27T14:22:47.000+0000
ipe_disk_slab shows how full each slab within the system is.

This allows skew of slabs to be identified, and reclaims/repacks to be scheduled when appropriate.

For example, if the logging slab (slab 2) is getting full, it should have old entries deleted and be reclaimed.


The relevant columns are:


mpid: the message passing id of the disk store in question.

slab_id: the slab id of the slab in question.

data_stored: how many 8KB disk pages have been used on this slab for user data.

free_space: how many 8KB disk pages are left for use on this slab for user data.

tidemark: the highest numbered page reached by user data.

lowmark: the upper limit towards which tidemark is moving (note: there is a buffer zone of a few pages between tidemark and lowmark such that tidemark cannot get right up to lowmark).
FAQ: How can I used the ipe_ftable virtual table?
Solution 00000257 by Kognitio Support at 2011-01-27T14:24:23.000+0000
ipe_ftable exposes metadata from within the disk subsystem to allow the user to identify which tables are taking up most space, and what space is used by already dropped tables (and hence will be recovered at the next reclaim).


The relevant columns are:


mpid: message passing id of the disk store in question.

partition_id: the slab id of the slab in question.

table_id: the table id of the table in question.

drop_tno: the transaction number which dropped this table - 2147483647
indicates the table has not been dropped.

first_ptr: the disk address of the first record on this slab.

last_ptr: the disk address of the last record on this slab.

nrows: an approximate count of the number of rows for this table in this slab, including deleted and rolled-back records. Note that this can be an overestimate with double-counting of records sometimes occurring, but it is usually fairly accurate.

low_tno: the lowest tno which added records to this table.

high_tno: the highest tno which added records to this table.

block_count: the number of 8KB disk pages containing records from this table.
FAQ: How can I used the ipe_query_queues virtual table?
Solution 00000258 by Kognitio Support at 2011-01-27T14:26:51.000+0000
ipe_query_queues contains information for each query currently queued/running which is queueable.


The relevant columns are:


queue: the numeric queue identifier.

pos: the queries position in the queue.

user_id: numeric identifier for the user submitting the query.

session: the session submitting the query (can be joined into ipe_allcursessions or the logging tables ipe_login and ipe_transaction).

tno: the transaction number for the query (can be joined into the logging tables, ipe_login and ipe_transaction).

amid: the internal process id of the connection handler for this query.

running: whether the query is running, as opposed to queued.

secs_queued: how many seconds this query was queued for to date..

secs_running: how many seconds this query has been running for to date (0 if still queued).
FAQ: How can I used the ipe_query_queue_stats virtual table?
Solution 00000259 by Kognitio Support at 2016-06-10T12:39:32.000+0000
ipe_query_queue_stats contains information on the current configuration of a query queue, and statistics on how the queue has been operating.

The relevant columns are:

queue: numeric identifier of the queue.
len: how many items are in the queue are the moment, including running queries.
nrunning: how many items are running from this queue at the moment.
nwaiting: how many queries are waiting to run in this queue.
nbarriers: how many barriers have been added to the queue to constrain the number of queries which can run concurrently.
totaljobs: how many jobs have been submitted to this queue since the last WX2 restart.
maxrun: how many queries are allowed to run concurrently from this queue.
maxwait: how many queries are allowed to wait in the queue(0 indicates no limit)
load5m, load1h, load5h: these show the load over the last 5 minutes, 1 hour and 5 hours in terms of the average length of the queue over that period including running queries.
queuetime10, queuetime50, queuetime100: these show the average queue time in seconds for the last n queries in the queue.
runtime10, runtime50, runtime100: these show the average runtime in seconds for the last n queries in the queue.

Note that a row is only put into this table when a queue is first used after a WX2 restart, so immediately after a restart the table will typically be empty.
FAQ: Create or replace view [image]
Solution 00000309 by Kognitio Support at 2009-11-04T08:51:38.000+0000
Create or replace view has a number of options. This solution explains those options.

1) CREATE OR REPLACE VIEW

If the signature of the view (i.e. the ordered list of column names and types) is unchanged, WX2 will change the details of the view in the system tables to reflect the new definition.

If the signature has changed, this command will only succeed if there are no dependent views. If that is not the case, you should use...

2) CREATE OR REPLACE VIEW ... CASCADE

The CASCADE option forces the definition of this view and any dependents to be regenerated (so for a view with a lot of dependents this will take longer than the non-cascade option). If any dependents have images the command will error. In this case, you should use ...

In all cases so far, any image of the original view is dropped.

3) CREATE OR REPLACE VIEW ... CASCADE { CREATE | DROP } IMAGES

This option indicates that existing images of this view and its dependents should be recreated / dropped. 

If CREATE IMAGES is specified, the target view for the command will have a random image generated - to specify a different distribution use CREATE OR REPLACE VIEW IMAGE.

In addition, the IGNORE ERRORS option can be specified with (2) or (3), which indicates any dependent views which can no longer be created (e.g. because their definition is no longer valid) will be removed. If used with CREATE IMAGES and a dependent view image can no longer be created although the view definition is still valid, the view definition will still exist but without an image.

CREATE OR REPLACE VIEW IMAGE  allows replacement of an existing view image, and can be used with all of the options above. If no distribution is specified in the command line a random image of the view will be created regardless of any previous distribution.
FAQ: What can be put in the configuration file?
Solution 00000325 by David Wild at 2012-09-03T15:11:16.000+0000
This is not a complete list of WX2 config. file settings. Note that in general you should not need to make these settings as the default behaviour should be sufficient. The only setting that MUST be made is [general] system_id.


[general]

1. system_id=mysystem

2. min_free_kbytes=50000

 

1. System id. Must match with system license name. This is what allows the smds to synchronise with each other to determine which nodes comprise a system.

2. Make the kernel swap daemon get in earlier, rather than waiting until there is almost no free memory. Should not normally be set.

 

[system]

1. external_net=eth0

2. memsize=16000000000

3. max_ram_per_proc=2139095040 

4. default_net=eth0

 

1. SF Solution 97: "To enable connection bouncing where ‘eth?’ is the interface that is being used by the clients to connect to the WX2 server". Note for this to work all clients should be able to see the relevant interface on all nodes, otherwise they can be bounced to a node that they cannot connect to.

2. Make this system behave as if the nodes were 16G. This will affect the output to wxprobe -HN

3. Controls how much RAM a WX2 process can use - a way of restricting how big RS nodes can grow.

4. Tells WX2 what network devices to use for internal traffic.  This isn't

always present though -- on appliance mode systems (LMG, POC10) this is

detected automatically from the Linux image.

 

[runtime parameters]

1. ci_systab_solo=1

2. ds_gsr_percent=80

3. lkti=30 

4. leave_embedded=1

5. rcus=10      

6. rsec=160

7. lockcheck_crash=0

8. q10_maxrun=1

9. q10_pause=1

 

1. Enable new "less intrusive, finer grain" locking code. The default setting from 6.1.7 onwards.

2. Reclaim a slab once we encounter a chunk (typically a 100KB buffer) with reclaimable space &gt; 80%. Then revert to reclaiming everything possible for that slab. Used to avoid worst case reclaim where a single record is reclaimable early in the slab, then hardly anything after that. 80 is the default now, so should be no need to set this parameter in normal use.

3. Lock Timeout setting. Time in seconds to wait for a pending lock. Default is one day.

4. Record embedded comments in SQL code in ipe_command. Default is to remove embedded comments. The only reason for setting this is to allow applications to pass audit information into the command log (as used on BT).

5. Number of pages to reserve per slab between the data growing up the slab, and data at the top of the slab. If the slab fills up, this setting can be reduced to allow a small number of writes to take place, which can be enough to allow reclaim to run. Default is 10, and if this is ever reduced on a machine, need to reset to the default afterwards otherwise the next time disk fills up you may not be able to recover.

6. Equivalent of (5) in sectors, but now obsolete as calculated from the setting in (5).

7. There is some code in the server which crashes if an internal lock is held for more than 10 minutes (not the lock manager locks - internal locks are grabbed to protect access to a particular resource inside the server, rather than a system/table/column lock). If the lockcheck_crash parameter is set to 0 this does not happen. Normally we want this switched on as it is a good way of detecting things like a node hanging - usually there will be attempts to send messages to that node, they will hold internal locks, and so after 10 minutes of the node not responding the server will tend to crash. 

8. Specify the maximum number of concurrent queries for queue 10 to be 1.

9. Specify that queue 10 is paused - i.e. no queries which would queue can be run.


[boot options]

1. raid_cluster_size=4   

2. virtual_diskstores=1    

3. enable_hugelb=no    

4. sessions_per_cp=30 

5. num_int_nodes=60

6. max_fixed_pool=4800000000

7. ds_ramsize=0x3400000

8. se_ramsize=300000000

9. fixed_pool_size=20

10. tm_ramsize=100000000

11. num_comp_nodes=20

 

1. RAID cluster size. Used to enable SW RAID. Newsys required to activate new settings.

2. Used to allow the system to boot even if disk resources are missing, assuming SW RAID is in use. WX2 will place a disk handling process for each missing disk resource somewhere in the system, and this will handle IO for that disk resource via SW RAID.

3. Enable huge pages, which was implemented as a possible aid for dealing with Linux resource exhaustion problems. No reason to use this.

4. Number of Association Managers (AMs or sessions) per I/O process (per WX node). Number of connections per node.

5. Number of interpreter processes (default is the maximum of the number of nodes in the system, and 20). If a system is likely to have more concurrent queries running, it is possible to exhaust the interpreter pool, so increasing this parameter and restarting WX2 is an expedient solution.
Note that to reduce the parameter you will probably need to set topup_intnodes=0 in the [boot options] section of the config file.
6. The maximum amount of memory allocated to non-WX2 usage. Before 6.1.7 this typically was the value used as the estimate from configuration tended to be too high. From 6.1.7 onwards, the estimates are better so this is simply a cap on the memory allowed for non-WX2 usage given all other settings.

7. Memory allocated to each Diskstore process. This amount of RAM used by each disk store is dependent on the size of the disk resources, and the number of ram stores in the system.

8. Sets the amount of RAM available for SQL compilers in the server. If set without the most significant bit being on, the compilers will be given dedicated RAM, whereas the default setting is for them to share with other processes on each node such as interpreters, misc and IO nodes (but not disk stores and ram stores).

9. Allocate 20% of memory for non-wx2usage - note this will be capped by max_fixed_pool. Currently used on mig_bupa2, which has a mixture of nodes with various RAM sizes, as the default sizing algorithm does not deal well with this situation. 

10. Amount of RAM available for each interpreter process. 

11. Number of compiler processes (default is 10).   


[disks]

1. direct_io=yes

2. sparse_file=yes

 

1. CASE 1108: Direct write to disk without going via Linux. To help eliminate Linux grabbing too much RAM and invoking OOM killer on WX2 processes. Now the default setting, so no need to use in config files.


2. Set to yes if you don't want the disks to be initialised during a newsys. Useful for quick recommisioning of test systems.

 

[wxsmd]

1. tolerate_changes=missing_node, missing_link, missing_disk

 

1. This allows WX2 to tolerate a missing disk resource when the system is started as well as broken links and missing nodes - otherwise the default is to fail to start with an error reporting which disk resources are missing. For the system to work in such a case, SW RAID must be on, and virtual_diskstores enabled.

 

[mpk]

1. queuelimit=10000

 

1. Prevents the Message Passing Kernel (MPK) from throttling messages at the default of 1000 outstanding messages. Not required.

 

[capabilities]

1. not_master_nodes=poc3-ap2


1. This is a way to prevent certain nodes being the master smd - the nodes in the system choose a master, and that has special requirements (e.g. if you have a recovery script which works off smd events, you'd want dumping to take place on a node which has access to lots of storage).
FAQ: Problems caused by security extension SELinux.
Solution 00000335 by David Wild at 2009-04-22T08:01:04.000+0000
In common with many applications, WX2 is not compatible with SELinux. Problems will occur if SELinux is installed and activated on any nodes.

For example, when running a wxserver command: 

/opt/kognitio/wx2/current/bin/../software/Linux/wxserver: error while loading shared libraries: /opt/kognitio/wx2/current/bin/../lib/Linux/libgputils_t.so: cannot restore segment prot after reloc: Permission denied

To test for SELinux being the cause deactivate it using...

/usr/sbin/setenforce 0

...and run the command which originally failed again.

To make the change persistent across reboots add the line
 
SELINUX=disabled
 
to the file 
 
/etc/sysconfig/selinux

and restart the host.
FAQ: troubleshooting WX2 install problems
Solution 00000336 by Kognitio Support at 2012-07-20T11:48:48.000+0000


First of all, look here for a checklist of steps to take with non-appliance installs.



1) After installation, each node is running an smd but they appear as a set of 1 node systems rather than 1 system with all nodes in.

Check for communication problems between the nodes. In particular, check for firewalls preventing the nodes communicating.

On RH you can dynamically disable the firewall with "service iptables stop" as root, although this does not persist across node reboots.

2) I get problems when running command line tools, such as wxserver or trying to start the debug server. Typical output:

/opt/kognitio/wx2/current/bin/../software/Linux/wxserver: error while loading shared libraries: /opt/kognitio/wx2/current/bin/../lib/Linux/libgputils_t.so: cannot restore segment prot after reloc: Permission denied

It is likely that SELinux is installed and enabled - see the solution on SELinux which explains how to disable it dynamically and persistently.

3) I haven't got enough disk space.

Ensure swap is off. WX2 is an in-memory database, so swap should not be used. Typical systems may be setup with swap matching the amount of RAM, so may be eating into disk space which is required for something else.

4) I'm having problems synchronising files between nodes.

Ensure that the nodes have their time synchronised - if the clock skew between nodes gets too great, files will not be synchronised correctly as determining the 'newest' version of a file can become difficult.

5) I seem to have a lot of RAM stores on my system, and the size of each is much smaller than usual.

Do "wxprobe -a '{can DB}' -HNF" to see the different types of WX2 nodes. It is likely that the amount of RAM per node differs somewhat, and this is why the configuration process is using lots of smaller RAM stores to avoid wasting RAM on nodes. To use larger RAM stores you will need to make the nodes have more similar amounts of RAM, ideally identical.

6) I've specified files for WX2 to use for disk resources. The commissioning step hangs saying "Initialising disks".

The files specified need to be accessible as wxroot, so ensure this is the case. For example, if you have specified one or more files in /data ensure they either already exist with appropriate permissions, or that wxroot has permission to create them in /data if they do not yet exist. Note that the "Initialising disks" phase can take a long time anyway, and you can find out how to monitor progress here.


 


7) My WX2 installation fails and the output file in the current smd directory contains the message "libgcc_s.so.1 must be installed for pthread_cancel to work" 


 


Install the 32-bit libgcc package and retry the newsys operation





8) My WX2 installation fails and the output file in the current smd 
directory contains the message "error while loading shared libraries: libz.so.1: cannot open shared object file: 
No such file or directory"


Install the 32-bit libz package and retry the newsys operation

 

FAQ: How do I recover if Kognitio is down?
Solution 00000350 by Simon Darkin at 2015-07-03T05:45:09.000+0000

First of all, it is important to verify whether the server is down, or whether there is a connectivity issue. To do this, follow the instructions here.
 
In the event of needing to restart the Kognitio system as the above tests indicate the problem is not related to connectivity or an administration task, you can optionally collect information to allow the problem to be investigated by Kognitio in future, then restart the system.

Note that the system can be setup to automatically detect hardware and software faults, capture diagnostic information, and restart - speak to your account manager if you would like assistance with this, or email helpdesk@kognitio.com for more information.

To collect information to allow the problem to be investigated, and restart the database

Details of how to collect information are in the attached document - please follow 
those instructions to give Kognitio the best chance of identifying the 
root cause of your problem. The document also has information on how to restart the database.

Attachment: Capturing debug information.pdf,
FAQ: How do I access the RAM and Disk monitoring facilities in Control Tower
Solution 00000351 by Simon Darkin at 2009-07-23T16:18:34.000+0000
A WX2 user with default privileges is not able to access all of the ram and disk monitoring facilities within Control Tower.   The group 'GRP_MONITOR' has all of the necessary privileges to do this.

 

GRP_MONITOR has select privileges on:

 

IPE_DISPLAY

IPE_BOOT

IPE_PROCESS

IPE_DISK

IPE_XOR_CLUSTER

IPE_XOR_ELEMENT

IPE_IDLE



You can either add a user directly to GRP_MONITOR with:


alter group GRP_MONITOR add user &lt;user&gt;;


or if you need these plus additional privileges then you may wish to create an admin group, add the user(s) to that group, then add the admin group to GRP_MONITOR as follows: 


create group GRP_ADMIN;

alter group GRP_ADMIN add user &lt;user&gt;;

alter group GRP_MONITOR add group GRP_ADMIN;


Now you can add other privileges to GRP_ADMIN as appropriate.



REVOKING PRIVILEGES

If you need to remove a user from a group you can achieve this with:


alter group &lt;GROUP&gt; drop user &lt;USER&gt;;
FAQ: How do I give an AP node full DB capabilities?
Solution 00000353 by Simon Darkin at 2009-07-27T13:08:57.000+0000
An AP can be given full database capabilities, for example when it is required to act as a replacement for a failed DB node.   Note this solution focuses on changing the AP node's capabilities and does not discuss the process of incorporating into the system a DB node previously configured as an AP.  


1) INITIAL CHECKS

Before you start, ensure that consideration has been given as to how the AP node will host a WX2 disk resource. This will be either in the form of a raw partion of type 0x60 or a dedicated file that resides in a linux partiton. 

If an AP node is currently being used for other activities that require a significant amount of system resources then it is NOT an ideal candidate for use as a DB node, as the database performance will be impacted by these other activities. 

Compare the file "/proc/cpuinfo" on the AP node with one of the DB nodes to ensure the CPUs are identical.  This is not essential but does removes the risk of introducing a node that has lower performing CPU than existing database nodes.

Make sure the AP node has the same amount of ram available as the DB nodes to avoid the risk of ending up with lots of small ramstores.  In the example output below we can see that AP node wx2-f0v6r6e5b1 has the same amount of ram as the other four nodes in the system. 

&gt; wxprobe -HN
Kognitio WX2 Hardware Discovery Tool v6.01.07-q on wx2-system1
(c)Copyright Kognitio Ltd 2001-2008.

1 node like this one:
  bay 1: ID wx2-f0v6r6e5b1, Status Up
         sys Linux-2.6.16.54-0.2.5.PTF.283002.0-smp, Disks 1, links 4
         ram 16671453184, mrpp 3921674240, swap 0, cpus 4, end L.
         Caps DBG-MASTER.

4 nodes like this one:
  bay 2: ID wx2-f0v6r6e5b2, Status Up
         sys Linux-2.6.16.54-0.2.5.PTF.283002.0-smp, Disks 1, links 4
         ram 16671453184, mrpp 3921674240, swap 0, cpus 4, end L.
         Caps DB-IO-DS-RS-MI-DBG.




2) ADDING FULL DATABASE CAPABILITIES TO THE AP NODE 

Connect to the AP node, stop the database and the SMD

&gt; ssh wxadmin@
&gt; wxserver stop
&gt; wxsvc stop 

As root edit the file /opt/kognitio/wx2/etc/local_config and replace the entry "nodeclass=cp" with "nodeclass=full" in the [attributes] section, e.g.

[attributes]
nodeclass=full

Restart the SMD and check that the new node has the "db" capability.

&gt; wxsvc start 

&gt; wxprobe -a '{wx2-f0v6r6e5b1}' -HC
Kognitio WX2 Hardware Discovery Tool v6.01.07-q on wx2-system1
(c)Copyright Kognitio Ltd 2001-2008.

1 node with capability db (DB).
1 node with capability db_io (IO).
1 node with capability db_ds (DS).
1 node with capability db_rs (RS).
0 nodes with capability daemon (PBD).
0 nodes with capability mpkd (MPKD).


Diskspace permitting this node is now ready to be configured with a WX2 disk resource which could be in the form of a raw partion of type 0x60 or a file resource on a linux filesystem.
FAQ: Investigating performance issues (locking, queueing, poor queries, ...)
Solution 00000356 by Simon Darkin at 2014-11-03T14:55:21.000+0000

From time to time you may find that queries are taking longer to run than you expect.  There are a number of factors that can affect query times, some of which are easily investigated.


1) Locking Issues


A query that appears to have been running a long time may actually be in a wait state pending a lock.  Locks are required to ensure inter-query integrity. The default lock timeout is 24 hours, so if a session holds a lock for a prolonged period (possibly as a result of having completed a command but not committed the transaction) it can make other queries with conflicting lock requirements hang. The lock timeout period can be changed by setting the lkti parameter in the runtime parameters section of the config file. The value of 
lkti specifies the lock timeout period in seconds.


IPE_ALLLOCKS allows users to see all the granted and pending locks in the system, and the view IPE_LOCKS lets users see portions of IPE_ALLLOCKS based on the privileges they have.

To investigate further you can need to identify the queries corresponding to the sessions/transactions seen in the lock output. The best way to do this is by looking in IPE_ALLCUR_TRANS, or the corresponding view IPE_CURTRANS. This will show the currently open transactions (remember that a client can run all the queries it wants, but then not end its transaction which will leave it holding all its locks and other resources).

If the commands have completed but their transactions have not been committed, they will still be holding locks as explained above. Uncommitted transactions will appear in IPE_TRANSACTION as a single row with an operation value of 2 denoting the transaction has started.

Kognitio Console has a "Blocking Locks Report" under View | Reports, which allows the current user to see which user queries, if any, are holding locks preventing the current user's queries from running.

In addition, the following query can be run as SYS to show all locks held which currently have a corresponding PENDING lock (i.e. these locks are preventing other queries from making progress):

select session_id, type, table_id, row_hash, s.user_id, s.client_name, s.last_command 
from sys.ipe_alllocks l, sys.ipe_allcursessions s, sys.ipe_allcurtrans t 
where l.session_id = s.session 
and t.session = l.session_id
and status = 'GRANTED' 
and table_id in (select table_id from sys.ipe_alllocks where status = 'PENDING')
and command_running = 0 
order by 1,3,4,2


Post-mortem diagnosis of locking issues is possible by querying IPE_ERRORLOG which will contain an entry like the following for a lock timeout:


Lock Manager: session 17201 transaction 16928 requested SH TAB on table 1095. Conflicts with session 17200 transaction 16927 EX ROW on table 1095, row hash 548587615


It is also possible to review `wxlogd smd`/locking* to see historical evidence of when a query waited on a pending lock for more than da_dump_lkti seconds (default 600), which results in output like this:

02-06_17:57:23_BST: DA: DUMPING LOCKS: Locks held for longer than da_dump_lkti
02-06_17:57:23_BST: DA: Locks granted:
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash 1173333716
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash -1277331839
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash 670452605
...
02-06_17:57:23_BST: DA: Session 26597 TNo  67489 type EX ROW table      1 row hash -789570782
...
02-06_17:57:23_BST: DA: Session 26597 TNo  67489 type SH ROW table      0 row hash 1385381117
02-06_17:57:23_BST: DA: Locks pending:
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table     12 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table      0 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table      1 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table     27 row hash -2147483648

In the case above you can see that the EX ROW (exclusive row) lock held on table 1 by tno 67489 is blocking the pending lock requests from tno 67494.

2) Queuing


Kognitio uses queues to restrict the number of concurrent queries running, so a query may not necessarily run as soon as it is submitted, it could remain queued for a while.

You can look in virtual tables like IPE_QUERY_QUEUES and IPE_QUERY_QUEUE_STATS to see how queues are operating.

You can also see from IPE_ALLCURSESSIONS whether commands are queuing (status is 0 if idle, 1 for running, 2 for transaction rollback, 3 for statement rollback, 4 for queued, 5 for being compiled).


In the example output below we can see that tno 17084 under session 17200 is in queue 100001 and has been queued for 481 seconds.  The queue has two queries running (NRUNNING=2), is only permitted to run two queries simultaneously (MAXRUN=2) and has one query waiting (NWAITING=1).  The final query against IPE_ALLCURSESSIONS shows which query under session 17200 is actually queued.


select 
queue,
pos,
priority,
user_id,
session,
tno,
running,
secs_queued

from sys.ipe_query_queues

where running = 0

Query 3

 QUEUE|POS|PRIORITY|USER_ID|SESSION|  TNO|RUNNING|SECS_QUEUED

100001|  0|     100|      9|  17200|17084|      0|        481

Query 3   1 Row(s) Retrieved.    -----  0:00.2  0:00.2  0:00.4    


select 
queue, 
len,
nrunning,
nwaiting,
nbarriers,
maxrun,
maxwait
from sys.ipe_query_queue_stats

where nwaiting = 1

Query 4

 QUEUE|LEN|NRUNNING|NWAITING|NBARRIERS|MAXRUN|MAXWAIT

100001|  1|       2|       1|        0|     2|      0

Query 4   1 Row(s) Retrieved.    -----  0:00.1  0:00.1  0:00.3    


select 
session, 
user_id,  
command_running,
client_name,
net_address,
last_command

from sys.ipe_allcursessions

where command_running = 4

order by session

Query 5

SESSION|USER_ID|COMMAND_RUNNING|CLIENT_NAME|NET_ADDRESS   |LAST_COMMAND

  17200|SD1    |              4|lego       |193.35.206.228|DECLARE SQL_CUR38B0928 CURSOR FOR select count(*) from t2 FOR READ ONLY

Query 5   1 Row(s) Retrieved.    -----  0:00.1  0:00.1  0:00.4    


The Kognitio guide provides more information on how to configure queues.



3) No statistics gathered on objects

To allow the query optimiser to choose the best plan, it needs to have statistics on the underlying objects in a query. Typically you would collect these with "update statistics for &lt;objectname&gt;" whilst &lt;objectname had a RAM image. This allows information on e.g. the cardinality of the table, and the selectivity of individual columns to be gathered. As a result, the optimiser can better determine things like the optimal order for joining objects.


4) Skewing queries

The database will always 
distribute data to allow each memory-scanning process (Ram Store, or RS 
for short) to deal with a portion of the whole query. All the RS 
processes will do this in parallel. If the data is distributed in a 
skewed fashion, such that a small number of RS processes need to do more
 than their fair share of processing, then the query time will be 
increased. In extreme situations, one RS might do almost all the 
processing for one or more steps of the query.

One way to 
identify this is to run "wxtop" from the Linux command line on one of 
the database nodes whilst the query is running - this shows how busy all
 the databases processes are, so if a small subset of the RS processes 
are very busy, that indicates skew is causing a performance issue.

To
 rectify this, review the query to see how skew is introduced. Most 
commonly this is by hashing on an attribute which has a very skewed 
distribution (e.g. hashing on gender for customers, or on an attribute 
which has a special value (often 0) when no valid information is 
available. Then either use partial hashing (see documentation) to remove
 the skew, or remove the skewing attributes from the query at an earlier
 stage if they are not required in the final result set (often this is 
the case).


5) General server activity


You can use the wxtop command-line utility as the wxadmin
 user to see how busy the Kognitio processes in the system are. It is 
similar to the linux 'top' command, except that wxtop will show all 
Kognitio processes across the system rather than just on one node.  


If wxtop shows a lot of activity on lots of nodes, the server is
 clearly busy and this will impact on performance of any other queries 
you run.  


If wxtop shows a small subset of the nodes are busy, there is 
likely to be some skewing in data causing the problem. You may be able 
to determine which objects are causing skew by looking in 
IPE_ALLRAM_IMAGES.  Common causes of skew are hashing on a skewed 
attribute, or running window functions with skewed partitions. Search 
the solutions for "skew" for further information.



6) Missing join conditions.


Kognitio by default will allow you to run a query against multiple tables without specifying a join condition.  This means the server will perform a cartesian join i.e. one where every row from one table is joined to every row from another table, should a join condition be missing.  Cartesian joins are typically expensive in performance terms, even on moderately sized tables. 


You can run "explain &lt;query&gt;" and then check the output for references to cartesian join to see if one will be used.


You can also guard against inadvertent cartesian joins by setting a parameter which will cause an error if the compiler detects that a cartesian join is required.  To do this run the following command in the current session:


set current_session parameter error_on_theta to 1;


Alternatively set the parameter on a per-user or system wide basis, the latter by running the following command as SYS:


set parameter error_on_theta to 1;



7) Fragmented tables on disk.


If scanning a relatively small disk-based table takes a lot longer than expected, this is often because the rows for this table are interleaved on disk with data from other tables. This is especially common for status tables which have ETL status recorded in them - by their nature they tend to be interleaved with a lot of data that the ETL process writes to disk.

To confirm data interleaving is the problem, do "create table test disk from select * from &lt;table that is slow to query&gt;", then "select count(*) from test" - if the select is a lot quicker than querying the original table, then fragmentation is the issue.

To resolve the issue, either create a RAM image of the fragmented table (usually the best option as these tables tend to be small), and/or defragment it by doing "alter table &lt;x&gt; set slabs to &lt;slab list&gt; migrate defrag"


8) inserts to a table with a foreign key are slow

inserts to a table containing a foreign key are slow, as the referential integrity check is done by pulling the rows to be inserted into the interpreter process, then checking for the foreign key's presence in the relevant table, then inserting the row and moving on to the next row. To avoid this, do not define tables with foreign references.


9) CREATE TABLE IMAGE commands are slow and make the server very unresponsive

Problems related to CREATE TABLE IMAGE performance are often related to UNIQUE / PRIMARY KEY definitions - the issues involved and their resolution are explained here.


10) Suspected hanging queries

If wxtop shows no activity it is likely that your query has hung in some other way. Assuming it is not a locking issue (see above), the cause of the hang can probably be deduced by looking at IPE_ALLRAM_ACCESS, and in particular ordering on request_id then looking for nodes which appear to be in an unusual state.  


The states include:

TWait - waiting to transmit data

SWait - waiting to scan data - i.e. waiting to receive it

PWait - waiting to project data - associated with row retrieval projection of data


You can also use IPE_ALLRAM_ACCESS to determine whether the query is
 making progress - e.g. "select sum(records_inserted) from 
ipe_allram_access where sesssion = &lt;your session id determined from 
ipe_allcursessions or wxsqlhist&gt;", repeat several times to check that
 the number of records inserted has changed.

If a query appears to hang, exporting the contents of IPE_ALLRAM_IMAGES, IPE_ALLRAM_ACCESS, IPE_DISK_ACCESS, and IPE_ALLCURSESSIONS, and attaching that information to a case allows Kognitio support to investigate the problem further.


11) Kognitio Console is slow to connect or when opening menus

There are a number of configuration settings which can be tweaked to speed up Kognitio Console performance against heavily loaded systems, systems with a lot of metadata (i.e. lots of tables/columns/users), or systems with high latency / low bandwidth between the Kognitio server and the Console machine. For a summary of these, see here.


12) None of the above

If a query is running slowly, and none of the above information helps improve performance, please raise a case containing as much relevant information as possible. This should include at least the following:
information on whether this is a query which is performing worse than before, or is a new query which you are trying to make run faster
diagnostic output for the query - typically obtained by running the query prefixed with the word diagnose e.g. "diagnose select ...". If the diagnostic output is too big to be pasted into a case it can be included as an attachment.
evidence of whether the server is busy whilst the query is running - e.g. output from doing "wxtop" from the command line on one of the database nodes.
information on any changes since the query last ran quickly (if appropriate). This would include:
extra concurrent activity on the server now
any data changes since the query last ran quickly
changes to imaging of underlying objects
extra imaging of other objects
server software version changes
information on changes to the Kognitio hardware platform
FAQ: What is the optimal number of streams for a parallel import
Solution 00000358 by Simon Darkin at 2009-08-18T10:16:18.000+0000
The main factors that affect wxpimport are CPU and network bandwidth, not disk count. By default, wxpimport sets -M to core count-1, but this is not currently factored out across all the nodes, so you may end up heavily loading the first couple of nodes, leaving the rest idle. 

A better rule of thumb is to assume -N 16 as a maximum and then choose -M to evenly distribute the threads across the available nodes, hence -M 4 for a four node system. Using -N &gt;16 does have a benefit, but it rapidly diminishes at that point. 

In addition, -M should always be at least one less than the core count, so in the four node example, if there are only four cores per node, the maximum for -M is 3, in which case -N 12 is the maximum. 

You can use wxprobe -wF to check how many cores exist on each node.  Each core is listed as "CPU ".  The following example output shows four nodes with four cores per node.

wxprobe -a '{can DB}' -wF | egrep '(bay|CPU)';

        bay 1: ID bl-f0v2r3e2b1, Status Up
            CPU 0: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 1: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 2: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 3: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
        bay 2: ID bl-f0v2r3e2b2, Status Up
            CPU 0: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 1: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 2: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 3: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
        bay 3: ID bl-f0v2r3e2b3, Status Up
            CPU 0: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 1: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 2: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 3: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
        bay 4: ID bl-f0v2r3e2b4, Status Up
            CPU 0: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 1: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 2: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
            CPU 3: MHz 2200, type Dual-Core AMD Opteron(tm) Processor 2214 HE.
FAQ: Why are user parameters within WX2 not documented?
Solution 00000370 by Simon Darkin at 2012-07-20T12:32:22.000+0000
Users should rarely have to set parameters in normal use as they tend to be provided in situations where short term workarounds are required, for example to improve problem query plans or help investigate problems.   


There are many parameters in existence, and each new version of code may include new ones, omit previous ones and even include changes to default values, so for these reasons they are excluded from the WX2 documentation.
FAQ: Common problems with non-appliance installs
Solution 00000474 by Simon Darkin at 2018-09-21T07:54:28.000+0000
A Kognitio appliance install starts with a series of tried and tested RDP jobs to ensure the system is configured in a consistent way. A Non-appliance install does not go through this strict configuration process and therefore a series of checks should be performed on ALL nodes before installing Kognitio software. 

1) Ensure the Linux firewall is not preventing communication between Kognitio nodes, or Kognitio server and clients

On RedHat the firewall can normally be permanently disabled as root with: 

/sbin/service iptables save 
/sbin/service iptables stop 
/sbin/chkconfig iptables off 

Check the firewall has been disabled with: 

/sbin/service iptables status 
Firewall is stopped. 



On SuSE check the status with:



/sbin/rcSuSEfirewall2 status

Checking the status of SuSEfirewall2                                                                        unused



If in doubt, discuss with the IT team responsible for the kit to ensure there is nothing preventing communication between the nodes. Note that being able to ping nodes does not indicate there is no problem here!




2) If the server is running Red Hat then check version 5.4 or higher is installed to avoid encountering a bug in realloc that exists in older versions. 

cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 5.4 (Tikanga) 


3) Ensure that kdump is disabled on DB nodes so that the memory available to WX2 is consistent across all these nodes 

Permanently disable kdump with: 

/sbin/service kdump stop 

Check kdump is disabled with: 

/sbin/service kdump status 
Kdump is not operational 


4) Ensure swap is disabled 

This is a recommendation rather than a requirement (swap space doesn't really do any harm, it just doesn't get used effectively).





You can permanently disable swap with: 

swapoff -a 
comment out (with a #) or remove the 'swap' line or lines from /etc/fstab 

Run 'free' and check that swap is set to 0 

free 
total used free shared buffers cached 
Mem: 37042356 362352 36680004 0 62596 96212 
-/+ buffers/cache: 203544 36838812 
Swap: 0 0 0 


5) Check that there is sufficient disk space for Kognitio disk resources. 

If the customer is using a server with multiple disks, for example a DL360, then it might be appropriate to create a mirrored pair out of the first two disks on which a small linux partition containing the OS can be installed, leaving enough space for a large partition to act as a Kognitio disk resource. The remaining disks can then be configured with identically sized partitions for further disk resources. 
The mirrored pair may have a different cylinder size to the individual disks resulting in a small size difference in the partitions, perhaps a few MB. This is ok. 

Ensure that any type 0x60 partitions are created OUTSIDE of an LVM 

Note that previous experience of cloning software has shown that the clone doesn't always end up with the same size 0x60 partitions as the server from which the image was taken, therefore it would be better to create the 0x60 partitions after the cloning process. It's a good idea to create the Kognitio partitions via an RDP job, if that's not possible then create them manually via fdisk on small systems or using a simple shell script taking on larger system being careful not to delete existing partitions. 
The script will need to be tailored for the system in hand but the following example demonstrates how you might create a single 290G partition on a mirrored pair of disks followed by a 135G partition on 6 individual disks. 


#!/bin/sh
#exchange the final 'p' for a 'w' when you're happy with the output
outfile=/opt/kognitio/wx2/etc/mkpart.out
rm -rf $outfile


(
echo -ne "n\np\n3\n\n+290G\n"
echo -ne "t\n3\n60\np\n"
) | /sbin/fdisk /dev/cciss/c0d0 &gt;&gt; $outfile 2&gt;&amp;1


 


for i in `seq 1 6`; do
( echo -ne "n\np\n1\n\n+290G\n"
echo -ne "t\n60\np\n"
) | /sbin/fdisk /dev/cciss/c0d${i} &gt;&gt; $outfile 2&gt;&amp;1
done






6) Check for the existence of 32 and 64 bit libraries to ensure both environments are supported 

ls /usr/lib64 | wc -l 
811 

ls /usr/lib | wc -l 
478 





If RedHat double check that 32 bit versions of libgcc and libz are installed:


 


find . -name libz.so* -print


find . -name libgcc*.so* -print


 


Expect to find these in /lib or /usr/lib




Other libraries that are required include libc6 (the installer will not run without this). You can use ldd as described at the end of this section to identify any missing libraries for a particular library.




If a library is missing and you are on a 64 bit version of Linux, check the 64bit version being used (example is for libgcc, same steps required for libz):


rpm -q --whatprovides /lib64/libgcc_s-[Tab to complete filename]





Now download and install the 32 bit version - will probably end .i386 rather than x86_64





If libgcc is not present, newsys will crash with Option6 error in misc node, check with:


grep libgcc `wxlogd smd`/output*





If no libz, newsys will also fail and evidence will be in the same smd output file:




grep libz `wxlogd smd`/output*



To check the relevant libraries for a particular binary (including the installer) exist, use ldd &lt;binary&gt;. If this has any lines like the following, the relevant libraries must be installed:

      libc.so.6 =&gt; not found




7) Check that the required ethernet devices are configured to come up on boot (ONBOOT=yes) once the config files have been populated. 

grep -i onboot /etc/sysconfig/network-scripts/ifcfg-eth* 
ifcfg-eth0:ONBOOT=yes 
ifcfg-eth1:ONBOOT=yes 
ifcfg-eth2:ONBOOT=yes 
ifcfg-eth3:ONBOOT=no 
ifcfg-lo:ONBOOT=yes 


8) Check that the network interfaces enumerate correctly. 

Ports can be assigned different names after a reboot, and so it's strongly advisable to create a renaming script that runs at boot time to ensure they are named in a consistent way. 
The example script attached to this solution assigns a name based on the bus id, essentially a physical location and so as long as all servers in the system are setup in an identical way you will have a consistent naming scheme. 
This method will also cope with a network card replacement without the need to edit config files as long as the replacement card is fitted into the same slot as the card being replaced. 

Copy the attached script "wx_dev_renamer" to /etc/init.d and make it executable 

Check the port order with: 

/etc/init/.d/wx_dev_renamer test 

Add the script as a service with: 

chkconfig --add wx_dev_renamer 

Check the run levels with: 

chkconfig --list wxx_dev_renamer 

Reboot the server and run /sbin/ifconfig -a to check that the interfaces have been assigned the correct names. 


9) The customer should ensure IP addresses assigned to Kognitio nodes do not exist anywhere else on the network 


10) If the system is going to use 10G networking check for the existence of a 10GbE adapter. 





Note SUSE may not support 10G cards without first installing the manufacturer's drivers. Red Hat does support certain 10G cards out of the box however these aren't always the recommended drivers and so it is best to check the manufacturers website. 

/sbin/lspci 
--snip-- 
04:00.0 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10-Gigabit Server Adapter (rev 42) 
04:00.1 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10-Gigabit Server Adapter (rev 42) 


11) Ensure firmware and drivers versions are up-to-date on 10G cards. 

Previous experience with the NetXen NC522SFP 10G card has shown that it can hang under load, evidenced by the following type of entries in /var/log/messages 

Feb 15 17:42:18 &lt;hostname&gt; kernel: NETDEV WATCHDOG: eth0: transmit timed out 
Feb 15 17:42:18 &lt;hostname&gt; kernel: netxen_nic eth0: transmit timeout, resetting. 
Feb 15 17:42:19 &lt;hostname&gt; kernel: NETDEV WATCHDOG: eth1: transmit timed out 
Feb 15 17:42:19 &lt;hostname&gt; kernel: netxen_nic eth1: transmit timeout, resetting. 

Typically you won't be able to ping the interfaces once the card has hung in this way. 

In the case of the NetXen NC522SFP upgrade the firmware and driver to version 4.0.516 or later. It is also advisable to use the nx_nic driver as recommended by and available from HP as opposed to the netxen_nic driver that comes with RedHat. 

You can check the current driver/firmware versions with: 

ethtool -i eth0 
driver: nx_nic 
version: 4.0.520 
firmware-version: 4.0.520 
bus-info: 0000:04:00.0 


Some firmware upgrades require that the eprom is flashed for the change to persist across boots. 


12) Ensure the wxadmin and wxroot users do not exist prior to installing the Kognitio software 

Check for existence of the wxadmin and wxroot user with: 

egrep '(wxadmin|wxroot)' /etc/passwd 

If these accounts already exist then ask the customer to remove them before installing Kognitio software. 


13) Ensure the BIOS is up-to-date 

HP have identified a bug in older versions of the BIOS running on DL360 servers (and some others) that can lead to uncorrectable memory errors being reported resulting in a server reset. Ensure the BIOS is at revision "2010.01.13 (1 Feb 2010)" or later to avoid the bug. 

Some HP systems include a cli utility that comes as part of the HP PSP which can be used to gathered useful information including the status of memory modules and even the IML logs. This is useful when reviewing memory related issues. 

/sbin/hpasmcli -s "SHOW DIMM" 
/sbin/hpasmcli -s "SHOW IML" 


14) Check network link speeds 

Once Kognitio software is installed check the network link speeds to ensure that they are all negotiating at the expected rate. We have seen incidents where at least one link was running at a degraded rate resulting in a significant drop in overall performance. Check the speeds with: 

wxtool -S 'ethtool eth0 | grep Speed' 


15) Change the IO scheduler if there are issues with concurrency during heavy disk IO activity. 

If heavy disk writes such as insert-selects or repack operations are hampering concurrency then there is a good chance the system is configured with the cfq IO scheduler rather than noop. Check which scheduler is active across all the DB nodes with: 

wxtool -a '{can DB}' -S 'cat /sys/block/cciss*/queue/scheduler' 

The active scheduler will be bracketed so if cfq is enabled you will see: 

noop anticipatory deadline [cfq] 

If cfq is enabled then try switching to noop so that IO is performed in the order that Kognitio requests and is not re-ordered by the scheduler. 

You will need to edit the schedule file for each disk device, the following example demonstrates how to do this for servers with multiple devices, in this case named c0d0 through to c0d6 

wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d0/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt;; /sys/block/cciss\!c0d1/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d2/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d3/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d4/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d5/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d6/queue/scheduler' 

Check the change has occurred on all disk with: 

wxtool -a '{can DB}' -S 'cat /sys/block/cciss*/queue/scheduler' 

You should now see noop encased in square brackets 

[noop] anticipatory deadline cfq 

To make the change permanent append the string "elevator=noop" to the end of the kernel entry in /boot/grub/grub.conf on all DB nodes. The exact line to be edited will depend on the kernel version and boot options but this is an example of what you'll need to change: 

kernel /boot/vmlinuz-2.6.18-164.11.1.el5 ro root=LABEL=/ rhgb quiet 

... is changed to ... 

kernel /boot/vmlinuz-2.6.18-164.11.1.el5 ro root=LABEL=/ rhgb quiet elevator=noop 

...then after a reboot check the scheduler files all show noop is being used 


For more information on changing the scheduler see http://www.redhat.com/docs/wp/performancetuning/iotuning/iosubsystem-scheduler-selection.html 

IMPORTANT: If you find that the scheduler settings are lost following a reboot then: 

* run dmesg to see which scheduler is flagged as the default 
* run 'cat /proc/cmdline' to see what the actual kernel command line is 

In all likelihood you'll find that cfq is still set to the default, and the kernel command line won't include the noop reference. Possible reasons include: 

* System is booting with LILO and not GRUB 
* The active partition from which the server boots (as denoted by an asterisk in the output from 'fdisk -l boot-device-name') has not been mounted to a directory. If it hasn't then almost certainly /boot/grub/grub.conf is not being used by the boot loader, instead it will use the file on the unmounted active partition which of course will not be available to view/edit. This situation occurred on a customer system and the recommended actions were: 

* rename /boot to /boot_old 
* make a new directory /boot 
* mount the partition /dev/cciss/c0d0p1 to the directory /boot and ensure /etc/fstab is updated 
* compare the contents of /boot with /boot_old copying across missing or out-of-date files as necessary 
* reboot the server and check all is ok 
* remove /boot_old 
* edit /boot/grub/grub.conf and add in the noop entry as described earlier 


16) Pre-existing issues with jexec 

There may be occasions where a jexec file is broken in some way which will prevent the Kognitio install process from setting up the wxsvc service. The install error might look something like this... 

Installed OK. 

Setting current pointer /opt/kognitio/wx2/current-&gt;ver70102k. 
Writing out system configuration. 
insserv: script jexec is broken: incomplete LSB comment. 
insserv: missing `Required-Stop:' entry: please add even if empty. 
--snip-- 

In this example the jexec file is missing the "Required-Stop" for a pre-existing service. The "Required-Stop" entry tells the system in what order services should be shutdown when certain system states are entered. 

If you look at our wxsvc service you can see that we want to start the system when the system is in states 2,3,5 and stop the service when it's in states 0,1,6 

poc1-wx2-rack2-enc2-3:/etc/init.d # cat /etc/init.d/wxsvc 
#!/bin/bash 

# chkconfig: 2345 99 10 
# description: Kognitio WX2 database server 

### BEGIN INIT INFO 
# Provides: wxsvc 
# Required-Start: $network 
# Required-Stop: 
# Default-Start: 2 3 5 
# Default-Stop: 0 1 6 
# Description: WX2 database server services 
### END INIT INFO 

exec /opt/kognitio/wx2/current/bin/wxsvc $* 

On a normal system you can see that the services have been setup in /etc/rc.d at the various run levels defined in the wxsvc services script. 

poc1-wx2-rack2-enc2-3:/etc/rc.d # ls -l /etc/rc.d/rc*.d/*wxsvc* 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc2.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc2.d/S06wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc3.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc3.d/S06wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc5.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc5.d/S06wxsvc -&gt; ../wxsvc 

If as in our example a pre-existing service is not setup correclty then it is really something for the system owner to address, but if circumstances dictate that we ourselves workaround the issue then the following steps (run as root) should help:- 

1. Add the missing stop lines to jexec 
2. Run "inserv" to setup the services in /etc/rc.d and check that they have created ok 
3. Remove the stop lines added in step 1 
4. Run "services wxsvc start" 

The last command should start the SMD and create the smd.* directory in the /var/log/wx2/logs-systemid area. 

The other alternative is to avoid doing the steps above and just run "wxsvc start" as the wxadmin user, but if the node is ever restarted then the SMD won't automatically restart and so is not ideal.








17) If using a SAN for Kognitio disk resources, ensure that the config files for each node limit that node to the set of LUNs that the node should be controlling. 





For example, use "wxviconf -l" on each node and set "partitions=/dev/mapper/x,/dev/mapper/y, ...". If all nodes have common names for the partitions, this operations can be done globally instead of on a node by node basis. If sets of nodes have common LUN names (even though these refer to different LUNs within the SAN), you can use "wxviconf -L" to mass edit the local configuration file on sets which should have identical "partitions=" settings.



 


18) Check that the 'connection tracker' is not running


 


There is a part of Linux called the 'connection tracker' which attempts to match up messages with responses.  It gets used for a number of things including firewalling and NAT routing.  


 


The connection tracker is a kernel module so you can find it like this:


 


[root@hostname ~]# lsmod | grep conntrack
ip_conntrack_netbios_ns    36032  0
ip_conntrack                     91621  3 iptable_nat,ip_nat,ip_conntrack_netbios_ns
nfnetlink                            40457  2 ip_nat,ip_conntrack



The example output tells you that the connection tracker is using the netfilter core. Netfilter is the linux kernel module that does firewalling which tells you the connection tracker probably got enabled by the firewall.  


 


You can also check for it by looking for the files it creates:


 


[root@hostname ~]# cat /proc/sys/net/ipv4/netfilter/ip_conntrack_count
106
[root@hostname ~]# cat /proc/sys/net/ipv4/netfilter/ip_conntrack_max
65536


 


These are present if the connection tracker is running and not if it isn't.  To remove it you can try taking the module out:


 


rmmod ip_conntrack


 


Beware though, it may start up again if the node is rebooted in which case you will need to edit the service responsible and prevent it from doing so.  


 

*** Note that when the Bet365 systems were commissioned in September 2018 the connection tracker was found to be on after the checks had been completed and so either we forget to check, the string we check for needs to change, or Bet365 enabled it after the checks.  Beware of this for future commissioning work as connection tracker has a big impact on message passing performance and so needs to be disabled.




19) Ensure that localhost is defined in the /etc/hosts file on every node :-


 


      127.0.0.1       localhost 

Attachment: wx_dev_renamer.sh,
FAQ: How does WX2 message-passing work?
Solution 00000513 by Kognitio Support at 2010-07-13T13:35:45.000+0000
WX2 assumes uniform connectivity and every process directly sends messages to every other process, letting the network route them as it sees fit.  WX2 doesn't sense the available bandwidth between servers, and doesn't have any variant of SPF (shortest path first) to determine message routes.

The message passing uses UDP as a transport mechanism with flow controls similar to those found in the TCP protocol; WX2 acks messages and uses dynamically changing window sizes and timeout values (in response to duplicate frames/packet drops) to cope with changing network conditions. Timeouts and window sizes are maintained on a per output link basis and the server assumes that each node has an equal bandwidth to all others.

In other words WX2 counts the number of unacked packets sent down a link regardless of which node the packets were sent to.  Typically what this means is WX2 throttles comms to everywhere based on the bandwidth it can send to the most remote node. This isn't a problem because message destinations are quite uniformly distributed so WX2 doesn't ever want to send extra data between two processes just because they are near/local to one another.

Duplicate frames are detected, counted, acked and ignored but WX2 doesn't stop sending just because it has received the same message many times. If WX2 detects too many packet drops on a particular outbound link (e.g. it sent the same thing too often) then it stops using that link and it can do that either for a particular set of destinations or just completely stop using the link to send any packets depending on which packets were dropped.

WX2 does have a configurable MTU but does not use any PMTUD (path MTU discovery) equivalent technology.  WX2 sets the MTU to 9,000 all the time and sends approximately 9KB (usually a bit smaller) IP frames all the time.  If Linux and the networking layer can send jumbo frames then they do, otherwise WX2 relies on IP packet fragmentation as this gives the best performance.  If you don't want IP fragmentation to occur, you need to manually configure WX2 for a smaller MTU, which is easy to do by setting the following configuration file entry:

[mpk]
dgram_mtu=1400

FAQ: Kognitio troubleshooting
Solution 00000690 by Kognitio Support at 2015-07-21T09:53:14.000+0000

This solution is the starting point for troubleshooting issues before contacting Kognitio support. Click on the appropriate topic below to find out more information (note that this will open another browser window):

Commissioning issues with Kognitio - i.e. problems when creating a new Kognitio system from scratch.

Writing effective queries for Kognitio systems.

Performance issues with Kognitio.

Data loading issues.

How to collect information and restart if Kognitio is down.

Issues with upgrading software for the Kognitio server.

How to understand what the Kognitio system is doing (disk/RAM usage, current queries, etc) by querying virtual tables.

Understanding how Kognitio splits disk resources into slabs, how those slabs are filled, and how to deal with running out of disk space.

Information on backup.

Problems with Kognitio JDBC or ODBC connectivity

MDX connectivity issues.

For any other issues, or if the links above do not help you resolve the issue, please contact Kognitio support using the contact details on the support home page.
FAQ: ODBC issues
Solution 00000691 by Kognitio Support at 2014-11-27T14:25:57.000+0000

If you are having problems with the ODBC driver, please click on the relevant link below for further information.

Download the latest Windows installer for 32/64 bit, including the ODBC driver.

Download the latest client packages for Linux/Solaris, including the ODBC driver.

Download the latest JDBC bridge, with installation notes.

Download the latest JDBC client software, with installation notes.

Collect extra debug information from the WX2 ODBC driver

Information on using SSH keys with WX2

Associating multiple IP addresses with one WX2 server

Understanding autocommit behaviour which can lead to function sequence errors amongst other things.
FAQ: How can I use the ipe_alllocks virtual table?
Solution 00000692 by Kognitio Support at 2011-01-28T13:15:00.000+0000
ipe_alllocks contains information about all granted/pending locks in the system.

The relevant columns in ipe_alllocks are:

session_id: the session for this lock.
tno: the transaction for this lock.
type: the lock type - typically a shared (SH) or exclusive (EX) lock on a table (TAB), or row (ROW).
table_id:
 the table_id this lock applies to.
row_hash: a hash of the row this lock applies to (NULL if the lock is a table rather than row lock).
status: either GRANTED or PENDING.

Looking in ipe_allcur_trans and joining on tno will allow you to see the corresponding SQL, user_id, etc for any lock in ipe_alllocks.
FAQ: wxpimport troubleshooting
Solution 00000695 by Kognitio Support at 2011-03-18T13:36:24.000+0000

Setup

To use wxpimport for parallel import there are a couple of steps that must be taken:

1) ensure the linux user that will run wxpimport is in the wxadmin group, and that the smds have been restarted since the user was added to that group.

2) ensure the WX2 user that wxpimport will connect as has SELECT privilege on SYS.IPE_BOOT

wxpimport can only be run on a node running the smd daemon - so either a DB node or, more commonly, an AP.

Error messages - only x import streams can be established

If you see an error along the lines of:

ERROR 999:

8 parallel import streams specified but only 0 import streams can be established

ERROR END


... this is indicative that an earlier wxpimport did not end normally, and has left some temporary files lying around which prevent subsequent wxpimports from running.

To confirm this, run the following from the command line:

wxtool -S 'ls -ltrd /tmp/wxpi.flag*'

If this shows directories exist, the following command will remove them which should allow you to run parallel imports again:

wxtool -S 'rm -rf /tmp/wxpi.flag*'


FAQ: What to do if the Linux OOM killer kills WX2 processes
Solution 00000696 by Kognitio Support at 2013-07-23T13:01:24.000+0000

If the Linux Out Of Memory (OOM) killer kills any WX2 processes this will cause WX2 to halt. 

Typical symptoms include:
a WX2 watchdog process showing up in "wxprobe -s" output with no accompanying WXDB process (as that was the process that was killed)
entries in /var/log/messages showing the OOM killer was activated and that it killed wxdb processes. For example:

Feb 14 11:45:38 KOGNITIO1 kernel: wxdb invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0
Feb 14 11:47:00 KOGNITIO1 kernel: 
...
Feb 14 11:47:00 KOGNITIO1 kernel: Out of memory: Killed process 12345, UID 0, (wxdb).
Feb 14 11:47:00 KOGNITIO1 kernel: wxdb invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0
...
Feb 14 11:47:00 KOGNITIO1 kernel: Out of memory: Killed process 12367, UID 0, (wxdb).
...

The reason WXDB processes are likely to be killed in this situation is not because they are leaking memory, but because they reserve a lot of memory for their use - WX2 is, after all, an in-memory database.

This situation typically arises when running on a very small development system rather than a production, and/or if applications other than WX2 are running on the database nodes.

If the OOM killer is activated on your system, there are a couple of steps you could take:

1) ensure that you do not run any other applications on the node, so the whole node is reserved for WX2. This is how WX2 is designed to work, and is always how production machines run. However, sometimes with small development systems people try to add other applications onto the node. If you are running things on the node and can stop them, this step will be all you need.

2) if you aren't running any applications on the node, or you really have no alternative, then restrict the RAM for Kognitio by setting the memsize entry in the [system] section of the config file. For example, on a 16GB node you could set that to 8GB (50% of your RAM) as follows:

[system]
memsize=8000000000

This will mean there is less RAM available for WX2, but it will allow you to use more RAM for anything else running on the node.



FAQ: WX2 disk usage, and how to tell when disks or slabs fill up
Solution 00000702 by Kognitio Support at 2011-06-03T15:27:49.000+0000

WX2 disk slabs

WX2 divides disks up into slabs. Slab 1 is reserved for system table entries, slab 2 for logging table entries, and the remaining disk space is split into as few slabs as possible based on the maximum slabs size (default 16GB user data, but can be overridden at commission time with the [fs] max_slab_gb entry). Section 7.1 of the Configuration and Maintenance manual describes slabs in more detail.

By default a 60GB disk resource with no software RAID will have:
slab 1 0.5GB
slab 2 0.5GB
slab 3 16GB
slab 4 16GB
slab 5 16GB
slab 6 11GB (to use up the remaining space)

How slabs are filled with data

User data can go in any slab except slabs 1 and 2. With a newly commissioned system, the default is for all data to go to the highest numbered slab until that reaches a threshold of usage, then data will start to be written to the next highest numbered slab until that reaches the same threshold, and so on. When all slabs have reached the threshold, WX2 returns to writing to the highest numbered slab until it reaches the next threshold, and so on.

Note that all disks in the system are following this process independently.

When a record is deleted / updated, the old record is marked as deleted,
 but the space it occupies cannot be reused until either all the data is
 removed from that slab with repack / drop table / truncate table 
operations, or a global lock is obtained on the system and a reclaim run
 including that slab. Section 6.4 of the WX2 Guide explains reclaim and repack in more detail.

When a delete / update operation occurs, the delete marker must be written to the same slab as the data that is being deleted. Therefore, if a slab becomes completely full an error will be returned if an attempt is made to delete any of its rows. For this reason, a small amount of space is reserved for non-record use, so it should always be possible to delete some records from a slab.

When a record is added to disk by the system, a disk is chosen at random for that record - if that disk does not have enough space for the record in the set of slabs eligible for the table concerned, then the operation will fail - the record insert will never be forwarded onto another disk. For this reason, if a record cannot be added to one of the disks in the system the system is effectively full for the table concerned and space will need to be recovered or more slabs allocated for that table before more data can be added to it.

It is possible to restrict which user data slabs are available for general use, for a particular schema, or for a particular table. This can have benefits in terms of imposing disk quotas, and simplifying the recovery of disk space via repack operations.

How to tell how much free disk space there is

There are a number of virtual tables available to explain how much free disk space there is. IPE_DISK_SLAB shows how full each slab on each disk in the system is, and IPE_FTABLE shows how many rows each table has on each disk and slab.

In addition, the wxsubmit command line tool has a number of pre-defined queries which show information on disk usage. $f4 shows disk usage by slab, and $f6 shows disk usage by table, whilst $f8 shows slab allocations for tables. From 7.1.2w onwards, the $f4 query has been modified to show the minimum free space for a given slab across all the disks in the system (this is to deal with the case where one disk has filled a particular slab but the others still have space - as mentioned earlier, this effectively means that slab is full for the system).

The 7.1.2w $f4 query is as follows (note the last column shows the minimum free space on any disk for the given slab):

select
  case when su.slabid is null then 'Total  '
       else cast(to_char(su.slabid,'9999999') as varchar(7))
  end as "Slab ID",
  cast(to_char(su.usedgb+su.freegb,'999,990.9') as varchar(9)) as "Slab GB",
  cast(to_char(su.usedgb,'999,990.9') as varchar(9)) as "Used GB",
  cast(to_char(su.freegb,'999,990.9') as varchar(9)) as "Free GB",
  cast(' '||to_char(coalesce((su.usedgb*100)!/(su.usedgb+su.freegb),0),'990.9') || '%' as varchar(7)) as "% Used",
  case when su.slabid is null then ''
       else cast(' '||to_char(coalesce((su.mingb*100)!/(su.usedgb+su.freegb),0),'990.9') || '%' as varchar(7))
  end as "% MinFr"
from
  (select
     st.slabid,
     sum(st.usedgb)/power(1024,3) as usedgb,
     sum(st.freegb)/power(1024,3) as freegb,
     (min(st.freegb)*count(st.mpid))/power(1024,3) as mingb
   from
     (select
        s.slab_id as slabid,
        s.mpid as mpid,
        s.data_stored*x.cu_size as usedgb,
        s.free_space*x.cu_size as freegb
      from sys.ipe_disk_slab s
             inner join
           sys.ipe_xor_element x
             on s.mpid=x.mpid) st(slabid,mpid,usedgb,freegb)
   group by grouping sets ((st.slabid),()) ) su(slabid,usedgb,freegb,mingb)
order by su.slabid desc nulls last

Further information

There is a presentation attached to this solution which explains more about disk slabs in general. In addition, section 6.4 of the WX2 Guide covers the general topic of recovering disk space.
Attachment: slabs.ppt,
FAQ: Why does the wxsmd process appear to be busy when viewed through wxtop?
Solution 00000709 by Simon Darkin at 2011-09-05T13:09:26.000+0000

The program wxtop is just a simple script that calls wxprobe with some options and displays the output using watch. To generate the process list, wxprobe contacts the local smd which sends requests to all nodes to collect quite a lot of information, not all of which is displayed by wxtop, including all the entries under /proc/&lt;pid&gt; for process status and timing information. It has to do this on every node and send it to the local smd which can sends it to wxprobe. For wxprobe's -S option, used by wxtop, it has to collect the process timing information twice to get a difference over an interval. (However, I've just checked the source code and, it doesn't collect information for individual threads.)


 


The majority of this activity is "charged" to wxsmd even though it was wxtop that requested it. So this is like looking at top or Task Manager and seeing that process's own activity.


 


You can demonstrate this (for an individual node) by running top when wxtop is and isn't running. When wxtop is running (or if you do wxprobe or some other operations such as installing packages) the wxsmd activity fluctuates around 10% but the rest of the time it is close to zero.


 


Note that the percentage busy as displayed in wxtop does not relate to the CPU as a whole but rather the combined usage of one or more threads, so for example 20% might mean that a single thread is 20% busy, or that two threads are each 10% busy and so on.

FAQ: Why should each link used by the MPK exist on a separate subnet?
Solution 00000711 by Simon Darkin at 2011-10-04T08:42:19.000+0000

Generally speaking applications that use IP for communication have no say in how the messages get from end to end. All they can do is say 'I want to send to this IP address' and the underlying operating system decides which link to send it out of. The way it does this (gateways, etc. aside) is by finding a link on the same subnet as the target IP address and sending the packet down that link.


 


Where you have multiple MPK links on the same subnet the operating system has two outbound links to choose from. It could do round-robin or use various other mechanisms to make this choice and that would be nice but that's not what happens. What operating systems do in this case is they pick whatever link is higher in the routing table and send *all* outbound packets out of that link. 


 


That means if I have:


 

node 1: eth0 IP 10.99.99.100, eth1 IP 10.99.99.101


node 2: eth0 IP 10.99.99.102, eth1 IP 10.99.99.103 


all subnets 255.255.255.0


 


Then I try to send a packet from IP 10.99.99.101 to 10.99.99.103 then Linux would send that packet out of eth0 instead of eth1. It does this because all routing is done based on the destination IP and the source IP is completely ignored. This is the way operating systems do IP and is completely standard. WX2 has to live with it. It means that all the network packets will be sent out of one interface, halving the bandwidth available for outbound messages. This will also be bad for reliability -- Linux won't even start using the second link if the first one stops working.


 


This is why we recommend that systems should have a different IP subnet for each ethernet device. You could have a subnet that goes, in the above example, 255.255.255.0 and use 10.99.99.X for eth0 devices and 10.99.100.X for eth1 devices. Then when WX2 tries to send from eth1-&gt;eth1 the messages will really go out of those links rather than going eth0-&gt;eth1. WX2 is engineered to never try to send messages across networks (i.e. it never tries to send eth0-&gt;eth1) so putting the interfaces on different subnets won't cause any problems.


 


The SMDs read the IP addresses at start time and so any address changes MUST be followed up with a SMD restart and then a WX2 restart in order for the new addresses to be used.


 


Note that you don't need to physically isolate the two networks by having two switches, and you don't need to put them on separate vlans. All you need to do is have separate IP subnets running on the same bit of wire.   Of course having a single switch is a single point of failure.

FAQ: can I abort or interrupt a reclaim?
Solution 00000713 by Kognitio Support at 2011-10-05T12:13:43.000+0000

It is not possible to interrupt a reclaim once it is running - it should be left to run to completion.

If you have a short window for running reclaim, you can reclaim a subset of slabs in the system (rather than all slabs) using the "reclaim for partition { space separated list of slabs } to now"

You can review the progress of reclaim by looking in the serverdbg files in `wxlogd smd` on a WX2 node - you will periodically see entries like the following which give an estimate for how much longer the reclaim will take (but remember that the implicit "create image" at the end of reclaim will still need to run to re-image objects in memory once the disk space recovery completes, and that time is not included in the estimate of time left):

20-08_20:07:40_BST: Reclaim News after 7m12s (est. time left is wrong on resumed reclaims)
20-08_20:07:40_BST: LC DS  57 read 98.47% of which 33.16% reclaimed, time left  0m 6s (slowest)
20-08_20:07:40_BST: LC DS  14 read 100.00% of which 32.78% reclaimed, time left  0m 0s (fastest)
20-08_20:07:40_BST: LC DS  29 read 100.00% of which 32.75% reclaimed, time left  0m 0s
20-08_20:07:40_BST: LC DS  43 read 100.00% of which 32.73% reclaimed, time left  0m 0s


FAQ: why am I getting variable results for a query?
Solution 00000716 by Kognitio Support at 2011-11-11T17:11:42.000+0000

Some SQL queries are inherently non-deterministic, and so can give varying results when rerun. This can also happen if the query is varied in a way which you would not necessarily expect to give variable results (e.g. querying a view gives one set of results, but imaging the view then querying it gives different results).

Valid reasons for non-deterministic results include:

1) floating point aggregation - when floating point numbers are aggregated, the order of aggregation is significant. This is a fundamental property of floating point arithmetic, as adding small values to a large number can often have no effect due to the limited precision, but summing the small values first will tend d to result in those small values affecting the total.

2) window functions with duplicates - if a window function's partition and order by clauses allow for duplicates, then repeating the window function can result in a different row being selected each time. For example, if the values (1,1,1), (1,1,2) are partitioned on the first column and ordered by the second, then sometimes the entry with row_number 1 will be the first of those rows and sometimes the second, and both of these are valid.

To deal with these issues:

1) do not rely on the results of floating point aggregation being exact, as this is something which is not guaranteed.

2) ensure that queries using window functions in this way specify enough information in the partition and order by clauses to ensure deterministic results.


FAQ: How does streaming affect query performance?
Solution 00000739 by Simon Darkin at 2012-03-02T11:52:40.000+0000
People often have the misconception that WX2 only streams when ram resource is low and that this can then lead to bad query performance but it's not as simple as that.  The following is feedback from Andy after a customer remarked that streaming was killing their query performance...


 


They probably just don't notice streaming in the times when it does work for them.  What happens is streaming is transparent and happens pretty much all the time on most systems but people think it's something that 'turns on' when they're running out of memory.  Then they assume that if a query goes slowly it must be streaming and if it goes quickly it must not be streaming so they start saying things like 'streaming makes my queries go slowly'.  Really what's more likely is that quite a few of the queries that are going fast are actually streaming too and at least some of the ones that are going slowly aren't doing any kind of streaming at all -- they're just hard/skewed/whatever queries that take a long time.  For a lot of queries streaming reduces the memory footprint of queries, dramatically in some cases, with a 0 cost or, in many cases, streaming actually makes the query faster too, particularly in the case where the source table is on disk.  

One choice is to turn streaming off with the ai_auto_stream parameter but this isn't a particularly good idea because it will make some of  their queries slower and make lots of queries use a lot more RAM, even the ones that they don't think are actually 'streaming'.  Also when you do this you get problems because one query giving an RS0001 causes the other queries in parallel to die too, whereas with streaming the others tend to just dump history buffers, continue, then reinstate the history buffers once when the problem query has gone away, which is another example of streaming happening when people don't think it will.

Really when people talk about streaming slowing stuff down they're talking about the part of it that re-executes accesses to rebuild results it had to throw away after running out of memory.  Often with crazy queries or high parallel loads you get situations where an access gets restarted a large number of times and that's where the slow queries come in.  What I can do is add a parameter called 'ai_max_restarts'.  That would control the number of times an access gets restarted by the streaming engine, which is the usual cause of the really slow queries.  Setting that parameter would result in an error after a certain number of restarts and would give them a nice amount of control over how much 'streaming' they want to do.  This is a fairly blunt instrument which will give false positives in some cases (i.e. occasionally error perfectly good queries that would run fairly fast) but they could experiment with it to get a behaviour they're happy with.


 


 


 


From 7.2.1 there will be some new parameters to give the user some control over the streaming engine:

ai_max_restarts -- limits the number of times an access can be restarted.  -ve is infinite.  
ai_max_recomps -- the number of *millions of rows* that can be recomputed before you get an error.  Set this to 10 means you could re-run a 2 million row access 5 times.
ai_max_restm -- the number of seconds after the first time an access is run during which we can restart it.  Setting this to 3600 means that 1 hour after the first time an access runs it stops being able to restart so anything that tries to recompute 1 hour old values will get an error.

These are all per-session, user or global.  It should be possible to fine tune one or more of these to stop the really nasty streaming cases without getting too many false positives, queries failing under heavy load, etc.  To turn what most people think of as streaming off altogether you can set ai_max_restarts to 0 and then every value can only be computed once.  The streaming engine won't optimise for this though (these are just checks done when we perform an operation, not optimisation targets) and it does tend to throw away values more often than it has to so people might be surprised at how often this makes queries error ...

FAQ: what is the maximum length for a system_id?
Solution 00000744 by Kognitio Support at 2018-02-07T15:00:18.000+0000

The system_id for a WX2 system is limited to a maximum length of 12 characters.
FAQ: How do I list out tables that are skewed on disk
Solution 00000745 by Simon Darkin at 2012-07-20T11:36:36.000+0000

--list tables that have 1M rows or greater on at least 1 disk store 


--and where the max row count is at least 1.3x greater than min row count


select 


  s.name schema_name, 


  t.name table_name, 


  dt2.id table_id, 


  high max_row_count, 


  low min_row_count, 


  ((1.00 * high)/low) skew_factor


from (


  select id, max(nrows), min(nrows)


  from ( 


    select table_id, mpid, nrows 


    from ipe_ftable 


  ) dt1(id, mpid, nrows) 


  group by 1


  having max(nrows) &gt; 1000000 


) dt2 (id, high, low)


left join ipe_alltable t on dt2.id = t.id


left join ipe_allschema s on s.id = t.schema_id


where ((high * 1.00) / low) &gt; 1.3 


order by 6 desc



 

FAQ: License key issues
Solution 00000749 by Kognitio Support at 2013-02-28T09:10:24.000+0000
If you have a problem with licensing on your Kognitio system, the following checks should be performed:

1) ensure the license key has not been corrupted when transferring between systems. For example, copying and pasting from a windows client to a putty session can result in hyphen characters in the key being corrupted. In addition, the automated emails sent when license keys are generated typically have a period after the key value, so if there is a trailing period in your license key try removing it when installing the key.

2) ensure the time on your Kognitio system is correct - your license key typically has a time range during which it is valid, so if your system time is very inaccurate the current system time may be outside that range. Ensure all clocks in your system are set correctly (you can use "wxtool -S date" from the Linux command line to see the current time on all nodes, "wxsync -c" to see the clock skew across all nodes, and "wxsync -C" to synchronize clocks, although an ntp daemon should normally be used to keep clocks accurate across the system.

3) ensure the system you are installing the license on has the same system_id as the license itself - licenses will only work on the system id they were generated for.

4) for a specific license provided by Kognitio, ensure that the hardware provided for the Kognitio system is no larger than the license allows. The license is for the whole Kognitio platform so if e.g. you have 1TB of RAM you cannot use a 500GB RAM license to restrict your system to only use half the memory - you need to have a license for the whole platform. If your license is too small for the platform you can either reduce the platform size or contact your account manager to arrange for a larger license to be generated.

5) if you are trying to use the implicit license allowed when no licenses have been installed on the system (noting that normal Kognitio support is not available in such a case, although there is a community support forum you can use), ensure no old licenses exist by running the "wxlicense -i" and "wxlicense -Xi" commands - if these show old licenses, you will need to delete them with the "wxlicense -Dn &lt;licence number&gt;" command before the implicit license is usable. 

6) Note that when using the implicit license you can use the [system] memsize parameter to restrict the amount of RAM available ON EACH NODE, so that the total RAM usable for in-memory processing on the system is less than the implicit license limit (currently 128GB).

FAQ: Commissioning issues
Solution 00000750 by Kognitio Support at 2012-07-04T09:09:39.000+0000

This solution deals with a number of commissioning problems with Kognitio.
License key issues.
Problems commissioning a Kognitio system on your own hardware.
If the "Initialising disks" stage of commissioning seems slow

FAQ: What to do if "Initialising disks" is slow / hanging
Solution 00000752 by Kognitio Support at 2012-07-04T09:05:34.000+0000

The "Initialising disks" phase of commissioning has to zero all the disk resources in the system, which can take minutes / hours.

You can 
monitor the progress on each node from the command line on that node by running the following commands in sequence:
cd `wxlogd smd`
tail -f output*

Typically you will see 
output like the following with a line being generated for each disk 
resource on that node for every 1% of the initialisation - this also 
allows you to estimate when the initialisation will complete:
21-05_08:20:38_BST: Stdout: Formatting disk uid WXD:4F9F8252:00000001, resource /dev/sda2.
21-05_08:22:40_BST: Stdout: FORMAT OF /dev/sda2:  1 complete. 
21-05_08:24:42_BST: Stdout: FORMAT OF /dev/sda2:  2 complete. 
21-05_08:26:45_BST: Stdout: FORMAT OF /dev/sda2:  3 complete.
21-05_08:28:47_BST: Stdout: FORMAT OF /dev/sda2:  4 complete.
21-05_08:30:50_BST: Stdout: FORMAT OF /dev/sda2:  5 complete.
21-05_08:32:52_BST: Stdout: FORMAT OF /dev/sda2:  6 complete.
...
FAQ: How do I use the import/export API
Solution 00000753 by Kognitio Support at 2012-07-04T09:39:55.000+0000

The Import/export API documentation is attached to this solution.

However, we'd recommend that you liaise with Kognitio before using this API. Moving forward, it may be more practical to use wxloader/wxunloader as they are much more performant. Also, in a lot of cases using the underlying binaries will be much easier to implement, and have less risk than trying to use the API directly.
Attachment: impexpapi.zip,
FAQ: Understanding ODBC AutoCommit behaviour
Solution 00000754 by Kognitio Support at 2014-12-02T17:22:52.000+0000

ODBC AUTOCOMMIT RULES

When the ODBC driver is in autocommit mode, applications have to be mindful that an automatic commit can close cursors. This can cause problems if, for example, an application:
opens statement handle A
executes a SELECT statement on statement handle A
opens statement handle B
executes an UPDATE statement on statement handle B
tries to fetch from statement handle A.
The fetch from statement handle A will give a Function Sequence Error (HY010), because the UPDATE caused an autocommit which closed the cursor on statement handle A.

It is up to the application to code around this behaviour: http://msdn.microsoft.com/en-us/library/ms711769%28VS.85%29.aspx

KOGNITIO ODBC BEHAVIOUR AND POINTS TO NOTE

To allow greater interoperability with third-party applications which use autocommit mode, the Kognitio ODBC driver only autocommits in certain circumstances, as detailed below.

The driver is allowed to autocommit on any SQLCloseCursor(), or, equivalently, any call to SQLFreeStmt() with the option SQL_CLOSE. Kognitio's ODBC driver will autocommit in this case if and only if the connection handle is in autocommit mode, and there is an open transaction, and there are no other open cursors on the same connection handle. This also applies to freeing a statement handle using SQLFreeHandle().

The driver is allowed to autocommit on any SQLExecute(), which may close cursors on other statement handles. However, Kognitio's ODBC driver only autocommits on an Execute if the statement was not a SELECT statement. This means that even in autocommit mode a SELECT statement will hold locks until the last statement on the connection handle has its cursor closed. It also means that an UPDATE, INSERT or DELETE statement run in autocommit mode will close any open cursors in the same connection handle.

SQLEndTran(), of course, always ends the transaction, as it would in transaction mode.

If the ODBC driver is in transaction mode, and there is a transaction open, then switching to autocommit mode automatically commits the open transaction. This is consistent with the ODBC specification. This autocommit, like any other commit, closes any open cursors on the connection handle.

WORKAROUNDS

If an application runs many statement handles concurrently, and all statement handles are associated with the same connection handle, all the statements run in the same transaction space. This means that any commit, automatic or otherwise, may close cursors on all statement handles.

To avoid this, the application has two options:
Don't run in autocommit mode, and instead do manual commits when it is known that there are no open cursors still to be fetched from.
Use more than one connection handle. In no case does any commit on one connection handle, automatic or otherwise, affect the state of any cursors on statement handles associated with a different connection.
FAQ: How to upgrade versions
Solution 00000755 by Kognitio Support at 2017-09-07T08:25:56.000+0000
This solution deals with issues when upgrading the Kognitio server software version.

Patching an existing version

Each Kognitio patch (aka version) contains two parts: the version number and the release number. For example, in version 7.2.1rel120529, the version number is 7.2.1 and the release is 120529. 
When changing the version of software running, but not changing the version number, the following steps are sufficient when run as wxadmin from the command line on one of the Kognitio nodes:
obtain a copy of the wxpkg file for the version. The latest version 8 can be obtained from http://www.kognitio.com/forums/viewtopic.php?f=2&amp;t=103. Unzip / untar as necessary and place the file in /tmp/ on one of the database nodes
install the new wxpkg file, with "wxserver install &lt;full path name for the wxpkg file&gt;"
set the new version as current with "wxserver set current_version &lt;versionnumber&gt;"
restart all the smds using "wxserver smd all restart"
restart the database with your normal restart command - "wxserver start" by default if you have no imaging script and want everything to be put back into memory.
For example, to switch from 7.2.1rel120529 to 7.2.1rel120627, do the following:
obtain the wxpkg file and place it in /tmp
wxserver install /tmp/wx2-70201rel120627.wxpkg
wxserver set current_version ver70201rel120627
wxserver smd all restart
wxserver start
If any problem occurs as a result of the patch, you can revert to the previous version by following the instructions above starting at the second step, and setting the version to the old version in use rather than the new one (so in this example, step 2 would become "wxserver set current_version ver70201rel120529").



Upgrading to a newer version of software (i.e. a different version number)

This happens when the version number has changed (e.g. going from 7.2.0 to 7.2.1 or from 7.2.1 to 8.1.0).

Before upgrading to a newer version number, read the release notes for the new version which are available from the support portal. Ensure that any specific upgrade instructions are read and understood.

In the absence of further instructions in the release notes, the steps to follow are:
run the SQL "create system image" if you have an imaging script to run afterwards. This will remove objects from memory for the duration of the upgrade, which will speed up the upgrade process. If you do not have an imaging script, do not execute this step.
upgrade to the new version with "wxserver upgrade using &lt;full path name for the wxpkg file"
if the new version has already been installed, upgrade with "wxserver upgrade to &lt;full path name for the wxpkg file&gt;"
if you have an imaging script, you will need to run it now.
I'm getting errors when installing a new version



There are two likely causes:

The version is already installed - in this case, if you are patching an existing version you can proceed to the second step in the instructions. If you are upgrading to a new version number, you can use "wxserver upgrade to &lt;version name&gt;" to continue, rather than "wxserver upgrade using ...". For example, going from 7.2.0 to 7.2.1rel120627, if the latter was already installed you can use "wxserver upgrade to ver70201rel120627" to complete the upgrade.
There is not enough disk space on all nodes to install the software - in this situation you need to free up some disk space. Typically you can do that by removing non-Kognitio files on the same volume, or by uninstalling old Kognitio releases which are no longer in user with "wxserver uninstall &lt;version name&gt;". Once you have enough disk space you can reinstall the software.
I've accidentally tried to follow the patch upgrade instructions when I really wanted to upgrade to a new version number



In this case, the server will normally give an error along the lines below if you were trying to go from 7.2.0 to 7.2.1:



CANNOT START WITHOUT UPGRADE: disk version 70200, current version 70201.

UNKNOWN ERROR RETURNED: -1234 



You need to follow the "Patching an existing version" instructions above, starting at the second step, to set the version back to the previous version that was in use. So in the example here, you would start by setting the current version to whichever 7.2.0 release was in use before the erroneous patch to 7.2.1 attempt.



My upgrade to a new version number fails early on



Prior to running with the new version, the upgrade process does some checks. For example, it will verify there is enough space in various disk slabs to allow the upgrade to proceed. This will generate output like the following for the session initiating the upgrade:


...
   --&gt;  Check there is enough space available

   --&gt;  Aborting upgrade: not enough free space available

   --&gt;  Upgrade failed
...

In this case, the `wxlogd smd`/upgrades directory will have a file containing more information on what went wrong to help diagnose the problem.



In the case of not having enough disk space the options are to recover disk space using reclaim or repack commands (see documentation), or to defer the upgrade - doing "wxserver start" or running your normal restart script should bring the server up with the old version.



FAQ: Performance figures
Solution 00000759 by Kognitio Support at 2012-07-24T08:02:09.000+0000

Scanning

With 2.6GHz AMD CPUs, 32 cores per node, we can scan at up to 1.5 billion rows/second/node with 7.2.1 code as of April 2012. With some internal experiments we have easily got to 3.9 billion rows/second/node (on very short rows, of course, to maximumse the rows/second figure). With a more representative row size of 64 bytes (rather than 4 used for the above), the original 1.5 billion rows/second/node becomes 600 million rows/second/node.

From disk, back in November 2011 with 7.1.2 code we were scanning at 92MB/s/disk, which was pretty much as fast as hdparm could read from disk on the hardware concerned.

From figures back in 2010, it did not make any difference to disk scan performance whether a raw partition or large file was used for the disk resource.

Loading

In November 2011 we used wxloader to load a 32 node server using 2 APs and got a rate of 13.4TB/hour.

Defragmentation

Defragmenting a 68GB image (140 million rows) to reclaim half the space (back to 34GB, 70 million rows) took 10-15 seconds back in early 2011.

Simple queries

With the work already done for simple queries, we are seeing them running 3 times as fast in pre-releases of version 8 as they were in 7.2.1.
FAQ: Will ALTER TABLE generate a new copy of my table?
Solution 00000764 by Kognitio Support at 2012-07-18T13:19:18.000+0000
ALTER TABLE will sometimes generate a new copy of the table, but not always.

As a general rule, if the rows in the table need to be modified as part of ALTER TABLE a new copy of the table will be generated, and this will use more storage. For example, if a column is being added to a table, every row will need to be written out again with the new column added. Due to the way Kognitio stores data, this will be an entire new copy of the table.

However, if rows do not need to be modified, no extra copy of the table will be generated. For example, if a column name is being changed, only metadata should need to be altered.

To confirm for yourself whether an ALTER TABLE command will make a copy of the table, look at the EXPLAIN output for your query.

Renaming a column in the example below does not generate a copy:

Connected to localhost ODBC Version 7.02.01-s120718 Server Version 07.02.0001

&gt;create table mark(a int, b int);

Query           1             Complete     ----   0:00.1   0:00.1   0:00.1

&gt;explain alter table mark alter column a alter name to a1 cascade invalidate dep

endent views on errors;

Diagnostic on george for 7.02.01-s120718:


 1 We apply an exclusive table lock on table MARK(1082).


 2 We apply 1 exclusive row lock on table IPE_ALLCOLUMN.


 3 We fetch one row from IPE_ALLCOLUMN using a lookup on a ram index.


 4 We create a new row.


 5 We insert a single row into table IPE_ALLCOLUMN on disk and in RAM.


 6 We fetch one row from IPE_ALLBASE using a lookup on a ram index.


 7 We create a new row.


 8 We insert a single row into table IPE_ALLBASE on disk and in RAM.


 9 We increment a system parameter to indicate a system change.


 -- Estimated Total Cost : 0.170


Query           2              22 rows     ----   0:00.0   0:00.0   0:00.0




However, changing a column type does in this case (this would not be the case if e.g. changing the size of a varchar field, so always verify with EXPLAIN on your Kognitio instance):


&gt;explain alter table mark alter column a alter type to bigint not null;

Diagnostic on george for 7.02.01-s120718:


 1 We apply an exclusive table lock on table MARK(1082).

...

 8 We use an embedded client to run the following SQL statement `CREATE

    TABLE "SYS"."MARK" ("A" bigint not null, "B" INTEGER )' as user SYS.

...

40 We use an embedded client to run the following SQL statement `INSERT INTO

    "SYS"."MARK" ("A","B") SELECT CAST ("A" AS bigint ),"B" FROM

    "SYS"."#PRE_ALTERED_TABLE_1458"' as user SYS.

...


FAQ: My backup/data load/other admin task fails when run from cron
Solution 00000767 by Kognitio Support at 2012-07-20T10:35:24.000+0000

Sometimes administrators have scripts which run fine when they execute them from a login, but fail when run from cron.

There are many possible reasons for this, typically relating to the environment used by cron being different to the one used from a login prompt.

Diagnosing all such possible problems is beyond the scope of this solution, but search engines should be able to help with most problems you encounter when you provide them with more information on the exact problem you are seeing.

A couple of common problems are:
Not being able to access files, as the script is not run from the same location as when the user is logged in, and/or does not have the same PATH set up. You can check all this information by e.g. writing environment information to a file in /tmp from a cron job, and comparing that to the login environment, then ensuring that the script sets up the environment appropriately for cron.
Running out of some resource when run from cron, but not from a login. Typically this is because the limits in effect for cron jobs are more strict than those in effect from a login. Use "ulimit -a" in a cron job to identify the limits it has, and compare to those for a login. Then modify the cron job to ensure it sets limits appropriately with the ulimit command.
FAQ: CREATE TABLE IMAGE is slow - what should I check?
Solution 00000768 by Kognitio Support at 2012-07-30T15:08:37.000+0000

Problems with CREATE TABLE IMAGE commands running slowly are normally down to one of the following factors, assuming that other general performance-affecting issues have been checked for (see here):

There is a unique / primary key definition for a column/combination of columns including a string, typically a long string. The database maintains an internal index in memory for these columns, and the mechanism used here is slow which will have a significant impact on performance. If the table is large, usually the best approach is to remove the relevant constraints on the table. This can be done by generating a new table with the same definition and using insert-select to copy the data over. Alternatively, a view can be created on top of the table, and then the view can be imaged rather than the table.
There are a lot of duplicates in a column defined to be unique. Uniqueness checking is only performed when inserting into the table whilst it has a RAM image, so if the table is on disk duplicated can be created. When the table is next imaged, the duplicates will slow down the imaging process. In this case, remove the duplicates and ensure the data load process is modified to prevent further duplicates being generated in future.
In addition, internal structures used to support unique columns are very slow to build, so the recommended approach with large tables containing UNIQUE columns is to remove the uniqueness constraint and ensure that integrity checking is done within the ETL process.


FAQ: How does locking work in Kognitio?
Solution 00000775 by Kognitio Support at 2012-08-23T08:54:11.000+0000

Lock types

Kognitio uses locks on objects to ensure consistency.

Typically there are two sorts of lock granularity - TABLE or ROW locks.

There are also two lock types - EXCLUSIVE or SHARED.


What locks conflict with each other?

As you would expect, ROW locks on different rows in a table do not conflict with each other, and SHARED locks do not conflict with other SHARED locks. Conflicts occur when an EXCLUSIVE lock request and any other lock request conflict in one of the following ways:
they are both row locks for the same row in the same table
they are both table locks for the same table
one is a row lock within a table, and the other is a table lock for that same table


Lock timeouts

Kognitio has a lock timeout mechanism to prevent lock requests being outstanding for too long - by default the timeout is 24 hours, but this can be reduced via the lkti parameter. However, there are normally better ways to deal with long-standing lock requests, so this parameter is not usually changed on production systems.


Deadlocks

Kognitio has a deadlock detection method. Imagine transaction A gets an EXCLUSIVE lock on table T1, then transaction B gets an EXCLUSIVE lock on table T2. Now if A tries to get a SHARED lock on T2 it will block, as B already holds a conflicting lock. Now if B tries to get a SHARED lock on T1, Kognitio detects the deadlock condition, and will error one of the transactions, allowing the other one to proceed.


Visibility of granted / pending locks

The virtual table view, IPE_LOCKS, allow users to see relevant locks which are currently held, or pending. There is also information on e.g. which session and transaction requested each lock, so it is possible to tie these back to entries in IPE_CURSESSIONS if the queries are still running, or IPE_COMMAND if an historic issue is being investigated. To find information on the contents of IPE_LOCKS look here (the description is for IPE_ALLLOCKS, but IPE_LOCKS is a simple view on top of that table).


Diagnosing locking problems which occurred in the past

For more information on collecting information on past lock timeout/deadlock errors, look here.


Explicitly getting table locks via SQL

Normally locks are obtained by Kognitio as part of a query - e.g. selecting from a user table will get some SHARED ROW locks on system tables, and a SHARED TABLE lock on the user table being queried. However, it is also possible to explicitly request locks with the following syntax, as also described in the LOCK TABLE section of the SQL Guide:

    LOCK TABLE &lt;tablename&gt; IN [EXCLUSIVE | SHARE] MODE

This command puts either an EXCLUSIVE or SHARED lock on the table named in the query. 


Lock duration

Note that locks are held for the duration of a transaction (as are other resources in Kognitio such as queues), so if a transaction runs a query but does not end it will continue to hold locks. Some third party tools may do this - if this causes problems, the options are:
consult with the tool vendor to see whether configuration options exist to allow the tool to commit transactions promptly
use security classes to end idle sessions, to prevent them holding locks for a long time
switch to using Kognitio Console if appropriate (e.g. if the third party tool is only used to submit SQL queries/scripts)
FAQ: All about CSV
Solution 00000777 by Kognitio Support at 2012-09-13T10:18:07.000+0000

How CSV Works


Introduction



Some of the most common problems reported about wximport or wxloader are those
concerning data files which claim to be CSV, but aren't. This solution describes how CSV files should be structured, how to handle various awkward characters, and how to code up functions for writing and reading CSV.



The CSV standard used
is that described in
RFC 4180, which provides a good overview. This solution explains the reasoning
behind CSV format rules, why you need to quote your string
fields, and why scouring the keyboard looking for a field separator
character that isn't used in your data is not a good approach.






This document is primarily intended for people exporting data from a database
that isn't WX2. It contains guidance on how CSV files should be formatted.
You should really only need to do this yourself if your database system doesn't
provide a bulk export tool which generates valid CSV in accordance with this solution.






If you're importing data into Kognitio, this document also touches on what options
you can use with wximport and wxloader to get it to load files that deviate
from standard CSV, if possible. For more information consult the man page for
wximport or wxloader






If you've been given a data file by someone else who wants you to load it into Kognitio, and the data file contains poorly-formatted CSV, then wxloader might be
able to understand it depending on how invalid it is. You'll probably need to
specify some command line options to wxloader. Consult the wxloader man page
for more information. If wxloader can't load it at all, probably because of
badly-quoted data, you need to go back to the person who produced the file and
get them to provide you with valid CSV. You could also give them the contents of this solution.



What is CSV?


CSV stands for Comma Separated Values, and is a textual data format. A CSV file consists of zero or more records. A
record consists of one or more fields, separated by a field separator character
(usually a comma, hence the C in CSV) and terminated with a
record terminator character. The record terminator is usually a line break,
which can be either a Unix line break (ASCII code 10) or a Windows line
break (ASCII codes 13 and 10). 






Each field represents a column in the table, and each record represents a row. Every record in a file should be of the same
type, so all the records from one file go into one table. For example, a CSV file containing people's dates and places of birth
might look like this:



Alice,Oxford,1982-05-10
Bob,Reading,1980-01-01
Fred,Swindon,1981-03-31



Sometimes the first record in the file doesn't contain data but instead contains
the names of the fields, like this:



NAME,PLACE OF BIRTH,DATE OF BIRTH
Alice,Oxford,1982-05-10
Bob,Reading,1980-01-01
Fred,Swindon,1981-03-31



This is allowed. If you want to load a file which has this, use
--skip-records=1 in wxloader or -sr 2 in wximport to skip
the first record. 



Commas in fields


What if one of the data fields contains a comma? Won't that be
mistaken for the end of the field? In our birthdates and birthplaces example
above, what if Bob's birthplace were recorded as Reading, Berkshire?
A naive CSV generator might produce a record like this:


Bob,Reading,Berkshire,1980-01-01



However, when wxloader, wximport or any other CSV parsing tool tries to read
this file, it takes a comma as the end of a field, so it'll take the name as
Bob, the birthplace as Reading, and the date of birth as
Berkshire. Then it'll probably complain that it's got an extra field
1980-01-01 on the end that it wasn't expecting, if it hasn't already complained when it tries to read a date and gets Berkshire. 




There are two solutions to this problem.




An Ineffective Solution



The ineffective solution is to use a field separator other than a comma. Sometimes
people use a pipe character (|), or a back tick (`), or a
tilde (~). It doesn't really matter what you pick, the only
requirement is that it's a character that isn't used anywhere in the data.
When you come to load the data into Kognitio, you'll need to specify what your
field separator is. In the case of a pipe character, that'd be -f "|"
for wxloader or -cs "|" for wximport.



Alice|Oxford|1982-05-10
Bob|Reading, Berkshire|1980-01-01
Fred|Swindon|1981-03-31



If your data includes free-form text fields, this is perhaps more difficult
than you might think. Also, it might work with today's data, but it is hard to guarantee that nobody will ever generate valid data which will fall foul of this approach.






Sometimes people get inventive at this point
by writing their export process to use an ASCII control character as the field
separator. This is not a good idea, as it makes any problems with the file much harder to spot.






Sometimes people get more inventive and use multiple obscure characters for
the field separator. This is also not a good idea.






If you've been given a file by someone who's used this ineffective solution to create
it, it's a fair bet they also don't quote their strings. If they did, they
wouldn't need to choose a custom field separator. So you'll probably also need
to use -N (wxloader) or -nd (wximport) to switch on naive
delimiting. Note that this is a stopgap measure which will break e.g. as soon as you start getting files which do contain quoted strings.



The Correct Solution



Proper CSV files don't need to use any field separator other than a comma. Put
away all your pipe characters and back ticks and searching the keyboard for a
character nobody's thought of. All you need to do is quote your string fields,
like so:



"Alice","Oxford",1982-05-10
"Bob","Reading, Berkshire",1980-01-01
"Fred","Swindon",1981-03-31



If a CSV reader encounters a double-quote character ("), then
everything from there to the closing quote character is taken as part of the
field - commas, line breaks, the lot.






Normally you should use a double quote character (", ASCII 34) as the
quote character. That's the only quote character allowed by RFC 4180. If you
must use a single quote (') or any other character, wxloader allows
this, but in that case you need to tell it what character you're using with the
--quote option.  What you can't do is use different quote characters
in different places in the file.

What about quotes themselves?



It's not quite over yet, though. You might have already spotted that there is
one character that needs special treatment if it appears in the field: the
quote character itself. If you have a quote character in the field, how do you
tell what's reading your file that it's part of the data, and not the end of
the string?






The answer is to write it as two quotes. What if two quotes appear in the
field? How do you express that? Same rule applies - each are expressed as two
quotes, so it ends up in the file as four quotes. This means a row of three
quotes in the data is expressed as six in the file; four as eight, five as ten,
and so on. In this way, we can export *any* data that's in the table, no
matter how many commas, newlines or quotes are in it, to a file and have
it read back in without any ambiguity.






By way of example, let's add one more record to our birthdates and birthplaces
file, for someone called Charlie "Chuck" Bloggs. The file should look like this:



"Alice","Oxford",1982-05-10
"Bob","Reading, Berkshire",1980-01-01
"Fred","Swindon",1981-03-31
"Charlie ""Chuck"" Bloggs","Exeter",1978-04-18



Note how the quote characters that are part of the field are distinguished from
the quotes that start and end the string by the fact that they're repeated. When
the parser is inside a quoted string and it reads a quote, it looks at the next
character as well - if that's also a quote, both quotes are consumed and it's
taken as a quote that's part of the data; otherwise the quote is consumed and
it's taken as the end of the quoted string. 


Summary

To recap, CSV should be formatted as follows:



Records are terminated with line breaks.
Fields within a record are separated with commas.
Any field may be quoted; that is, enclosed within double-quote characters.
A field must be quoted if the field contains the field separator
character (comma), a line break, or a double-quote character.
Double-quote characters within a quoted field must be represented
as a pair of double-quote characters.


Reference implementation


Writing a CSV field



The following C function takes a string as an argument, formats it as a quoted
CSV field, and writes the result to standard output.



#include &lt;stdio.h&gt;

void
write_csv_field(const char *data) {
    /* Write opening quote */
    putchar('\"');

    /* Write each character in the string to standard output. But if we
       find a quote in the string, repeat it so it's not mistaken for a
       closing quote. */
    while (*data) {
        if (*data == '\"')
            putchar('\"');
        putchar(*data);

        ++data;
    }

    /* Write closing quote */
    putchar('\"');
}


Reading a CSV field


The following C function reads a single CSV field from standard input, and
writes the data contained in it to the buffer pointed to by dest, up to
dest_max bytes. It reads until it finds a field separator (specified by
field_sep) or record terminator (assumed to be a Unix or Windows line
break) which is not inside a quoted string. *stop is set to the
consumed character which caused the function to stop. This will be
field_sep, '\n' or EOF, indicating that we found the
end of the field, record or file respectively.





It returns the number of characters that would have been written to
dest (not including the trailing NUL character) had enough
space been available. A trailing NUL is always written unless
dest_max is 0.



#include &lt;stdio.h&gt;

int
read_csv_field(char *dest, int dest_max, char field_sep, int *stop) {
    int in_quote = 0;   // are we inside a quoted string?
    int c;              // character we've read
    int outchar;        // character we'll write to dest on each iteration
    int dest_len = 0;   // number of chars we've written to dest

    /* Prerequisites:
     *     field_sep is not 0
     *     if dest_max is greater than 0, dest points to a buffer to which at
     *        least dest_max bytes may be written
     *     stop points to valid memory
     */

    /* Keep *stop as 0 until we finish */
    *stop = 0;
    
    /* Read character by character until we reach the end of the file or
       we find a character we should stop on */
    while (*stop == 0 &amp;&amp; (c = getchar()) != EOF) {
        if (in_quote) {
            /* We're inside a quote: only stop if we see a closing quote */
            if (c == '\"') {
                /* Peek at the next character */
                int next = getchar();
                if (next == '\"') {
                    /* Literal quote */
                    outchar = '\"';
                }
                else {
                    /* It's a real closing quote.
                       Put the peeked character back. */
                    ungetc(next, stdin);

                    /* We're no longer in a quoted string, and don't write
                       this quote character out. */
                    in_quote = 0;
                    outchar = -1;
                }
            }
            else {
                /* Whatever this character is, it's inside a quote so
                   write it as-is. */
                outchar = c;
            }
        }
        else {
            /* Not inside a quoted string */
            if (c == '\"') {
                /* We're inside a quoted string now, and don't write this
                   quote character out. */
                in_quote = 1;
                outchar = -1;
            }
            else if (c == field_sep || c == '\n') {
                /* End of field or end of record. */
                outchar = -1;
                *stop = c;
            }
            else {
                /* Ordinary character. */
                outchar = c;
            }
        }

        /* If we have a character to write, and there's enough space,
           then write it. */
        if (outchar != -1) {
            if (dest_len &lt; dest_max) {
                dest[dest_len] = outchar;
            }
            dest_len++;
        }
    }

    if (c == EOF)
        *stop = EOF;

    if (dest_max &gt; 0) {
        /* If the last character in dest was \r and we stopped on \n, then
           remove the \r - this means we got a Windows newline, and the \r
           isn't really part of the data. */
        if (dest_len &gt; 0 &amp;&amp; dest[dest_len-1] == '\r' &amp;&amp; *stop == '\n') {
            dest_len--;
        }
           
        /* NUL-terminate dest. If we can't because there isn't enough space in
           dest, then put the NUL over the last character. */
        if (dest_len &lt; dest_max)
            dest[dest_len] = '\0';
        else
            dest[dest_max - 1] = '\0';
    }

    return dest_len;
}

FAQ: Kognitio Console failed to read configuration file - how can I resolve this?
Solution 00000778 by Kognitio Support at 2012-09-21T10:12:25.000+0000

Kognitio Console has a configuration file, wxconsole.xml, which can usually be found in C:\Documents and Settings\&lt;UserName&gt;\.wxconsole or C:\Users\&lt;UserName&gt;\.wxconsole.

If this file is corrupted (typically by a client machine crashing), Kognitio Console may fail to cope with the corruption when starting, and hence fail to start.

In such cases, renaming the configuration file should allow you to restart the Console. Depending on the nature of the configuration file corruption you may be able to recover some configuration settings from it, although we have seen cases where the file is completely zeroed.

We are working on making KC more tolerant of all forms of configuration file corruption, which will result in it automatically handling this situation and notifying the user that the old configuration file was corrupt.

If your client platform has frequent crashes, it would be wise to frequently take copies of the configuration file rather than having to manually restore configuration file settings each time the file is corrupted. 
FAQ: Deskew does not result in disks being balanced
Solution 00000790 by Kognitio Support at 2013-02-11T12:51:56.000+0000
The instructions in section 7.9 of the Configuration and Maintenance manual explain how to use deskew to add new disks to a system, and balance existing data across those disks.

Having done this, the old and new disks may still be unbalanced. This is covered in the C&amp;M 7.9, but for ease of reference the reasons are:
deskewing works on the total number of rows in a table, including deleted rows. So if a table has a lot of deleted rows early on, it is possible that relatively few rows are migrated to new disks. Repeating the deskew for such tables will decrease the skew.
the old disks still have the old data left on them, even though it is no longer scanned when disk table scans are executed. To recover the space, use normal disk space recovery techniques (repacking, or reclaiming).
FAQ: External Scripts FAQ
Solution 00000796 by Kognitio Support at 2013-04-19T11:25:51.000+0000
What happens if one or more script partitions fails?

When one or more of the external script partitions in a query fails, the whole script execution will fail with an error message.

If the failure is likely to have been caused by data skew, query the data being fed to the script instances using the hash_value function to determine whether data being fed to script instances is skewed.

How can I control the amount of memory used by an external script instance?

Wrap the external script executable in a shell script that sets a specific ulimit for the process.

How can I prevent users running anonymous external scripts?

Ensure that users are only given the "EXECUTE ON EXTERNAL SCRIPT" privilege for scripts you want them to run - do not give them the "EXECUTE ON SCRIPT ENVIRONMENT" privilege which would allow them to run anonymous scripts.

FAQ: How big can Kognitio disk resources be?
Solution 00000803 by Kognitio Support at 2015-04-29T13:38:07.000+0000
There are some limits on the size of disk resources that Kognitio can use.

Currently these are:

7.2.1 - the limit here is 1TB of disk resource because the internal 'sector' numbers are stored as signed 32-bit integers. So the maximum sector is 2^31, and the internal sector size is 512 bytes, meaning that disk resources can be 2^31 * 512 = 1TB. Note that this is physical disk resource size, rather than space for user data, so with a cluster size of 2 you can only have 500GB of user data per disk resource.

8.1.0 - the sector limit above has been removed. A theoretical limit exists as only 256 slabs are allowed per disk resource. With default settings this means each disk resource can have 254 x 16GB slabs (as slabs 1 and 2 are reserved for system and logging tables), giving a total of 4064GB of user data per disk resource. The extra space required for software RAID is added on top of this, so with a cluster size of 2, disk resources could be around 8TB, with just under 4TB of user data. Of course, if the maximum slab size is reduced, the maximum space available for user data per disk resource will drop accordingly. Testing has been done with disk resources of 1.8TB.
FAQ: NUMA awareness with KAP, NUMA skew, and resulting OOM issues
Solution 00000807 by Kognitio Support at 2013-08-22T11:50:07.000+0000
What is NUMA awareness?

From the version 8 technology previews onwards, the KAP software is NUMA aware. This means it tries to allocate memory in an optimal fashion to take advantage of the local NUMA zone for each core, to give the best possible performance.

What problems can it cause?

We've seen some problems with the Linux OOM killer as a result of this, as if a NUMA zone ends up getting too full the OOM killer will be invoked, even if other NUMA zones have plenty of free space.

In version 8, we expect this to improve, as NUMA bindings have been weakened to deal with the NUMA OOM issues seen previously.

You can use the numactl command to find out how many NUMA zones 
there are, and how much free space there is in each one at any given 
time:

#numactl --hardware
available: 4 nodes (0-3)
node 0 size: 64586 MB
node 0 free: 2752 MB
node 1 size: 64640 MB
node 1 free: 3770 MB
node 2 size: 64640 MB
node 2 free: 3634 MB
node 3 size: 64640 MB
node 3 free: 3284 MB
node distances:
node   0   1   2   3 
  0:  10  20  20  20 
  1:  20  10  20  20 
  2:  20  20  10  20 
  3:  20  20  20  10 
#

What mitigating factors should I check for?

As with other OOM issues, ensure that the database nodes are not running extra software other than the core KAP server software, unless extra memory has been reserved for that extra software.

Common examples would be:
running backup from a DB node
using an editor such as vi to look at a large file - we have seen examples of vi using over 9GB on a node because of this
using external scripts which have a significant memory requirement

If any of the above are required, use a min_fixed_pool setting in the [boot options] section of the config file to increase the amount of memory KAP leaves for other software.

Disabling NUMA awareness for KAP

One way to mitigate this is to add the following line to the [boot options] section of the config file and restart the server software:
numa_aware=no

FAQ: what options should I consider to maximise resilience?
Solution 00000815 by Kognitio Support at 2015-06-29T09:01:20.000+0000
There are a number of resiliency options to consider when commissioning a system.

The attached document outlines those options, explains how they increase resilience, and what other consequences there are from using them.

Attachment: Resilience options.pdf,
FAQ: Debugging WX2 crashes
Solution 00000827 by Michael Dams at 2016-06-06T09:12:22.000+0000
1) Getting the core file information 

There are various tools that can be used to debug wx2 crashes. 

NORMAL DEBUGGING: 
The preferred option is "wxdgtool -D" which generates info, history and wxd files, with the wxd file containing the relevant core files. If the system has a dedicated AP (e.g. TCG) you can usually do "wxdgtool -D" from there as APs tend to have lots of disk space which is what you will need, especially if one or more RS nodes have crashed. You can specify the -O option to force the output to a specific directory. 

WHAT TO DO IF NO AP IN THE SYSTEM: 
If no node in the system has a lot of disk space you can use ssh from another node which does have disk space e.g. here I'm using ngpc12 to dump off from a WX2 system which includes the node 10.6.7.62: 
mark on ngpc12 at /nextgen/debug/mark/poc5 $ssh wcsadmin@10.6.7.62 wxdgtool -D -o - &gt; poc5-dgtool.wxd 
You can use "wxdumpx -XF history &lt;wxd file&gt;" and "wxdumpx -XF info &lt;wxd file&gt;" to extract the history and info files from a wxd file obtained in this way. 
Sometimes "wxdgtool -D" runs out of time and does not capture a full core file for any crashed process. You should see something like the following at the end of the output in this case: 

WARNING -- NO COMPLETE CRASHED CORES DUMPED!
Dump complete in 496 secs. 

You should use wxdgdump to extract at least one relevant core file if this happens. The syntax is: 
wxdgdump -t &lt;target host&gt; -p &lt;crashed process&gt; -C &lt;corefilename&gt; 
If there isn't enough space on any node that can run wxdgdump against the system to dump the file (e.g. it is an RS and there is no AP available), you can do: 
ssh user@victim wxdgtool -o - -c \'"target {172.99.xxx.xxx},pid xxx"\' 
-d &gt; /local/file/to/put/it/in 
Then use wxdumpx -Xc /local/file/to/put/it/in to get the corefile out. 

GETTING JUST AN INFO FILE 
To get an info file you would run 
wxdgtool -i 

GETTING JUST A HISTORY FILE 
To get a history file you would run 
wxdgtool -H 

2) OOM problems - when the Linux Out Of Memory killer gets in and kills off processes, including wxdb/wxsmd ones. 

Check /var/log/messages on all nodes to see if there have been any out of memory problems at the time of the problem (watch out for seeing lots of old OOM problems which aren't related to the current issue under investigation though). Something like the following should do the trick: 
wxtool -S 'grep Memory: /var/log/messages' 
You can then check it for the date you are interested in by doing something like: 
wxtool -S 'grep Memory: /var/log/messages | grep June'</t>
<t tx="jonathanhudson.20201006155205.1">@language md

#### Slab

What is a disk store slab?

A disk store slab is a logical division of a disk or disk partition into individually addressable areas, each of which may be customised to different requirements.

Slabs can each contain up to 16GB of addressable user data per disk store. Slabs can be assembled into slab groups, with each slab in the group having identical characteristics. Slab groups allow a user to specify that more than 16GB of storage per disk should be allocated to a particular set of tables.

The first slab is used for system tables, the second for logging tables (primarily to log connections, transactions, and commands), and all other slabs are used for the remaining user data. Operations attempt to remain within one slab for a reasonable period to minimise disk head movement (e.g. when importing data, a slab has a significant chunk added to it before considering use of another slab).

Tables are assigned to a slab group. If more than one slab exists within the group, data will be written to the first available slab until that is almost full, then to the second, and so on. This ensures that data loaded contiguously remains as contiguous as possible on disk – maximising the efficiency of compressed indices, disk scans and reclaims.

WX2 does not allow the user to use SQL to set up or manipulate slabs. System settings are defined when the newsys command is run and these cannot be changed.

#### mpid

The message passing id of the ram store concerned

#### request_id

A unique identifier for each access.

#### access_type

Textual description of the access.

#### state

Description of the access state - normally if problems are observed it is a good idea to check for any mpids which are in a different state to the others involved in this access.

### session

Session for this access, can be joined into ipe_allcursessions or the logging tables (ipe_login, ipe_transaction).

#### transaction

Transaction for this access, can be joined into the logging tables (ipe_transaction, ipe_command).

#### user_id

Numeric identifier for the user triggering the access. Can be joined into ipe_alluser or ipe_allcursessions

##### table_id

table1_id, table2_id: ids of tables involved in the access - can be joined into ipe_allram_access, and other system tables if the ids are &lt; 10,000,000 (ipe_alltable), or 10,000,000 to 20,000,000 (ipe_allview_img).

#### records_inserted

An indication of how many records have been inserted into the target for this access. This will be reset each time the access is restarted, so most commonly use is to check this field is changing rather than relying on its absolute value.
</t>
<t tx="jonathanhudson.20201006160826.1">@language md

#### How can I use virtual tables to find out what WX2 is doing?

There are a number of virtual tables in the SYS schema which allow the state of WX2 to be determined via SQL queries. The most useful are:

ipe_allcursessions      shows current sessions and their associated queries
ipe_allram_access       for looking at current RAM accesses
ipe_allram_images       for looking at current RAM images
ipe_disk_access         for looking at current disk accesses
ipe_disk_slab           how full each disk slab is
ipe_ftable              for replacing the old disku functionality - can see all the disk metadata broken down by disk and slab
ipe_query_queues        the status of each queue - which queries are queued/running for each one, how long they spent in each state
ipe_query_queue_stats   historic information for queues, and their current configuration
ipe_alllocks            shows information on all the granted and pending locks in the system
ipe_mpk                 tables which provide information on the network performance between nodes and processes.

For many of these virtual tables there are views in the SYS schema restricting their output to information relevant to an individual user - e.g. 

ipe_cursessions         is a view on ipe_allcursessions.

As well as the links above, it is possible to find out about a virtual table by executing the following SQL from 7.2.0 onwards: "explain &lt;virtual table name&gt;". This shows the comments for the table and every commented column in that table.

For example, "explain SYS.IPE_DISK_SLAB" gives:

Diagnostic on george for 7.02.01-s120714:
VIRTUAL TABLE IPE_DISK_SLAB in schema SYS
ID 480, location RAMONLY, distribution RAND, 20 columns
Owner: "SYS"
Table created: 2012-05-21 08:22:18
Width in RAM:  24 bytes header + 4 null bytes + 96 data bytes = 124 bytes

Comment: 'information on logical disk usage by slab'

Columns:-
MPID                        INTEGER  'message passing id of the disk store'
SLAB_ID                     INTEGER  'slab number'
DATA_STORED                 INTEGER  'logical disk pages stored'
FREE_SPACE                  INTEGER  'logical disk pages free'
TIDEMARK                    INTEGER  'high water mark of user data'
LOWMARK                     INTEGER  'low mark for system informaton stored at top of slab'
LOGICAL_END_ADDR            INTEGER  'The logical address at the end of the slab'
LOGICAL_END_CU              INTEGER  'The final (logical) cache unit of the slab'
LOGICAL_SPACE_LEFT          INTEGER  'How much space is left for data on the slab measured in cache units'
INITIAL_INSERT_THRESHOLD    INTEGER  'The initial disk usage level before other, lower, slabs are considered for inserts'
MAX_INSERT_THRESHOLD        INTEGER  'How full we allow the disk to become before rejecting new inserts'
CURRENT_INSERT_THRESHOLD    INTEGER  'The current disk usage threshold'
PHYS_START_SEC              BIGINT   'The physical disk sector that is the origin point of this slab'
R5_PHYS_START_SEC           BIGINT   'phys_start_sec adjusted to take into account raid 5 stripes, this will be the true value'
R5_PHYS_END_SEC             BIGINT   'The final, physical disk, sector of a slab adjusted to take into account raid 5 stripes'
PHYS_DATA_START_SEC         BIGINT   'The physical disk sector which is the start of user data for this slab'
INITIAL_FTABLE_RESERVE_THRESHOLD    INTEGER  'Initial filetable reserved space (percentage) before considering other slabs'
CURRENT_FTABLE_RESERVE_THRESHOLD    INTEGER  'Current filetable feserved space (percentage) before considering other slabs'
FTABLE_RESERVE_THRESHOLD_INC        INTEGER  'How much to increase the current_ftable_reserve_threshold by when that limit is reached'
FTABLE_USAGE                INTEGER  'How full the file table (in terms of supported table ids) is as a percentage'

No selectivity information

Create text:-
CREATE RAM ONLY TABLE "SYS"."IPE_DISK_SLAB" ("MPID" INTEGER , "SLAB_ID" INTEGER , "DATA_STORED" INTEGER , "FREE_SPACE" INTEGER , "TIDEMARK" INTEGER , "LOWMARK" INTEGER , "LOGICAL_END_ADDR" INTEGER
 , "LOGICAL_END_CU" INTEGER , "LOGICAL_SPACE_LEFT" INTEGER , "INITIAL_INSERT_THRESHOLD" INTEGER , "MAX_INSERT_THRESHOLD" INTEGER , "CURRENT_INSERT_THRESHOLD" INTEGER , "PHYS_START_SEC" BIGINT , "R
5_PHYS_START_SEC" BIGINT , "R5_PHYS_END_SEC" BIGINT , "PHYS_DATA_START_SEC" BIGINT , "INITIAL_FTABLE_RESERVE_THRESHOLD" INTEGER , "CURRENT_FTABLE_RESERVE_THRESHOLD" INTEGER , "FTABLE_RESERVE_THRES
HOLD_INC" INTEGER , "FTABLE_USAGE" INTEGER )

Create image text:-
CREATE TABLE IMAGE "SYS"."IPE_DISK_SLAB"

Query           2              43 rows     ----   0:00.0   0:00.0   0:00.0

In addition, SYS.IPE_DESCRIPTION contains all comments, so can be used to locate relevant virtual tables. For example, "select * from SYS.IPE_DESCRIPTION where COMMENT imatching 'slab' order by 1,2" gives:

SCHEMANAME    TABLENAME    COLUMNNAME    COMMENT
SYS    IPE_ALLCOMP    INDEX_SIZE    how large the index is in bytes PER SLAB
SYS    IPE_ALLSLABS    (null)    information on slab allocation for future inserts
SYS    IPE_ALLSLABS    SLAB_ID    slab id
SYS    IPE_DISK_ACCESS    PARTITION_ID    slab number
SYS    IPE_DISK_COMPRESSION    (null)    information about disk compression by slab
SYS    IPE_DISK_COMPRESSION    SLAB_ID    slab number
SYS    IPE_DISK_SLAB    (null)    information on logical disk usage by slab
SYS    IPE_DISK_SLAB    CURRENT_FTABLE_RESERVE_THRESHOLD    Current filetable feserved space (percentage) before considering other slabs
SYS    IPE_DISK_SLAB    INITIAL_FTABLE_RESERVE_THRESHOLD    Initial filetable reserved space (percentage) before considering other slabs
SYS    IPE_DISK_SLAB    INITIAL_INSERT_THRESHOLD    The initial disk usage level before other, lower, slabs are considered for inserts
SYS    IPE_DISK_SLAB    LOGICAL_END_ADDR    The logical address at the end of the slab
SYS    IPE_DISK_SLAB    LOGICAL_END_CU    The final (logical) cache unit of the slab
SYS    IPE_DISK_SLAB    LOGICAL_SPACE_LEFT    How much space is left for data on the slab measured in cache units
SYS    IPE_DISK_SLAB    LOWMARK    low mark for system informaton stored at top of slab
SYS    IPE_DISK_SLAB    PHYS_DATA_START_SEC    The physical disk sector which is the start of user data for this slab
SYS    IPE_DISK_SLAB    PHYS_START_SEC    The physical disk sector that is the origin point of this slab
SYS    IPE_DISK_SLAB    R5_PHYS_END_SEC    The final, physical disk, sector of a slab adjusted to take into account raid 5 stripes
SYS    IPE_DISK_SLAB    SLAB_ID    slab number
SYS    IPE_DISK_TRAIN    (null)    An overview of each request registered with the disk train on a per-slab basis
SYS    IPE_DISK_TRAIN    BLOCKS_TO_READ    How many blocks the train will have to read for this request on the relevant slab
SYS    IPE_DISK_TRAIN    SLAB_ID    Which slab is this request targeting
SYS    IPE_DISK_TRAIN    SRR_ID    The Slab Read Request id, a unique identifier for a scan of a slab for a partiular read request
SYS    IPE_DISK_TRAIN_PENDING_READS    (null)    Details of each request made to the disk train on a per-slab basis, shows the break down of each requests current state, how many blocks remain to be read, each sub-group of blocks
SYS    IPE_DISK_TRAIN_PENDING_READS    SLAB_ID    Which slab the read pertains to
SYS    IPE_DISK_TRAIN_PENDING_READS    SRR_ID    The Slab Read Request id
SYS    IPE_DISK_TRAIN_QUEUES    SLAB_ID    The slab to which the blocks this element holds belong to
SYS    IPE_FTABLE    BLOCK_COUNT    number of blocks touched by the table on the slab
SYS    IPE_FTABLE    DEL_ROWS    approximate count of the number of deleted rows on the slab
SYS    IPE_FTABLE    DEL_WORDS    approximate size of the inactive portion of the table on the slab
SYS    IPE_FTABLE    FIRST_PTR    disk address of first record for the table on the slab
SYS    IPE_FTABLE    LAST_PTR    disk address of last record for the table on the slab
SYS    IPE_FTABLE    NROWS    approximate count of number of table rows on the slab
SYS    IPE_FTABLE    PARTITION_ID    slab number
SYS    IPE_FTABLE    TRUNC_WORDS    approximate size of any truncated sections of the table on the slab
SYS    IPE_FTABLE    WORDS    approximate size of the active portion of the table on the slab
SYS    IPE_SLAB    (null)    information about slab size and other metadata
SYS    IPE_SLAB    MAX_FILES    maximum number of tables on a single slab instance
SYS    IPE_SLAB    SLAB_ID    slab number
SYS    IPE_SLAB    START_SECTOR    sector the slab starts on
SYS    IPE_SLAB_TABLE    (null)    information about which slabs can contain which tables at commission time
SYS    IPE_SLAB_TABLE    SEQ    sequence number of table number range for the slab
SYS    IPE_SLAB_TABLE    SLAB_ID    slab number

</t>
<t tx="jonathanhudson.20201006161615.1">@language md

ipe_allram_images contains a row for every RAM store for every image in RAM (whether a table image, view image, or temporary table).

The relevant columns are:

mpid: the message passing id of the ram store in question.

table_id: up to 10,000,000 for a table image, 10,000,000 to 20,000,000 for view images, and over 20,000,000 for temporary tables / streams.

numrecs: the number of records for this table_id on this ram store.

crtrans: the transaction number which created the table - can be used to join to the logging tables (ipe_transaction, ipe_command).

deltrans: the transaction number which dropped the table.

headertype: indicates how large the header is for each row in the image. FULL is 24 bytes, NONE is 0 bytes, SIZELINK is 8 bytes for variable length records.

recordlen: the record length in bytes (does not include the variable portion for variable length records).

distribution: how the table is distributed in RAM.

free_space: how much space is available in bytes out of the space already allocated to this table.

ram_used: the ram allocated to this table in bytes.

owner: an integer identifying the user who created the image.</t>
<t tx="jonathanhudson.20201006161722.1">@language md

ipe_disk_access has a row for each access on each disk store slab. Note that an access involves reading data from WX2, so there will be no entries for e.g. parameterised inserts / imports.


The relevant columns are:


mpid: the message passing id of the disk store in question.

partition_id: the slab id of the slab in question.

operation: a textual description of the operation being performed.

using_ci: an indication of whether the operation is using a compressed index.

table_id: the id of the table upon which the operation is being performed.

first_page: the first page on this slab which is touched by the table.

last_page: the last page on this slab which is touched by the table.

current_page: the page currently being processed.

session: the session id for this operation.

tno: the transaction number for this operation.

active: whether the operation is currently active (alternatively, it could be queued waiting for a table scanning thread to become available).
</t>
<t tx="jonathanhudson.20201006161753.1">@language md

ipe_disk_slab shows how full each slab within the system is.

This allows skew of slabs to be identified, and reclaims/repacks to be scheduled when appropriate.

For example, if the logging slab (slab 2) is getting full, it should have old entries deleted and be reclaimed.


The relevant columns are:


mpid: the message passing id of the disk store in question.

slab_id: the slab id of the slab in question.

data_stored: how many 8KB disk pages have been used on this slab for user data.

free_space: how many 8KB disk pages are left for use on this slab for user data.

tidemark: the highest numbered page reached by user data.

lowmark: the upper limit towards which tidemark is moving (note: there is a buffer zone of a few pages between tidemark and lowmark such that tidemark cannot get right up to lowmark).
</t>
<t tx="jonathanhudson.20201006161819.1">@language md

ipe_ftable exposes metadata from within the disk subsystem to allow the user to identify which tables are taking up most space, and what space is used by already dropped tables (and hence will be recovered at the next reclaim).


The relevant columns are:


mpid: message passing id of the disk store in question.

partition_id: the slab id of the slab in question.

table_id: the table id of the table in question.

drop_tno: the transaction number which dropped this table - 2147483647
indicates the table has not been dropped.

first_ptr: the disk address of the first record on this slab.

last_ptr: the disk address of the last record on this slab.

nrows: an approximate count of the number of rows for this table in this slab, including deleted and rolled-back records. Note that this can be an overestimate with double-counting of records sometimes occurring, but it is usually fairly accurate.

low_tno: the lowest tno which added records to this table.

high_tno: the highest tno which added records to this table.

block_count: the number of 8KB disk pages containing records from this table.</t>
<t tx="jonathanhudson.20201006161849.1">@language md

ipe_query_queues contains information for each query currently queued/running which is queueable.


The relevant columns are:


queue: the numeric queue identifier.

pos: the queries position in the queue.

user_id: numeric identifier for the user submitting the query.

session: the session submitting the query (can be joined into ipe_allcursessions or the logging tables ipe_login and ipe_transaction).

tno: the transaction number for the query (can be joined into the logging tables, ipe_login and ipe_transaction).

amid: the internal process id of the connection handler for this query.

running: whether the query is running, as opposed to queued.

secs_queued: how many seconds this query was queued for to date..

secs_running: how many seconds this query has been running for to date (0 if still queued).

</t>
<t tx="jonathanhudson.20201006161919.1">@language md

ipe_query_queue_stats contains information on the current configuration of a query queue, and statistics on how the queue has been operating.

The relevant columns are:

queue: numeric identifier of the queue.
len: how many items are in the queue are the moment, including running queries.
nrunning: how many items are running from this queue at the moment.
nwaiting: how many queries are waiting to run in this queue.
nbarriers: how many barriers have been added to the queue to constrain the number of queries which can run concurrently.
totaljobs: how many jobs have been submitted to this queue since the last WX2 restart.
maxrun: how many queries are allowed to run concurrently from this queue.
maxwait: how many queries are allowed to wait in the queue(0 indicates no limit)
load5m, load1h, load5h: these show the load over the last 5 minutes, 1 hour and 5 hours in terms of the average length of the queue over that period including running queries.
queuetime10, queuetime50, queuetime100: these show the average queue time in seconds for the last n queries in the queue.
runtime10, runtime50, runtime100: these show the average runtime in seconds for the last n queries in the queue.

Note that a row is only put into this table when a queue is first used after a WX2 restart, so immediately after a restart the table will typically be empty.
</t>
<t tx="jonathanhudson.20201007072520.1">@language md

#### Initialising Disks

The "Initialising disks" phase of commissioning has to zero all the disk resources in the system, which can take minutes / hours.

You can 
monitor the progress on each node from the command line on that node by running the following commands in sequence:

```
cd `wxlogd smd`
tail -f output*

Typically you will see 
output like the following with a line being generated for each disk 
resource on that node for every 1% of the initialisation - this also 
allows you to estimate when the initialisation will complete:
21-05_08:20:38_BST: Stdout: Formatting disk uid WXD:4F9F8252:00000001, resource /dev/sda2.
21-05_08:22:40_BST: Stdout: FORMAT OF /dev/sda2:  1 complete. 
21-05_08:24:42_BST: Stdout: FORMAT OF /dev/sda2:  2 complete. 
21-05_08:26:45_BST: Stdout: FORMAT OF /dev/sda2:  3 complete.
21-05_08:28:47_BST: Stdout: FORMAT OF /dev/sda2:  4 complete.
21-05_08:30:50_BST: Stdout: FORMAT OF /dev/sda2:  5 complete.
21-05_08:32:52_BST: Stdout: FORMAT OF /dev/sda2:  6 complete.
```
</t>
<t tx="jonathanhudson.20201007073358.1">@language md

#### Introduction

In the event of a software crash on KAP, it is useful to collect debug information
prior to a restart, this information can then be used by Kognitio to diagnose the
fault.

This document deals with the steps required to gather debug information and
the options available when initiating a restart afterwards.
The basic steps involved are:

1. Confirm the software is in a crashed state: &lt;1 minute
2. Generate a hands-off dump file and capture log files : &lt;10 minutes
3. Identify the initial crashed process: &lt;1 minute
4. Generate a core file for the crashed process: &lt;5 minutes
5. Restart the database: 1-? minutes

More detail on each of the steps is contained below.

#### Confirm that the KAP software has crashed

As the wxadmin/wxroot user make an ssh connection to any node running the
SMD and run:
```
wxserver info state
```
If the status field in the output reads:
```
Status: Fatal error - ERROR: WXDB crash detected by monitor..
```
then the KAP software has crashed and you should proceed to the next step.

#### Generate a hands-off dump and capture KAP log files

As the wxadmin/wxroot user make an ssh connection to the AP, navigate to an
area that has at least 1GB of free disk space and run:
```
wxprobe –s &gt; probeoutput.txt
```
This will generate small file with information about which processes have
crashed. Next run:
```
wxdgtool –D –O .
```
This will generate small history and info files, and a dump file of around 1GB in
size.
Now tar up the KAP log files with:
```
tar czvf ./logfiles.tgz /var/log/wx2
```

#### Identify the initial crashed process

The dump file captured in the previous step does not include full information for
large processes such as ram stores, so it is important to explicitly capture a core
file for the first crashed process, which is what this step does.

The monitor file keeps a chronological record of all crashed processes, in most
cases however it is only the initial crashed process that is of interest.

View the monitor file with:

```
wxtool -S 'cat `wxlogd smd`/monitor*'
```

Search through the output for the string ‘SERVER CRASH!’ keeping in mind that
the file may have a record of historical crashes and so check the timestamp
associated with each entry to ensure you are looking at the correct timeframe.

The initial crashed process will be the one listed immediately after the ‘SERVER
CRASH!’ string.

Included below is some example output from a monitor file on a crashed server
from which four pertinent items of information must be noted before extracting
the core file, namely the node upon which the crashed process was running, the
Linux process id, the mpid and the process type, all highlighted below.

```
&gt; wxtool -S 'cat `wxlogd smd`/monitor*'
Kognitio WX2 Administration Utility v8.01.00-rel141124 on wxsupp01
(c)Copyright Kognitio Ltd 2003-2014.
Results:
--snip--
For node wxsupp01-wx2-rack4-enc1-4 (ecode 0, 1005 bytes):
T_2015-07-01_10:38:17_BST: Monitor started.
T_2015-07-01_10:38:39_BST: Detected a crash on node wxsupp01-wx2-rack4-enc1-1:
T_2015-07-01_10:38:39_BST: Process 4520: Name WXDB(WATCHDOG), nthreads 1, state T,
size 14184448.
T_2015-07-01_10:38:39_BST: : ppid 1, pgid 4520, tracerpid 0, time
4(1,3)+0(0,0).
T_2015-07-01_10:38:39_BST: : status Crashed(ERROR: Child pid 4555
stopped (probably crashed)).
T_2015-07-01_10:38:39_BST: : mpid -1, type WATCHDOG
T_2015-07-01_10:38:39_BST: SERVER CRASH!
T_2015-07-01_10:38:39_BST: Detected a crash on node wxsupp01-wx2-rack4-enc1-1:
T_2015-07-01_10:38:39_BST: Process 4555: Name WXDB(11): Compiler, nthreads 35,
state T, size 63885312.
T_2015-07-01_10:38:39_BST: : ppid 4520, pgid 4520, tracerpid 0, time
0(0,0)+607(420,187).
T_2015-07-01_10:38:39_BST: : status Crashed(ERROR: GULHalt_ called at
gul.c:1570).
T_2015-07-01_10:38:39_BST: : mpid 11, type Compiler
```

From the example output we identify the following items:

```
Node: wxsupp01-wx2-rack4-enc1-1
Linux Process id: 4555
Message passing id (mpid): 11
Process type: Compiler
```

Identify the IP address of the node with the crashed process by way of the mpid
identified previously:

```
wxprobe -a '{mpid 11}' –l | grep ip
&gt; wxprobe -a '{mpid 11}' -l | grep ip
Kognitio WX2 Hardware Discovery Tool v8.01.00-rel141124 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2014.
Node wxsupp01-wx2-rack4-enc1-1, ip 14.4.41.1: npr 24.
```

So from the example the IP address is identified as 14.4.41.1

#### Generate a core file for the crashed process

Using the information gathered in step 4 construct the dump command to
generate the core file for the crashed process. Note that large processes such
as ram stores will produce a core file around 4GB in size, so ensure you have
sufficient disk space before proceeding.

The dump command takes the form:

```
wxdgdump –t &lt;IP address&gt; -p &lt;Linux process id&gt; -C &lt;filename&gt;
```
So using the values from our example we would run:

```
wxdgdump –t 14.4.41.1 –p 4555 –C Compiler_11.core
```

This concludes the debug capture phase and the database can now be restarted.

#### Restart the database

If you have a script that restarts the KAP software and re-images user data then
use that to restart the system.

In the absence of a script you can either restart with a system image which
won’t load any user data back into RAM with:

```
wxserver start sysimage
```

or restart with a full image and have user data re-imaged with:

```
wxserver start
```

In KAP version 8 and above wxserver start will attempt to use image recovery
by default allowing for a fast restart. 

If that is not possible (for example,following removal of a failed node), then user images will be reloaded from disk;
this should be expected to take significantly longer. 
</t>
<t tx="jonathanhudson.20201007074942.1">@language md

A WX2 user with default privileges is not able to access all of the ram and disk monitoring facilities within Control Tower.

The group 'GRP_MONITOR' has all of the necessary privileges to do this.

#### GRANTING PRIVILEGES

GRP_MONITOR has select privileges on:

```
IPE_DISPLAY
IPE_BOOT
IPE_PROCESS
IPE_DISK
IPE_XOR_CLUSTER
IPE_XOR_ELEMENT
IPE_IDLE
```

You can either add a user directly to GRP_MONITOR with:

```
alter group GRP_MONITOR add user &lt;user&gt;;
```

or if you need these plus additional privileges then you may wish to create an admin group, add the user(s) to that group, then add the admin group to GRP_MONITOR as follows: 

```
create group GRP_ADMIN;

alter group GRP_ADMIN add user &lt;user&gt;;

alter group GRP_MONITOR add group GRP_ADMIN;
```

Now you can add other privileges to GRP_ADMIN as appropriate.


#### REVOKING PRIVILEGES

If you need to remove a user from a group you can achieve this with:

```
alter group &lt;GROUP&gt; drop user &lt;USER&gt;;
```
</t>
<t tx="jonathanhudson.20201007075718.1">@language md

#### How do I give an AP node full DB capabilities?

An AP can be given full database capabilities, for example when it is required to act as a replacement for a failed DB node.   Note this solution focuses on changing the AP node's capabilities and does not discuss the process of incorporating into the system a DB node previously configured as an AP.  

1. INITIAL CHECKS

Before you start, ensure that consideration has been given as to how the AP node will host a WX2 disk resource. This will be either in the form of a raw partion of type 0x60 or a dedicated file that resides in a linux partiton. 

If an AP node is currently being used for other activities that require a significant amount of system resources then it is NOT an ideal candidate for use as a DB node, as the database performance will be impacted by these other activities. 

Compare the file "/proc/cpuinfo" on the AP node with one of the DB nodes to ensure the CPUs are identical.  This is not essential but does removes the risk of introducing a node that has lower performing CPU than existing database nodes.

Make sure the AP node has the same amount of ram available as the DB nodes to avoid the risk of ending up with lots of small ramstores.  In the example output below we can see that AP node wx2-f0v6r6e5b1 has the same amount of ram as the other four nodes in the system. 

```
&gt; wxprobe -HN
Kognitio WX2 Hardware Discovery Tool v6.01.07-q on wx2-system1
(c)Copyright Kognitio Ltd 2001-2008.

1 node like this one:
  bay 1: ID wx2-f0v6r6e5b1, Status Up
         sys Linux-2.6.16.54-0.2.5.PTF.283002.0-smp, Disks 1, links 4
         ram 16671453184, mrpp 3921674240, swap 0, cpus 4, end L.
         Caps DBG-MASTER.

4 nodes like this one:
  bay 2: ID wx2-f0v6r6e5b2, Status Up
         sys Linux-2.6.16.54-0.2.5.PTF.283002.0-smp, Disks 1, links 4
         ram 16671453184, mrpp 3921674240, swap 0, cpus 4, end L.
         Caps DB-IO-DS-RS-MI-DBG.

```

2. ADDING FULL DATABASE CAPABILITIES TO THE AP NODE 

Connect to the AP node, stop the database and the SMD

```
&gt; ssh wxadmin@
&gt; wxserver stop
&gt; wxsvc stop 
```

As root edit the file /opt/kognitio/wx2/etc/local_config and replace the entry "nodeclass=cp" with "nodeclass=full" in the [attributes] section, e.g.

```
[attributes]
nodeclass=full
```

Restart the SMD and check that the new node has the "db" capability.

```
&gt; wxsvc start 

&gt; wxprobe -a '{wx2-f0v6r6e5b1}' -HC
Kognitio WX2 Hardware Discovery Tool v6.01.07-q on wx2-system1
(c)Copyright Kognitio Ltd 2001-2008.

1 node with capability db (DB).
1 node with capability db_io (IO).
1 node with capability db_ds (DS).
1 node with capability db_rs (RS).
0 nodes with capability daemon (PBD).
0 nodes with capability mpkd (MPKD).
```

Diskspace permitting this node is now ready to be configured with a WX2 disk resource which could be in the form of a raw partion of type 0x60 or a file resource on a linux filesystem.</t>
<t tx="jonathanhudson.20201007085021.1">@language md

### How can I use the ipe_allcursessions virtual table?

ipe_allcursessions contains the following columns of interest:

session: this is the session id, which can be used to look up information in the logging tables (ipe_login, ipe_transaction).
user_id: the name of the user for this session.
abort:   updating the table setting abort to 1 results in the current query being aborted. Setting value to 2 results in the session being aborted.
connect_time:    a timestamp indicating when this session connected to WX2.
last_command:    the last SQL command run by this session (may still be running).
command_running: indicates the state of the last command as follows:

0: idle (note - may still be holding database locks)
1: running
2: rolling back the last statement
3: rolling back the last transaction
4: queued
5: waiting for the query to be compiled

client_name: a textual name for the client machine involved in this session
net_if_name: identifies the connection handling process within WX2 that is responsible for this session
net_address: address of the client machine</t>
<t tx="jonathanhudson.20201007085852.1">@language md

#### Investigating performance issues (locking, queueing, poor queries, ...)

From time to time you may find that queries are taking longer to run than you expect.  There are a number of factors that can affect query times, some of which are easily investigated.


1) Locking Issues


A query that appears to have been running a long time may actually be in a wait state pending a lock.  Locks are required to ensure inter-query integrity. The default lock timeout is 24 hours, so if a session holds a lock for a prolonged period (possibly as a result of having completed a command but not committed the transaction) it can make other queries with conflicting lock requirements hang. The lock timeout period can be changed by setting the lkti parameter in the runtime parameters section of the config file. The value of 
lkti specifies the lock timeout period in seconds.


IPE_ALLLOCKS allows users to see all the granted and pending locks in the system, and the view IPE_LOCKS lets users see portions of IPE_ALLLOCKS based on the privileges they have.

To investigate further you can need to identify the queries corresponding to the sessions/transactions seen in the lock output. The best way to do this is by looking in IPE_ALLCUR_TRANS, or the corresponding view IPE_CURTRANS. This will show the currently open transactions (remember that a client can run all the queries it wants, but then not end its transaction which will leave it holding all its locks and other resources).

If the commands have completed but their transactions have not been committed, they will still be holding locks as explained above. Uncommitted transactions will appear in IPE_TRANSACTION as a single row with an operation value of 2 denoting the transaction has started.

Kognitio Console has a "Blocking Locks Report" under View | Reports, which allows the current user to see which user queries, if any, are holding locks preventing the current user's queries from running.

In addition, the following query can be run as SYS to show all locks held which currently have a corresponding PENDING lock (i.e. these locks are preventing other queries from making progress):

select session_id, type, table_id, row_hash, s.user_id, s.client_name, s.last_command 
from sys.ipe_alllocks l, sys.ipe_allcursessions s, sys.ipe_allcurtrans t 
where l.session_id = s.session 
and t.session = l.session_id
and status = 'GRANTED' 
and table_id in (select table_id from sys.ipe_alllocks where status = 'PENDING')
and command_running = 0 
order by 1,3,4,2


Post-mortem diagnosis of locking issues is possible by querying IPE_ERRORLOG which will contain an entry like the following for a lock timeout:


Lock Manager: session 17201 transaction 16928 requested SH TAB on table 1095. Conflicts with session 17200 transaction 16927 EX ROW on table 1095, row hash 548587615


It is also possible to review `wxlogd smd`/locking* to see historical evidence of when a query waited on a pending lock for more than da_dump_lkti seconds (default 600), which results in output like this:

02-06_17:57:23_BST: DA: DUMPING LOCKS: Locks held for longer than da_dump_lkti
02-06_17:57:23_BST: DA: Locks granted:
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash 1173333716
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash -1277331839
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH ROW table      1 row hash 670452605
...
02-06_17:57:23_BST: DA: Session 26597 TNo  67489 type EX ROW table      1 row hash -789570782
...
02-06_17:57:23_BST: DA: Session 26597 TNo  67489 type SH ROW table      0 row hash 1385381117
02-06_17:57:23_BST: DA: Locks pending:
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table     12 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table      0 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table      1 row hash -2147483648
02-06_17:57:23_BST: DA: Session 26589 TNo  67494 type SH TAB table     27 row hash -2147483648

In the case above you can see that the EX ROW (exclusive row) lock held on table 1 by tno 67489 is blocking the pending lock requests from tno 67494.

</t>
<t tx="jonathanhudson.20201007085952.1">@language md

#### Queuing

Kognitio uses queues to restrict the number of concurrent queries running, so a query may not necessarily run as soon as it is submitted, it could remain queued for a while.

You can look in virtual tables like IPE_QUERY_QUEUES and IPE_QUERY_QUEUE_STATS to see how queues are operating.

You can also see from IPE_ALLCURSESSIONS whether commands are queuing (status is 0 if idle, 1 for running, 2 for transaction rollback, 3 for statement rollback, 4 for queued, 5 for being compiled).

In the example output below we can see that tno 17084 under session 17200 is in queue 100001 and has been queued for 481 seconds.  The queue has two queries running (NRUNNING=2), is only permitted to run two queries simultaneously (MAXRUN=2) and has one query waiting (NWAITING=1).  The final query against IPE_ALLCURSESSIONS shows which query under session 17200 is actually queued.

select 
queue,
pos,
priority,
user_id,
session,
tno,
running,
secs_queued

from sys.ipe_query_queues

where running = 0

Query 3

 QUEUE|POS|PRIORITY|USER_ID|SESSION|  TNO|RUNNING|SECS_QUEUED

100001|  0|     100|      9|  17200|17084|      0|        481

Query 3   1 Row(s) Retrieved.    -----  0:00.2  0:00.2  0:00.4    


select 
queue, 
len,
nrunning,
nwaiting,
nbarriers,
maxrun,
maxwait
from sys.ipe_query_queue_stats

where nwaiting = 1

Query 4

 QUEUE|LEN|NRUNNING|NWAITING|NBARRIERS|MAXRUN|MAXWAIT

100001|  1|       2|       1|        0|     2|      0

Query 4   1 Row(s) Retrieved.    -----  0:00.1  0:00.1  0:00.3    


select 
session, 
user_id,  
command_running,
client_name,
net_address,
last_command

from sys.ipe_allcursessions

where command_running = 4

order by session

Query 5

SESSION|USER_ID|COMMAND_RUNNING|CLIENT_NAME|NET_ADDRESS   |LAST_COMMAND

  17200|SD1    |              4|lego       |193.35.206.228|DECLARE SQL_CUR38B0928 CURSOR FOR select count(*) from t2 FOR READ ONLY

Query 5   1 Row(s) Retrieved.    -----  0:00.1  0:00.1  0:00.4    


The Kognitio guide provides more information on how to configure queues.

</t>
<t tx="jonathanhudson.20201007090528.1">@language md

#### No statistics gathered on objects

To allow the query optimiser to choose the best plan, it needs to have statistics on the underlying objects in a query. 

Typically you would collect these with 

```
update statistics for &lt;objectname&gt;
```

whilst &lt;objectname had a RAM image. 

This allows information on e.g. the cardinality of the table, and the selectivity of individual columns to be gathered. As a result, the optimiser can better determine things like the optimal order for joining objects.</t>
<t tx="jonathanhudson.20201007090731.1">@language md

#### Skewing queries

The database will always distribute data to allow each memory-scanning process (Ram Store, or RS 
for short) to deal with a portion of the whole query. All the RS processes will do this in parallel. If the data is distributed in a 
skewed fashion, such that a small number of RS processes need to do more than their fair share of processing, then the query time will be 
increased. In extreme situations, one RS might do almost all the processing for one or more steps of the query.

One way to identify this is to run "wxtop" from the Linux command line on one of the database nodes whilst the query is running,
this shows how busy all the databases processes are, so if a small subset of the RS processes are very busy, that indicates skew is causing a performance issue.

To rectify this, review the query to see how skew is introduced. Most commonly this is by hashing on an attribute which has a very skewed 
distribution (e.g. hashing on gender for customers, or on an attribute which has a special value (often 0) when no valid information is 
available. Then either use partial hashing (see documentation) to remove the skew, or remove the skewing attributes from the query at an earlier
stage if they are not required in the final result set (often this is the case).

You can use the wxtop command-line utility as the wxadmin user to see how busy the Kognitio processes in the system are. It is 
similar to the linux 'top' command, except that wxtop will show all Kognitio processes across the system rather than just on one node.  

If wxtop shows a lot of activity on lots of nodes, the server is clearly busy and this will impact on performance of any other queries 
you run

If wxtop shows a small subset of the nodes are busy, there is likely to be some skewing in data causing the problem. You may be able 
to determine which objects are causing skew by looking in IPE_ALLRAM_IMAGES.  Common causes of skew are hashing on a skewed 
attribute, or running window functions with skewed partitions. Search the solutions for "skew" for further information.

How do I list out tables that are skewed on disk

--list tables that have 1M rows or greater on at least 1 disk store 

--and where the max row count is at least 1.3x greater than min row count

```
select 
  s.name schema_name, 
  t.name table_name, 
  dt2.id table_id, 
  high max_row_count, 
  low min_row_count, 
  ((1.00 * high)/low) skew_factor
from (
  select id, max(nrows), min(nrows)
  from ( 
    select table_id, mpid, nrows 
    from ipe_ftable 
  ) dt1(id, mpid, nrows) 
  group by 1
  having max(nrows) &gt; 1000000 
) dt2 (id, high, low)
left join ipe_alltable t on dt2.id = t.id
left join ipe_allschema s on s.id = t.schema_id
where ((high * 1.00) / low) &gt; 1.3 
order by 6 desc
```

#### Skew by Table

You can query 'ipe_ram_images' which has a row for each table and RAMstore (RS) combination, so for a given table there will be one row per RS. You can then look at the discrepancy between minimum and maximum RAM used to see potential skew.

The following SQL statement should be helpful in looking for skew:

```
select * from (select table_id, (max(ram_used) / min(ram_used)) skew from sys.ipe_ram_images group by 1) dt where skew &gt; 1.3 order by 2 desc
```

Deskewing works on the total number of rows in a table, including deleted rows. So if a table has a lot of deleted rows early on, it is possible that relatively few rows are migrated to new disks. Repeating the deskew for such tables will decrease the skew.

The old disks still have the old data left on them, even though it is no longer scanned when disk table scans are executed. To recover the space, use normal disk space recovery techniques (repacking, or reclaiming).
</t>
<t tx="jonathanhudson.20201007091308.1">@language md

#### Missing join conditions.

Kognitio by default will allow you to run a query against multiple tables without specifying a join condition.  This means the server will perform a cartesian join i.e. one where every row from one table is joined to every row from another table, should a join condition be missing.  Cartesian joins are typically expensive in performance terms, even on moderately sized tables. 

You can run "explain &lt;query&gt;" and then check the output for references to cartesian join to see if one will be used.

You can also guard against inadvertent cartesian joins by setting a parameter which will cause an error if the compiler detects that a cartesian join is required.  To do this run the following command in the current session:

set current_session parameter error_on_theta to 1;

Alternatively set the parameter on a per-user or system wide basis, the latter by running the following command as SYS:

set parameter error_on_theta to 1;
</t>
<t tx="jonathanhudson.20201007091424.1">@language md

#### Fragmented tables on disk.

If scanning a relatively small disk-based table takes a lot longer than expected, this is often because the rows for this table are interleaved on disk with data from other tables. This is especially common for status tables which have ETL status recorded in them - by their nature they tend to be interleaved with a lot of data that the ETL process writes to disk.

To confirm data interleaving is the problem, do "create table test disk from select * from &lt;table that is slow to query&gt;", then "select count(*) from test" - if the select is a lot quicker than querying the original table, then fragmentation is the issue.

To resolve the issue, either create a RAM image of the fragmented table (usually the best option as these tables tend to be small), and/or defragment it by doing "alter table &lt;x&gt; set slabs to &lt;slab list&gt; migrate defrag"
</t>
<t tx="jonathanhudson.20201007091617.1">@language md

#### Suspected hanging queries

If wxtop shows no activity it is likely that your query has hung in some other way. Assuming it is not a locking issue (see above), the cause of the hang can probably be deduced by looking at IPE_ALLRAM_ACCESS, and in particular ordering on request_id then looking for nodes which appear to be in an unusual state.  


The states include:

TWait - waiting to transmit data

SWait - waiting to scan data - i.e. waiting to receive it

PWait - waiting to project data - associated with row retrieval projection of data


You can also use IPE_ALLRAM_ACCESS to determine whether the query is
 making progress - e.g. "select sum(records_inserted) from 
ipe_allram_access where sesssion = &lt;your session id determined from 
ipe_allcursessions or wxsqlhist&gt;", repeat several times to check that
 the number of records inserted has changed.

If a query appears to hang, exporting the contents of IPE_ALLRAM_IMAGES, IPE_ALLRAM_ACCESS, IPE_DISK_ACCESS, and IPE_ALLCURSESSIONS, and attaching that information to a case allows Kognitio support to investigate the problem further.
</t>
<t tx="jonathanhudson.20201007092220.1">@language md

1) Ensure the Linux firewall is not preventing communication between Kognitio nodes, or Kognitio server and clients

On RedHat the firewall can normally be permanently disabled as root with: 

/sbin/service iptables save 
/sbin/service iptables stop 
/sbin/chkconfig iptables off 

Check the firewall has been disabled with: 

/sbin/service iptables status 
Firewall is stopped. 



On SuSE check the status with:



/sbin/rcSuSEfirewall2 status

Checking the status of SuSEfirewall2                                                                        unused



If in doubt, discuss with the IT team responsible for the kit to ensure there is nothing preventing communication between the nodes. Note that being able to ping nodes does not indicate there is no problem here!




2) If the server is running Red Hat then check version 5.4 or higher is installed to avoid encountering a bug in realloc that exists in older versions. 

cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 5.4 (Tikanga) 


3) Ensure that kdump is disabled on DB nodes so that the memory available to WX2 is consistent across all these nodes 

Permanently disable kdump with: 

/sbin/service kdump stop 

Check kdump is disabled with: 

/sbin/service kdump status 
Kdump is not operational 


4) Ensure swap is disabled 

This is a recommendation rather than a requirement (swap space doesn't really do any harm, it just doesn't get used effectively).





You can permanently disable swap with: 

swapoff -a 
comment out (with a #) or remove the 'swap' line or lines from /etc/fstab 

Run 'free' and check that swap is set to 0 

free 
total used free shared buffers cached 
Mem: 37042356 362352 36680004 0 62596 96212 
-/+ buffers/cache: 203544 36838812 
Swap: 0 0 0 


5) Check that there is sufficient disk space for Kognitio disk resources. 

If the customer is using a server with multiple disks, for example a DL360, then it might be appropriate to create a mirrored pair out of the first two disks on which a small linux partition containing the OS can be installed, leaving enough space for a large partition to act as a Kognitio disk resource. The remaining disks can then be configured with identically sized partitions for further disk resources. 
The mirrored pair may have a different cylinder size to the individual disks resulting in a small size difference in the partitions, perhaps a few MB. This is ok. 

Ensure that any type 0x60 partitions are created OUTSIDE of an LVM 

Note that previous experience of cloning software has shown that the clone doesn't always end up with the same size 0x60 partitions as the server from which the image was taken, therefore it would be better to create the 0x60 partitions after the cloning process. It's a good idea to create the Kognitio partitions via an RDP job, if that's not possible then create them manually via fdisk on small systems or using a simple shell script taking on larger system being careful not to delete existing partitions. 
The script will need to be tailored for the system in hand but the following example demonstrates how you might create a single 290G partition on a mirrored pair of disks followed by a 135G partition on 6 individual disks. 


#!/bin/sh
#exchange the final 'p' for a 'w' when you're happy with the output
outfile=/opt/kognitio/wx2/etc/mkpart.out
rm -rf $outfile


(
echo -ne "n\np\n3\n\n+290G\n"
echo -ne "t\n3\n60\np\n"
) | /sbin/fdisk /dev/cciss/c0d0 &gt;&gt; $outfile 2&gt;&amp;1


 


for i in `seq 1 6`; do
( echo -ne "n\np\n1\n\n+290G\n"
echo -ne "t\n60\np\n"
) | /sbin/fdisk /dev/cciss/c0d${i} &gt;&gt; $outfile 2&gt;&amp;1
done






6) Check for the existence of 32 and 64 bit libraries to ensure both environments are supported 

ls /usr/lib64 | wc -l 
811 

ls /usr/lib | wc -l 
478 





If RedHat double check that 32 bit versions of libgcc and libz are installed:


 


find . -name libz.so* -print


find . -name libgcc*.so* -print


 


Expect to find these in /lib or /usr/lib




Other libraries that are required include libc6 (the installer will not run without this). You can use ldd as described at the end of this section to identify any missing libraries for a particular library.




If a library is missing and you are on a 64 bit version of Linux, check the 64bit version being used (example is for libgcc, same steps required for libz):


rpm -q --whatprovides /lib64/libgcc_s-[Tab to complete filename]





Now download and install the 32 bit version - will probably end .i386 rather than x86_64





If libgcc is not present, newsys will crash with Option6 error in misc node, check with:


grep libgcc `wxlogd smd`/output*





If no libz, newsys will also fail and evidence will be in the same smd output file:




grep libz `wxlogd smd`/output*



To check the relevant libraries for a particular binary (including the installer) exist, use ldd &lt;binary&gt;. If this has any lines like the following, the relevant libraries must be installed:

      libc.so.6 =&gt; not found




7) Check that the required ethernet devices are configured to come up on boot (ONBOOT=yes) once the config files have been populated. 

grep -i onboot /etc/sysconfig/network-scripts/ifcfg-eth* 
ifcfg-eth0:ONBOOT=yes 
ifcfg-eth1:ONBOOT=yes 
ifcfg-eth2:ONBOOT=yes 
ifcfg-eth3:ONBOOT=no 
ifcfg-lo:ONBOOT=yes 


8) Check that the network interfaces enumerate correctly. 

Ports can be assigned different names after a reboot, and so it's strongly advisable to create a renaming script that runs at boot time to ensure they are named in a consistent way. 
The example script attached to this solution assigns a name based on the bus id, essentially a physical location and so as long as all servers in the system are setup in an identical way you will have a consistent naming scheme. 
This method will also cope with a network card replacement without the need to edit config files as long as the replacement card is fitted into the same slot as the card being replaced. 

Copy the attached script "wx_dev_renamer" to /etc/init.d and make it executable 

Check the port order with: 

/etc/init/.d/wx_dev_renamer test 

Add the script as a service with: 

chkconfig --add wx_dev_renamer 

Check the run levels with: 

chkconfig --list wxx_dev_renamer 

Reboot the server and run /sbin/ifconfig -a to check that the interfaces have been assigned the correct names. 


9) The customer should ensure IP addresses assigned to Kognitio nodes do not exist anywhere else on the network 


10) If the system is going to use 10G networking check for the existence of a 10GbE adapter. 





Note SUSE may not support 10G cards without first installing the manufacturer's drivers. Red Hat does support certain 10G cards out of the box however these aren't always the recommended drivers and so it is best to check the manufacturers website. 

/sbin/lspci 
--snip-- 
04:00.0 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10-Gigabit Server Adapter (rev 42) 
04:00.1 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10-Gigabit Server Adapter (rev 42) 


11) Ensure firmware and drivers versions are up-to-date on 10G cards. 

Previous experience with the NetXen NC522SFP 10G card has shown that it can hang under load, evidenced by the following type of entries in /var/log/messages 

Feb 15 17:42:18 &lt;hostname&gt; kernel: NETDEV WATCHDOG: eth0: transmit timed out 
Feb 15 17:42:18 &lt;hostname&gt; kernel: netxen_nic eth0: transmit timeout, resetting. 
Feb 15 17:42:19 &lt;hostname&gt; kernel: NETDEV WATCHDOG: eth1: transmit timed out 
Feb 15 17:42:19 &lt;hostname&gt; kernel: netxen_nic eth1: transmit timeout, resetting. 

Typically you won't be able to ping the interfaces once the card has hung in this way. 

In the case of the NetXen NC522SFP upgrade the firmware and driver to version 4.0.516 or later. It is also advisable to use the nx_nic driver as recommended by and available from HP as opposed to the netxen_nic driver that comes with RedHat. 

You can check the current driver/firmware versions with: 

ethtool -i eth0 
driver: nx_nic 
version: 4.0.520 
firmware-version: 4.0.520 
bus-info: 0000:04:00.0 


Some firmware upgrades require that the eprom is flashed for the change to persist across boots. 


12) Ensure the wxadmin and wxroot users do not exist prior to installing the Kognitio software 

Check for existence of the wxadmin and wxroot user with: 

egrep '(wxadmin|wxroot)' /etc/passwd 

If these accounts already exist then ask the customer to remove them before installing Kognitio software. 


13) Ensure the BIOS is up-to-date 

HP have identified a bug in older versions of the BIOS running on DL360 servers (and some others) that can lead to uncorrectable memory errors being reported resulting in a server reset. Ensure the BIOS is at revision "2010.01.13 (1 Feb 2010)" or later to avoid the bug. 

Some HP systems include a cli utility that comes as part of the HP PSP which can be used to gathered useful information including the status of memory modules and even the IML logs. This is useful when reviewing memory related issues. 

/sbin/hpasmcli -s "SHOW DIMM" 
/sbin/hpasmcli -s "SHOW IML" 


14) Check network link speeds 

Once Kognitio software is installed check the network link speeds to ensure that they are all negotiating at the expected rate. We have seen incidents where at least one link was running at a degraded rate resulting in a significant drop in overall performance. Check the speeds with: 

wxtool -S 'ethtool eth0 | grep Speed' 


15) Change the IO scheduler if there are issues with concurrency during heavy disk IO activity. 

If heavy disk writes such as insert-selects or repack operations are hampering concurrency then there is a good chance the system is configured with the cfq IO scheduler rather than noop. Check which scheduler is active across all the DB nodes with: 

wxtool -a '{can DB}' -S 'cat /sys/block/cciss*/queue/scheduler' 

The active scheduler will be bracketed so if cfq is enabled you will see: 

noop anticipatory deadline [cfq] 

If cfq is enabled then try switching to noop so that IO is performed in the order that Kognitio requests and is not re-ordered by the scheduler. 

You will need to edit the schedule file for each disk device, the following example demonstrates how to do this for servers with multiple devices, in this case named c0d0 through to c0d6 

wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d0/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt;; /sys/block/cciss\!c0d1/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d2/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d3/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d4/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d5/queue/scheduler' 
wxtool -a '{can DB}' -S 'echo "noop" &gt; /sys/block/cciss\!c0d6/queue/scheduler' 

Check the change has occurred on all disk with: 

wxtool -a '{can DB}' -S 'cat /sys/block/cciss*/queue/scheduler' 

You should now see noop encased in square brackets 

[noop] anticipatory deadline cfq 

To make the change permanent append the string "elevator=noop" to the end of the kernel entry in /boot/grub/grub.conf on all DB nodes. The exact line to be edited will depend on the kernel version and boot options but this is an example of what you'll need to change: 

kernel /boot/vmlinuz-2.6.18-164.11.1.el5 ro root=LABEL=/ rhgb quiet 

... is changed to ... 

kernel /boot/vmlinuz-2.6.18-164.11.1.el5 ro root=LABEL=/ rhgb quiet elevator=noop 

...then after a reboot check the scheduler files all show noop is being used 


For more information on changing the scheduler see http://www.redhat.com/docs/wp/performancetuning/iotuning/iosubsystem-scheduler-selection.html 

IMPORTANT: If you find that the scheduler settings are lost following a reboot then: 

* run dmesg to see which scheduler is flagged as the default 
* run 'cat /proc/cmdline' to see what the actual kernel command line is 

In all likelihood you'll find that cfq is still set to the default, and the kernel command line won't include the noop reference. Possible reasons include: 

* System is booting with LILO and not GRUB 
* The active partition from which the server boots (as denoted by an asterisk in the output from 'fdisk -l boot-device-name') has not been mounted to a directory. If it hasn't then almost certainly /boot/grub/grub.conf is not being used by the boot loader, instead it will use the file on the unmounted active partition which of course will not be available to view/edit. This situation occurred on a customer system and the recommended actions were: 

* rename /boot to /boot_old 
* make a new directory /boot 
* mount the partition /dev/cciss/c0d0p1 to the directory /boot and ensure /etc/fstab is updated 
* compare the contents of /boot with /boot_old copying across missing or out-of-date files as necessary 
* reboot the server and check all is ok 
* remove /boot_old 
* edit /boot/grub/grub.conf and add in the noop entry as described earlier 


16) Pre-existing issues with jexec 

There may be occasions where a jexec file is broken in some way which will prevent the Kognitio install process from setting up the wxsvc service. The install error might look something like this... 

Installed OK. 

Setting current pointer /opt/kognitio/wx2/current-&gt;ver70102k. 
Writing out system configuration. 
insserv: script jexec is broken: incomplete LSB comment. 
insserv: missing `Required-Stop:' entry: please add even if empty. 
--snip-- 

In this example the jexec file is missing the "Required-Stop" for a pre-existing service. The "Required-Stop" entry tells the system in what order services should be shutdown when certain system states are entered. 

If you look at our wxsvc service you can see that we want to start the system when the system is in states 2,3,5 and stop the service when it's in states 0,1,6 

poc1-wx2-rack2-enc2-3:/etc/init.d # cat /etc/init.d/wxsvc 
#!/bin/bash 

# chkconfig: 2345 99 10 
# description: Kognitio WX2 database server 

### BEGIN INIT INFO 
# Provides: wxsvc 
# Required-Start: $network 
# Required-Stop: 
# Default-Start: 2 3 5 
# Default-Stop: 0 1 6 
# Description: WX2 database server services 
### END INIT INFO 

exec /opt/kognitio/wx2/current/bin/wxsvc $* 

On a normal system you can see that the services have been setup in /etc/rc.d at the various run levels defined in the wxsvc services script. 

poc1-wx2-rack2-enc2-3:/etc/rc.d # ls -l /etc/rc.d/rc*.d/*wxsvc* 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc2.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc2.d/S06wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc3.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc3.d/S06wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc5.d/K16wxsvc -&gt; ../wxsvc 
lrwxrwxrwx 1 root root 8 Dec 30 13:59 /etc/rc.d/rc5.d/S06wxsvc -&gt; ../wxsvc 

If as in our example a pre-existing service is not setup correclty then it is really something for the system owner to address, but if circumstances dictate that we ourselves workaround the issue then the following steps (run as root) should help:- 

1. Add the missing stop lines to jexec 
2. Run "inserv" to setup the services in /etc/rc.d and check that they have created ok 
3. Remove the stop lines added in step 1 
4. Run "services wxsvc start" 

The last command should start the SMD and create the smd.* directory in the /var/log/wx2/logs-systemid area. 

The other alternative is to avoid doing the steps above and just run "wxsvc start" as the wxadmin user, but if the node is ever restarted then the SMD won't automatically restart and so is not ideal.








17) If using a SAN for Kognitio disk resources, ensure that the config files for each node limit that node to the set of LUNs that the node should be controlling. 





For example, use "wxviconf -l" on each node and set "partitions=/dev/mapper/x,/dev/mapper/y, ...". If all nodes have common names for the partitions, this operations can be done globally instead of on a node by node basis. If sets of nodes have common LUN names (even though these refer to different LUNs within the SAN), you can use "wxviconf -L" to mass edit the local configuration file on sets which should have identical "partitions=" settings.



 


18) Check that the 'connection tracker' is not running


 


There is a part of Linux called the 'connection tracker' which attempts to match up messages with responses.  It gets used for a number of things including firewalling and NAT routing.  


 


The connection tracker is a kernel module so you can find it like this:


 


[root@hostname ~]# lsmod | grep conntrack
ip_conntrack_netbios_ns    36032  0
ip_conntrack                     91621  3 iptable_nat,ip_nat,ip_conntrack_netbios_ns
nfnetlink                            40457  2 ip_nat,ip_conntrack



The example output tells you that the connection tracker is using the netfilter core. Netfilter is the linux kernel module that does firewalling which tells you the connection tracker probably got enabled by the firewall.  


 


You can also check for it by looking for the files it creates:


 


[root@hostname ~]# cat /proc/sys/net/ipv4/netfilter/ip_conntrack_count
106
[root@hostname ~]# cat /proc/sys/net/ipv4/netfilter/ip_conntrack_max
65536


 


These are present if the connection tracker is running and not if it isn't.  To remove it you can try taking the module out:


 


rmmod ip_conntrack


 


Beware though, it may start up again if the node is rebooted in which case you will need to edit the service responsible and prevent it from doing so.  


 

*** Note that when the Bet365 systems were commissioned in September 2018 the connection tracker was found to be on after the checks had been completed and so either we forget to check, the string we check for needs to change, or Bet365 enabled it after the checks.  Beware of this for future commissioning work as connection tracker has a big impact on message passing performance and so needs to be disabled.




19) Ensure that localhost is defined in the /etc/hosts file on every node :-


 


      127.0.0.1       localhost 

Attachment: wx_dev_renamer.sh,</t>
<t tx="jonathanhudson.20201007092421.1"></t>
<t tx="jonathanhudson.20201007092455.1">@language md

```
#!/bin/bash
# chkconfig: 2345 1 98
# description: script to correct network device names 

# Check that we're a priviledged user
[ `id -u` = 0 ] || exit 0

. /etc/init.d/functions

RETVAL=0

function mkmactab()
{
ct=0
for i in `ls /sys/bus/*/devices/*/net:*/address |sort` 
do
        if [ $ct -gt 3 ] ; then
                dn=$ct
        else
                dn=$(( ($ct + 2) % 4 ))  
        fi
    echo -n ${1}eth$dn" "
    ct=$(($ct + 1)) 
    cat  $i
done &gt;/etc/mactab
}


# perform the update
function start ()
{
        RETVAL=1

mkmactab temp
nameif
mkmactab 
nameif

        return $RETVAL
}

stop()
{
        return 0
}

case "$1" in
  start)
        start
        exit 0
        ;;
  stop)
        stop
        ;;
  restart|reload|force-reload)
    stop
        start
        ;;
  status)
        ;;
  test)
        mv /etc/mactab /etc/mactab2
        mkmactab
        cat /etc/mactab
        mv /etc/mactab2 /etc/mactab
        ;;
  *)
        echo $"Usage: $0 {start|stop|restart}"
        exit 1
esac
exit $?
```</t>
<t tx="jonathanhudson.20201007092619.1">@language md

#### How does WX2 message-passing work?

WX2 assumes uniform connectivity and every process directly sends messages to every other process, letting the network route them as it sees fit.  WX2 doesn't sense the available bandwidth between servers, and doesn't have any variant of SPF (shortest path first) to determine message routes.

The message passing uses UDP as a transport mechanism with flow controls similar to those found in the TCP protocol; WX2 acks messages and uses dynamically changing window sizes and timeout values (in response to duplicate frames/packet drops) to cope with changing network conditions. Timeouts and window sizes are maintained on a per output link basis and the server assumes that each node has an equal bandwidth to all others.

In other words WX2 counts the number of unacked packets sent down a link regardless of which node the packets were sent to.  Typically what this means is WX2 throttles comms to everywhere based on the bandwidth it can send to the most remote node. This isn't a problem because message destinations are quite uniformly distributed so WX2 doesn't ever want to send extra data between two processes just because they are near/local to one another.

Duplicate frames are detected, counted, acked and ignored but WX2 doesn't stop sending just because it has received the same message many times. If WX2 detects too many packet drops on a particular outbound link (e.g. it sent the same thing too often) then it stops using that link and it can do that either for a particular set of destinations or just completely stop using the link to send any packets depending on which packets were dropped.

WX2 does have a configurable MTU but does not use any PMTUD (path MTU discovery) equivalent technology.  WX2 sets the MTU to 9,000 all the time and sends approximately 9KB (usually a bit smaller) IP frames all the time.  If Linux and the networking layer can send jumbo frames then they do, otherwise WX2 relies on IP packet fragmentation as this gives the best performance.  If you don't want IP fragmentation to occur, you need to manually configure WX2 for a smaller MTU, which is easy to do by setting the following configuration file entry:

[mpk]
dgram_mtu=1400</t>
<t tx="jonathanhudson.20201007094348.1">@language md

ipe_alllocks contains information about all granted/pending locks in the system.

The relevant columns in ipe_alllocks are:

session_id:     the session for this lock.
tno:            the transaction for this lock.
type:           the lock type - typically a shared (SH) or exclusive (EX) lock on a table (TAB), or row (ROW).
table_id:       the table_id this lock applies to.
row_hash:       a hash of the row this lock applies to (NULL if the lock is a table rather than row lock).
status:         either GRANTED or PENDING.

Looking in ipe_allcur_trans and joining on tno will allow you to see the corresponding SQL, 
user_id, etc for any lock in ipe_alllocks.</t>
<t tx="jonathanhudson.20201007094724.1">@language md

#### OOM Process Killer

What to do if the Linux OOM killer kills WX2 processes

If the Linux Out Of Memory (OOM) killer kills any WX2 processes this will cause WX2 to halt. 

Typical symptoms include:
a WX2 watchdog process showing up in "wxprobe -s" output with no accompanying WXDB process (as that was the process that was killed)
entries in /var/log/messages showing the OOM killer was activated and that it killed wxdb processes. For example:

Feb 14 11:45:38 KOGNITIO1 kernel: wxdb invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0
Feb 14 11:47:00 KOGNITIO1 kernel: 
...
Feb 14 11:47:00 KOGNITIO1 kernel: Out of memory: Killed process 12345, UID 0, (wxdb).
Feb 14 11:47:00 KOGNITIO1 kernel: wxdb invoked oom-killer: gfp_mask=0x201d2, order=0, oomkilladj=0
...
Feb 14 11:47:00 KOGNITIO1 kernel: Out of memory: Killed process 12367, UID 0, (wxdb).
...

The reason WXDB processes are likely to be killed in this situation is not because they are leaking memory, but because they reserve a lot of memory for their use - WX2 is, after all, an in-memory database.

This situation typically arises when running on a very small development system rather than a production, and/or if applications other than WX2 are running on the database nodes.

If the OOM killer is activated on your system, there are a couple of steps you could take:

1) ensure that you do not run any other applications on the node, so the whole node is reserved for WX2. This is how WX2 is designed to work, and is always how production machines run. However, sometimes with small development systems people try to add other applications onto the node. If you are running things on the node and can stop them, this step will be all you need.

2) if you aren't running any applications on the node, or you really have no alternative, then restrict the RAM for Kognitio by setting the memsize entry in the [system] section of the config file. For example, on a 16GB node you could set that to 8GB (50% of your RAM) as follows:

[system]
memsize=8000000000

This will mean there is less RAM available for WX2, but it will allow you to use more RAM for anything else running on the node.

#### Numa Zones

In version 8, we expect this to improve, as NUMA bindings have been weakened to deal with the NUMA OOM issues seen previously.

You can use the numactl command to find out how many NUMA zones 
there are, and how much free space there is in each one at any given 
time:

```
#numactl --hardware
available: 4 nodes (0-3)
node 0 size: 64586 MB
node 0 free: 2752 MB
node 1 size: 64640 MB
node 1 free: 3770 MB
node 2 size: 64640 MB
node 2 free: 3634 MB
node 3 size: 64640 MB
node 3 free: 3284 MB
node distances:
node   0   1   2   3 
  0:  10  20  20  20 
  1:  20  10  20  20 
  2:  20  20  10  20 
  3:  20  20  20  10 
#
```

What mitigating factors should I check for?

As with other OOM issues, ensure that the database nodes are not running extra software other than the core KAP server software, unless extra memory has been reserved for that extra software.

Common examples would be:
running backup from a DB node
using an editor such as vi to look at a large file - we have seen examples of vi using over 9GB on a node because of this
using external scripts which have a significant memory requirement

If any of the above are required, use a min_fixed_pool setting in the [boot options] section of the config file to increase the amount of memory KAP leaves for other software.

Disabling NUMA awareness for KAP

One way to mitigate this is to add the following line to the [boot options] section of the config file and restart the server software:
numa_aware=no
</t>
<t tx="jonathanhudson.20201007095015.1">@language md

To restrict the memory usage of Kognitio on Dev box

if you aren't running any applications on the node, or you really have no alternative, then restrict the RAM for Kognitio by setting the memsize entry in the [system] section of the config file. 

For example, on a 16GB node you could set that to 8GB (50% of your RAM) as follows:

```
[system]
memsize=8000000000
```

#### Edit the WX2 configuration file as follows:

```
[system]
memsize=0x400000000
```

In this example, the memsize setting will make WX2 behave as though only 16GB of RAM is available to it

```
wxprobe -H' (or -HN)            # should show the node reporting itself as a 16GB node.
```

This setting can be applied to just one node by editing its local configuration file or to all nodes by editing the global configuration file. 

Common values are:

```
0x100000000 - 4GB
0x200000000 - 8GB
0x300000000 - 12GB
0x400000000 - 16GB
```
</t>
<t tx="jonathanhudson.20201007095558.1">@language md

WX2 disk usage, and how to tell when disks or slabs fill up

#### WX2 disk slabs

WX2 divides disks up into slabs. Slab 1 is reserved for system table entries, slab 2 for logging table entries, and the remaining disk space is split into as few slabs as possible based on the maximum slabs size (default 16GB user data, but can be overridden at commission time with the [fs] max_slab_gb entry). Section 7.1 of the Configuration and Maintenance manual describes slabs in more detail.

By default a 60GB disk resource with no software RAID will have:
```
slab 1 0.5GB
slab 2 0.5GB
slab 3 16GB
slab 4 16GB
slab 5 16GB
slab 6 11GB (to use up the remaining space)
```

#### How slabs are filled with data

User data can go in any slab except slabs 1 and 2. With a newly commissioned system, the default is for all data to go to the highest numbered slab until that reaches a threshold of usage, then data will start to be written to the next highest numbered slab until that reaches the same threshold, and so on. When all slabs have reached the threshold, WX2 returns to writing to the highest numbered slab until it reaches the next threshold, and so on.

Note that all disks in the system are following this process independently.

When a record is deleted / updated, the old record is marked as deleted,
 but the space it occupies cannot be reused until either all the data is
 removed from that slab with repack / drop table / truncate table 
operations, or a global lock is obtained on the system and a reclaim run
 including that slab. Section 6.4 of the WX2 Guide explains reclaim and repack in more detail.

When a delete / update operation occurs, the delete marker must be written to the same slab as the data that is being deleted. Therefore, if a slab becomes completely full an error will be returned if an attempt is made to delete any of its rows. For this reason, a small amount of space is reserved for non-record use, so it should always be possible to delete some records from a slab.

When a record is added to disk by the system, a disk is chosen at random for that record - if that disk does not have enough space for the record in the set of slabs eligible for the table concerned, then the operation will fail - the record insert will never be forwarded onto another disk. For this reason, if a record cannot be added to one of the disks in the system the system is effectively full for the table concerned and space will need to be recovered or more slabs allocated for that table before more data can be added to it.

It is possible to restrict which user data slabs are available for general use, for a particular schema, or for a particular table. This can have benefits in terms of imposing disk quotas, and simplifying the recovery of disk space via repack operations.

How to tell how much free disk space there is

There are a number of virtual tables available to explain how much free disk space there is. IPE_DISK_SLAB shows how full each slab on each disk in the system is, and IPE_FTABLE shows how many rows each table has on each disk and slab.

In addition, the wxsubmit command line tool has a number of pre-defined queries which show information on disk usage. $f4 shows disk usage by slab, and $f6 shows disk usage by table, whilst $f8 shows slab allocations for tables. From 7.1.2w onwards, the $f4 query has been modified to show the minimum free space for a given slab across all the disks in the system (this is to deal with the case where one disk has filled a particular slab but the others still have space - as mentioned earlier, this effectively means that slab is full for the system).

The 7.1.2w $f4 query is as follows (note the last column shows the minimum free space on any disk for the given slab):

```
select
  case when su.slabid is null then 'Total  '
       else cast(to_char(su.slabid,'9999999') as varchar(7))
  end as "Slab ID",
  cast(to_char(su.usedgb+su.freegb,'999,990.9') as varchar(9)) as "Slab GB",
  cast(to_char(su.usedgb,'999,990.9') as varchar(9)) as "Used GB",
  cast(to_char(su.freegb,'999,990.9') as varchar(9)) as "Free GB",
  cast(' '||to_char(coalesce((su.usedgb*100)!/(su.usedgb+su.freegb),0),'990.9') || '%' as varchar(7)) as "% Used",
  case when su.slabid is null then ''
       else cast(' '||to_char(coalesce((su.mingb*100)!/(su.usedgb+su.freegb),0),'990.9') || '%' as varchar(7))
  end as "% MinFr"
from
  (select
     st.slabid,
     sum(st.usedgb)/power(1024,3) as usedgb,
     sum(st.freegb)/power(1024,3) as freegb,
     (min(st.freegb)*count(st.mpid))/power(1024,3) as mingb
   from
     (select
        s.slab_id as slabid,
        s.mpid as mpid,
        s.data_stored*x.cu_size as usedgb,
        s.free_space*x.cu_size as freegb
      from sys.ipe_disk_slab s
             inner join
           sys.ipe_xor_element x
             on s.mpid=x.mpid) st(slabid,mpid,usedgb,freegb)
   group by grouping sets ((st.slabid),()) ) su(slabid,usedgb,freegb,mingb)
order by su.slabid desc nulls last
```

Further information

There is a presentation attached to this solution which explains more about disk slabs in general. In addition, section 6.4 of the WX2 Guide covers the general topic of recovering disk space.
Attachment: slabs.ppt,</t>
<t tx="jonathanhudson.20201007100941.1">@language md

#### SQK Slabs
```
CREATE SCHEMA… [SET SLABS TO {ALL | slab-list}]
CREATE USER...SCHEMA schema [SET SLABS TO {ALL | slab-list}
ALTER SCHEMA nameSET SLABS TO {ALL | SYSTEM DEFAULT | slab-list} [MIGRATE [DEFRAG]]
CREATE TABLE tableSET SLABS TO slab-list | SCHEMA DEFAULT | ALL
ALTER TABLE table SET SLABS TO slab-list | SCHEMA DEFAULT | ALL [MIGRATE [DEFRAG]]
ALTER TABLE tableADD | DROP SLABS slab-list [MIGRATE [DEFRAG]]
ALTER SYSTEM SET DEFAULT SLABS TO slab-list | ALL
```

Show Slabs

```
SELECT * FROM ipe_allslabs ORDER BY 1, 2, 3
 	SCHEMA_ID	TABLE_ID	SEQ	SLAB_ID 
	-1      	-1      	0       13 
	-1      	-1      	1       14 
	-1      	1080    	0       6 	
	-1      	1080    	1       7 
	-1      	1080    	2       8 
	2       	-1      	0       3 
	2       	-1      	1       4 
	2       	-1      	2       5 
	3       	-1      	0       6 
	3       	-1      	1       7 
	3       	-1      	2       8 
```

Note: This indicates how slabs will be used in the future - it does not indicate which slabs are currently being used to hold the data!
</t>
<t tx="jonathanhudson.20201007104419.1"></t>
<t tx="jonathanhudson.20201007104430.1">@language md

#### Subnet Masks

Why should each link used by the MPK exist on a separate subnet?
Solution 00000711 by Simon Darkin at 2011-10-04T08:42:19.000+0000

Generally speaking applications that use IP for communication have no say in how the messages get from end to end. All they can do is say 'I want to send to this IP address' and the underlying operating system decides which link to send it out of. The way it does this (gateways, etc. aside) is by finding a link on the same subnet as the target IP address and sending the packet down that link.

Where you have multiple MPK links on the same subnet the operating system has two outbound links to choose from. It could do round-robin or use various other mechanisms to make this choice and that would be nice but that's not what happens. What operating systems do in this case is they pick whatever link is higher in the routing table and send *all* outbound packets out of that link. 

That means if I have:
 
```
node 1: eth0 IP 10.99.99.100, eth1 IP 10.99.99.101

node 2: eth0 IP 10.99.99.102, eth1 IP 10.99.99.103 

all subnets 255.255.255.0
```

Then I try to send a packet from IP 10.99.99.101 to 10.99.99.103 then Linux would send that packet out of eth0 instead of eth1. It does this because all routing is done based on the destination IP and the source IP is completely ignored. This is the way operating systems do IP and is completely standard. WX2 has to live with it. It means that all the network packets will be sent out of one interface, halving the bandwidth available for outbound messages. This will also be bad for reliability -- Linux won't even start using the second link if the first one stops working.

This is why we recommend that systems should have a different IP subnet for each ethernet device. You could have a subnet that goes, in the above example, 255.255.255.0 and use 10.99.99.X for eth0 devices and 10.99.100.X for eth1 devices. Then when WX2 tries to send from eth1-&gt;eth1 the messages will really go out of those links rather than going eth0-&gt;eth1. WX2 is engineered to never try to send messages across networks (i.e. it never tries to send eth0-&gt;eth1) so putting the interfaces on different subnets won't cause any problems.

The SMDs read the IP addresses at start time and so any address changes MUST be followed up with a SMD restart and then a WX2 restart in order for the new addresses to be used.

Note that you don't need to physically isolate the two networks by having two switches, and you don't need to put them on separate vlans. All you need to do is have separate IP subnets running on the same bit of wire.   Of course having a single switch is a single point of failure
</t>
<t tx="jonathanhudson.20201007104651.1">@language md

Can I abort or interrupt a reclaim?

It is not possible to interrupt a reclaim once it is running - it should be left to run to completion.

If you have a short window for running reclaim, you can reclaim a subset of slabs in the system (rather than all slabs) using the "reclaim for partition { space separated list of slabs } to now"

You can review the progress of reclaim by looking in the serverdbg files in `wxlogd smd` on a WX2 node - you will periodically see entries like the following which give an estimate for how much longer the reclaim will take (but remember that the implicit "create image" at the end of reclaim will still need to run to re-image objects in memory once the disk space recovery completes, and that time is not included in the estimate of time left):

```
20-08_20:07:40_BST: Reclaim News after 7m12s (est. time left is wrong on resumed reclaims)
20-08_20:07:40_BST: LC DS  57 read 98.47% of which 33.16% reclaimed, time left  0m 6s (slowest)
20-08_20:07:40_BST: LC DS  14 read 100.00% of which 32.78% reclaimed, time left  0m 0s (fastest)
20-08_20:07:40_BST: LC DS  29 read 100.00% of which 32.75% reclaimed, time left  0m 0s
20-08_20:07:40_BST: LC DS  43 read 100.00% of which 32.73% reclaimed, time left  0m 0s
```
</t>
<t tx="jonathanhudson.20201007104730.1">@language md

Why am I getting variable results for a query?

Some SQL queries are inherently non-deterministic, and so can give varying results when rerun. This can also happen if the query is varied in a way which you would not necessarily expect to give variable results (e.g. querying a view gives one set of results, but imaging the view then querying it gives different results).

Valid reasons for non-deterministic results include:

1) floating point aggregation - when floating point numbers are aggregated, the order of aggregation is significant. This is a fundamental property of floating point arithmetic, as adding small values to a large number can often have no effect due to the limited precision, but summing the small values first will tend d to result in those small values affecting the total.

2) window functions with duplicates - if a window function's partition and order by clauses allow for duplicates, then repeating the window function can result in a different row being selected each time. For example, if the values (1,1,1), (1,1,2) are partitioned on the first column and ordered by the second, then sometimes the entry with row_number 1 will be the first of those rows and sometimes the second, and both of these are valid.

To deal with these issues:

1) do not rely on the results of floating point aggregation being exact, as this is something which is not guaranteed.

2) ensure that queries using window functions in this way specify enough information in the partition and order by clauses to ensure deterministic results.
</t>
<t tx="jonathanhudson.20201007105118.1">@language md

How does streaming affect query performance?
Solution 00000739 by Simon Darkin at 2012-03-02T11:52:40.000+0000
People often have the misconception that WX2 only streams when ram resource is low and that this can then lead to bad query performance but it's not as simple as that.  The following is feedback from Andy after a customer remarked that streaming was killing their query performance...


 


They probably just don't notice streaming in the times when it does work for them.  What happens is streaming is transparent and happens pretty much all the time on most systems but people think it's something that 'turns on' when they're running out of memory.  Then they assume that if a query goes slowly it must be streaming and if it goes quickly it must not be streaming so they start saying things like 'streaming makes my queries go slowly'.  Really what's more likely is that quite a few of the queries that are going fast are actually streaming too and at least some of the ones that are going slowly aren't doing any kind of streaming at all -- they're just hard/skewed/whatever queries that take a long time.  For a lot of queries streaming reduces the memory footprint of queries, dramatically in some cases, with a 0 cost or, in many cases, streaming actually makes the query faster too, particularly in the case where the source table is on disk.  

One choice is to turn streaming off with the ai_auto_stream parameter but this isn't a particularly good idea because it will make some of  their queries slower and make lots of queries use a lot more RAM, even the ones that they don't think are actually 'streaming'.  Also when you do this you get problems because one query giving an RS0001 causes the other queries in parallel to die too, whereas with streaming the others tend to just dump history buffers, continue, then reinstate the history buffers once when the problem query has gone away, which is another example of streaming happening when people don't think it will.

Really when people talk about streaming slowing stuff down they're talking about the part of it that re-executes accesses to rebuild results it had to throw away after running out of memory.  Often with crazy queries or high parallel loads you get situations where an access gets restarted a large number of times and that's where the slow queries come in.  What I can do is add a parameter called 'ai_max_restarts'.  That would control the number of times an access gets restarted by the streaming engine, which is the usual cause of the really slow queries.  Setting that parameter would result in an error after a certain number of restarts and would give them a nice amount of control over how much 'streaming' they want to do.  This is a fairly blunt instrument which will give false positives in some cases (i.e. occasionally error perfectly good queries that would run fairly fast) but they could experiment with it to get a behaviour they're happy with.

From 7.2.1 there will be some new parameters to give the user some control over the streaming engine:

ai_max_restarts -- limits the number of times an access can be restarted.  -ve is infinite.  
ai_max_recomps -- the number of *millions of rows* that can be recomputed before you get an error.  Set this to 10 means you could re-run a 2 million row access 5 times.
ai_max_restm -- the number of seconds after the first time an access is run during which we can restart it.  Setting this to 3600 means that 1 hour after the first time an access runs it stops being able to restart so anything that tries to recompute 1 hour old values will get an error.

These are all per-session, user or global.  It should be possible to fine tune one or more of these to stop the really nasty streaming cases without getting too many false positives, queries failing under heavy load, etc.  To turn what most people think of as streaming off altogether you can set ai_max_restarts to 0 and then every value can only be computed once.  The streaming engine won't optimise for this though (these are just checks done when we perform an operation, not optimisation targets) and it does tend to throw away values more often than it has to so people might be surprised at how often this makes queries error ...
</t>
<t tx="jonathanhudson.20201007105812.1">@language md

The "Initialising disks" phase of commissioning has to zero all the disk resources in the system, which can take minutes / hours.

You can 
monitor the progress on each node from the command line on that node by running the following commands in sequence:

```
cd `wxlogd smd`
tail -f output*
```

Typically you will see 
output like the following with a line being generated for each disk 
resource on that node for every 1% of the initialisation - this also 
allows you to estimate when the initialisation will complete:

```
21-05_08:20:38_BST: Stdout: Formatting disk uid WXD:4F9F8252:00000001, resource /dev/sda2.
21-05_08:22:40_BST: Stdout: FORMAT OF /dev/sda2:  1 complete. 
21-05_08:24:42_BST: Stdout: FORMAT OF /dev/sda2:  2 complete. 
21-05_08:26:45_BST: Stdout: FORMAT OF /dev/sda2:  3 complete.
21-05_08:28:47_BST: Stdout: FORMAT OF /dev/sda2:  4 complete.
21-05_08:30:50_BST: Stdout: FORMAT OF /dev/sda2:  5 complete.
21-05_08:32:52_BST: Stdout: FORMAT OF /dev/sda2:  6 complete.
...
```</t>
<t tx="jonathanhudson.20201007110100.1">@language md

How to upgrade versions
Solution 00000755 by Kognitio Support at 2017-09-07T08:25:56.000+0000
This solution deals with issues when upgrading the Kognitio server software version.

Patching an existing version

Each Kognitio patch (aka version) contains two parts: the version number and the release number. For example, in version 7.2.1rel120529, the version number is 7.2.1 and the release is 120529. 

When changing the version of software running, but not changing the version number, the following steps are sufficient when run as wxadmin from the command line on one of the Kognitio nodes:
obtain a copy of the wxpkg file for the version. The latest version 8 can be obtained from http://www.kognitio.com/forums/viewtopic.php?f=2&amp;t=103. 

Unzip / untar as necessary and place the file in /tmp/ on one of the database nodes

install the new wxpkg file, with "wxserver install &lt;full path name for the wxpkg file&gt;"

set the new version as current with "wxserver set current_version &lt;versionnumber&gt;"

restart all the smds using "wxserver smd all restart"

restart the database with your normal restart command - "wxserver start" by default if you have no imaging script and want everything to be put back into memory.

For example, to switch from 7.2.1rel120529 to 7.2.1rel120627, do the following:
obtain the wxpkg file and place it in /tmp

```
wxserver install /tmp/wx2-70201rel120627.wxpkg
wxserver set current_version ver70201rel120627
wxserver smd all restart
wxserver start
```

If any problem occurs as a result of the patch, you can revert to the previous version by following the instructions above starting at the second step, and setting the version to the old version in use rather than the new one (so in this example, step 2 would become "wxserver set current_version ver70201rel120529").

Upgrading to a newer version of software (i.e. a different version number)

This happens when the version number has changed (e.g. going from 7.2.0 to 7.2.1 or from 7.2.1 to 8.1.0).

Before upgrading to a newer version number, read the release notes for the new version which are available from the support portal. Ensure that any specific upgrade instructions are read and understood.

In the absence of further instructions in the release notes, the steps to follow are:

run the SQL "create system image" if you have an imaging script to run afterwards. This will remove objects from memory for the duration of the upgrade, which will speed up the upgrade process. If you do not have an imaging script, do not execute this step.

upgrade to the new version with 

```
wxserver upgrade using &lt;full path name for the wxpkg file&gt;
```

if the new version has already been installed, upgrade with 

```
wxserver upgrade to &lt;full path name for the wxpkg file&gt;
```

if you have an imaging script, you will need to run it now.

I'm getting errors when installing a new version



There are two likely causes:

The version is already installed - in this case, if you are patching an existing version you can proceed to the second step in the instructions. If you are upgrading to a new version number, you can use "wxserver upgrade to &lt;version name&gt;" to continue, rather than "wxserver upgrade using ...". For example, going from 7.2.0 to 7.2.1rel120627, if the latter was already installed you can use "wxserver upgrade to ver70201rel120627" to complete the upgrade.
There is not enough disk space on all nodes to install the software - in this situation you need to free up some disk space. Typically you can do that by removing non-Kognitio files on the same volume, or by uninstalling old Kognitio releases which are no longer in user with "wxserver uninstall &lt;version name&gt;". Once you have enough disk space you can reinstall the software.
I've accidentally tried to follow the patch upgrade instructions when I really wanted to upgrade to a new version number



In this case, the server will normally give an error along the lines below if you were trying to go from 7.2.0 to 7.2.1:



CANNOT START WITHOUT UPGRADE: disk version 70200, current version 70201.

UNKNOWN ERROR RETURNED: -1234 



You need to follow the "Patching an existing version" instructions above, starting at the second step, to set the version back to the previous version that was in use. So in the example here, you would start by setting the current version to whichever 7.2.0 release was in use before the erroneous patch to 7.2.1 attempt.



My upgrade to a new version number fails early on



Prior to running with the new version, the upgrade process does some checks. For example, it will verify there is enough space in various disk slabs to allow the upgrade to proceed. This will generate output like the following for the session initiating the upgrade:


...
   --&gt;  Check there is enough space available

   --&gt;  Aborting upgrade: not enough free space available

   --&gt;  Upgrade failed
...

In this case, the `wxlogd smd`/upgrades directory will have a file containing more information on what went wrong to help diagnose the problem.



In the case of not having enough disk space the options are to recover disk space using reclaim or repack commands (see documentation), or to defer the upgrade - doing "wxserver start" or running your normal restart script should bring the server up with the old version.

</t>
<t tx="jonathanhudson.20201007110752.1">@language md

ALTER TABLE will sometimes generate a new copy of the table, but not always.

As a general rule, if the rows in the table need to be modified as part of ALTER TABLE a new copy of the table will be generated, and this will use more storage. For example, if a column is being added to a table, every row will need to be written out again with the new column added. Due to the way Kognitio stores data, this will be an entire new copy of the table.

However, if rows do not need to be modified, no extra copy of the table will be generated. For example, if a column name is being changed, only metadata should need to be altered.

To confirm for yourself whether an ALTER TABLE command will make a copy of the table, look at the EXPLAIN output for your query.

Renaming a column in the example below does not generate a copy:

Connected to localhost ODBC Version 7.02.01-s120718 Server Version 07.02.0001

```
&gt;create table mark(a int, b int);
```

Query           1             Complete     ----   0:00.1   0:00.1   0:00.1

```
&gt;explain alter table mark alter column a alter name to a1 cascade invalidate dependent views on errors;
```

Diagnostic on george for 7.02.01-s120718:

 1 We apply an exclusive table lock on table MARK(1082).
 2 We apply 1 exclusive row lock on table IPE_ALLCOLUMN.
 3 We fetch one row from IPE_ALLCOLUMN using a lookup on a ram index.
 4 We create a new row.
 5 We insert a single row into table IPE_ALLCOLUMN on disk and in RAM.
 6 We fetch one row from IPE_ALLBASE using a lookup on a ram index.
 7 We create a new row.
 8 We insert a single row into table IPE_ALLBASE on disk and in RAM.
 9 We increment a system parameter to indicate a system change.

 -- Estimated Total Cost : 0.170

Query           2              22 rows     ----   0:00.0   0:00.0   0:00.0

However, changing a column type does in this case (this would not be the case if e.g. changing the size of a varchar field, so always verify with EXPLAIN on your Kognitio instance):

```
&gt;explain alter table mark alter column a alter type to bigint not null;
```

Diagnostic on george for 7.02.01-s120718:

 1 We apply an exclusive table lock on table MARK(1082).
...
 8 We use an embedded client to run the following SQL statement `
 
```
CREATE TABLE "SYS"."MARK" ("A" bigint not null, "B" INTEGER )' as user SYS;
```

We use an embedded client to run the following SQL statement 

```
INSERT INTO
    "SYS"."MARK" ("A","B") SELECT CAST ("A" AS bigint ),"B" FROM
    "SYS"."#PRE_ALTERED_TABLE_1458"' as user SYS.
```
</t>
<t tx="jonathanhudson.20201007111129.1">@language md

CREATE TABLE IMAGE is slow - what should I check?

Problems with CREATE TABLE IMAGE commands running slowly are normally down to one of the following factors, assuming that other general performance-affecting issues have been checked for (see here):

There is a unique / primary key definition for a column/combination of columns including a string, typically a long string. The database maintains an internal index in memory for these columns, and the mechanism used here is slow which will have a significant impact on performance. If the table is large, usually the best approach is to remove the relevant constraints on the table. This can be done by generating a new table with the same definition and using insert-select to copy the data over. Alternatively, a view can be created on top of the table, and then the view can be imaged rather than the table.

There are a lot of duplicates in a column defined to be unique. Uniqueness checking is only performed when inserting into the table whilst it has a RAM image, so if the table is on disk duplicated can be created. When the table is next imaged, the duplicates will slow down the imaging process. In this case, remove the duplicates and ensure the data load process is modified to prevent further duplicates being generated in future.

In addition, internal structures used to support unique columns are very slow to build, so the recommended approach with large tables containing UNIQUE columns is to remove the uniqueness constraint and ensure that integrity checking is done within the ETL process.
</t>
<t tx="jonathanhudson.20201007111229.1">@language md

Lock types

Kognitio uses locks on objects to ensure consistency.

Typically there are two sorts of lock granularity - TABLE or ROW locks.

There are also two lock types - EXCLUSIVE or SHARED.


What locks conflict with each other?

As you would expect, ROW locks on different rows in a table do not conflict with each other, and SHARED locks do not conflict with other SHARED locks. Conflicts occur when an EXCLUSIVE lock request and any other lock request conflict in one of the following ways:
they are both row locks for the same row in the same table
they are both table locks for the same table
one is a row lock within a table, and the other is a table lock for that same table


Lock timeouts

Kognitio has a lock timeout mechanism to prevent lock requests being outstanding for too long - by default the timeout is 24 hours, but this can be reduced via the lkti parameter. However, there are normally better ways to deal with long-standing lock requests, so this parameter is not usually changed on production systems.


Deadlocks

Kognitio has a deadlock detection method. Imagine transaction A gets an EXCLUSIVE lock on table T1, then transaction B gets an EXCLUSIVE lock on table T2. Now if A tries to get a SHARED lock on T2 it will block, as B already holds a conflicting lock. Now if B tries to get a SHARED lock on T1, Kognitio detects the deadlock condition, and will error one of the transactions, allowing the other one to proceed.


Visibility of granted / pending locks

The virtual table view, IPE_LOCKS, allow users to see relevant locks which are currently held, or pending. There is also information on e.g. which session and transaction requested each lock, so it is possible to tie these back to entries in IPE_CURSESSIONS if the queries are still running, or IPE_COMMAND if an historic issue is being investigated. To find information on the contents of IPE_LOCKS look here (the description is for IPE_ALLLOCKS, but IPE_LOCKS is a simple view on top of that table).


Diagnosing locking problems which occurred in the past

For more information on collecting information on past lock timeout/deadlock errors, look here.


Explicitly getting table locks via SQL

Normally locks are obtained by Kognitio as part of a query - e.g. selecting from a user table will get some SHARED ROW locks on system tables, and a SHARED TABLE lock on the user table being queried. However, it is also possible to explicitly request locks with the following syntax, as also described in the LOCK TABLE section of the SQL Guide:

    LOCK TABLE &lt;tablename&gt; IN [EXCLUSIVE | SHARE] MODE

This command puts either an EXCLUSIVE or SHARED lock on the table named in the query. 


Lock duration

Note that locks are held for the duration of a transaction (as are other resources in Kognitio such as queues), so if a transaction runs a query but does not end it will continue to hold locks. Some third party tools may do this - if this causes problems, the options are:
consult with the tool vendor to see whether configuration options exist to allow the tool to commit transactions promptly
use security classes to end idle sessions, to prevent them holding locks for a long time
switch to using Kognitio Console if appropriate (e.g. if the third party tool is only used to submit SQL queries/scripts)
</t>
<t tx="jonathanhudson.20201007111714.1">@language md

How do I review issues with lock timeouts/deadlocks from log files?

This solution concentrates on reviewing locking problems that have occurred in the past. If you suspect there are current locking issues on your system which are preventing queries from making progress, search for "Investigating performance issues" in the support portal rather than using the contents of this solution.

There are two sorts of locking problems:

1) lock timeouts - these occur when a lock request has been pending for longer than the lock timeout period. The default lock timeout period is one day, but on some machines it is set to a much lower value e.g. 30 seconds. The "lkti" parameter specifies the lock timeout period in seconds. If a lock timeout occurs, the client receives the DA0003 lock timeout error code.

2) deadlocks - here, two sessions have conflicting locks. For example, session A has an exclusive table lock on T1 and is trying to get an exclusive table lock on T2, while session B is holding an exclusive table lock on T2. If B now tries to get an exclusive table lock on T1, the deadlock will be detected and one of the queries generating a lock request will get the deadlock error code, DA0005. In this simple case, only two sessions are involved, but clearly there could be many more.

In case (1), an entry will appear in SYS.IPE_ERRORLOG looking something like this:

Lock Manager: session 213669 transaction 452609 requested EX TAB on table 12039. Conflicts with session 213668 transaction 452608 SH TAB on table 12039 

Now it is possible to look in ipe_command to see which transactions were being run for the transactions listed above, and abort one of the sessions if appropriate, or wait for one session to complete then resubmit the query.

In case (2), the SYS.IPE_ERRORLOG contains an entry of the following form:

Lock Manager: session 63506 transaction 291265 requested SH TAB on table 3727. Deadlock detected: dependency cycle of sessions 63506, 62908.

In this case, look in ipe_command for commands run by transactions from the relevant sessions to see which queries caused the problems, and resolve as for case (1).

Note that locks are held for the duration of a transaction, so the completion of a command is not sufficient - the transaction must be rolled back/committed to free any locks held. Check in ipe_transaction to see if the relevant transaction has two entries - if it has only one it is still open and still holding locks.
</t>
<t tx="jonathanhudson.20201007111940.1">@language md

How CSV Works

#### Introduction

Some of the most common problems reported about wximport or wxloader are those
concerning data files which claim to be CSV, but aren't. This solution describes how CSV files should be structured, how to handle various awkward characters, and how to code up functions for writing and reading CSV.

The CSV standard used is that described in
RFC 4180, which provides a good overview. This solution explains the reasoning
behind CSV format rules, why you need to quote your string
fields, and why scouring the keyboard looking for a field separator
character that isn't used in your data is not a good approach.


This document is primarily intended for people exporting data from a database
that isn't WX2. It contains guidance on how CSV files should be formatted.
You should really only need to do this yourself if your database system doesn't
provide a bulk export tool which generates valid CSV in accordance with this solution.

If you're importing data into Kognitio, this document also touches on what options
you can use with wximport and wxloader to get it to load files that deviate
from standard CSV, if possible. For more information consult the man page for
wximport or wxloader

If you've been given a data file by someone else who wants you to load it into Kognitio, and the data file contains poorly-formatted CSV, then wxloader might be
able to understand it depending on how invalid it is. You'll probably need to
specify some command line options to wxloader. Consult the wxloader man page
for more information. If wxloader can't load it at all, probably because of
badly-quoted data, you need to go back to the person who produced the file and
get them to provide you with valid CSV. You could also give them the contents of this solution.

#### What is CSV?

CSV stands for Comma Separated Values, and is a textual data format. A CSV file consists of zero or more records. A
record consists of one or more fields, separated by a field separator character
(usually a comma, hence the C in CSV) and terminated with a
record terminator character. The record terminator is usually a line break,
which can be either a Unix line break (ASCII code 10) or a Windows line
break (ASCII codes 13 and 10). 

Each field represents a column in the table, and each record represents a row. Every record in a file should be of the same
type, so all the records from one file go into one table. For example, a CSV file containing people's dates and places of birth
might look like this:

```
Alice,Oxford,1982-05-10
Bob,Reading,1980-01-01
Fred,Swindon,1981-03-31
```

Sometimes the first record in the file doesn't contain data but instead contains
the names of the fields, like this:

```
NAME,PLACE OF BIRTH,DATE OF BIRTH
Alice,Oxford,1982-05-10
Bob,Reading,1980-01-01
Fred,Swindon,1981-03-31
```

This is allowed. If you want to load a file which has this, use
--skip-records=1 in wxloader or -sr 2 in wximport to skip
the first record. 

#### Commas in fields

What if one of the data fields contains a comma? Won't that be
mistaken for the end of the field? In our birthdates and birthplaces example
above, what if Bob's birthplace were recorded as Reading, Berkshire?
A naive CSV generator might produce a record like this:

```
Bob,Reading,Berkshire,1980-01-01
```

However, when wxloader, wximport or any other CSV parsing tool tries to read
this file, it takes a comma as the end of a field, so it'll take the name as
Bob, the birthplace as Reading, and the date of birth as
Berkshire. Then it'll probably complain that it's got an extra field
1980-01-01 on the end that it wasn't expecting, if it hasn't already complained when it tries to read a date and gets Berkshire. 

There are two solutions to this problem.

#### An Ineffective Solution

The ineffective solution is to use a field separator other than a comma. Sometimes
people use a pipe character (|), or a back tick (`), or a
tilde (~). It doesn't really matter what you pick, the only
requirement is that it's a character that isn't used anywhere in the data.
When you come to load the data into Kognitio, you'll need to specify what your
field separator is. In the case of a pipe character, that'd be -f "|"
for wxloader or -cs "|" for wximport.

```
Alice|Oxford|1982-05-10
Bob|Reading, Berkshire|1980-01-01
Fred|Swindon|1981-03-31
```

If your data includes free-form text fields, this is perhaps more difficult
than you might think. Also, it might work with today's data, but it is hard to guarantee that nobody will ever generate valid data which will fall foul of this approach.

Sometimes people get inventive at this point
by writing their export process to use an ASCII control character as the field
separator. This is not a good idea, as it makes any problems with the file much harder to spot.

Sometimes people get more inventive and use multiple obscure characters for
the field separator. This is also not a good idea.

If you've been given a file by someone who's used this ineffective solution to create
it, it's a fair bet they also don't quote their strings. If they did, they
wouldn't need to choose a custom field separator. So you'll probably also need
to use -N (wxloader) or -nd (wximport) to switch on naive
delimiting. Note that this is a stopgap measure which will break e.g. as soon as you start getting files which do contain quoted strings.

#### The Correct Solution

Proper CSV files don't need to use any field separator other than a comma. Put
away all your pipe characters and back ticks and searching the keyboard for a
character nobody's thought of. All you need to do is quote your string fields,
like so:

```
"Alice","Oxford",1982-05-10
"Bob","Reading, Berkshire",1980-01-01
"Fred","Swindon",1981-03-31
```

If a CSV reader encounters a double-quote character ("), then
everything from there to the closing quote character is taken as part of the
field - commas, line breaks, the lot.

Normally you should use a double quote character (", ASCII 34) as the
quote character. That's the only quote character allowed by RFC 4180. If you
must use a single quote (') or any other character, wxloader allows
this, but in that case you need to tell it what character you're using with the
--quote option.  What you can't do is use different quote characters
in different places in the file.

What about quotes themselves?



It's not quite over yet, though. You might have already spotted that there is
one character that needs special treatment if it appears in the field: the
quote character itself. If you have a quote character in the field, how do you
tell what's reading your file that it's part of the data, and not the end of
the string?

The answer is to write it as two quotes. What if two quotes appear in the
field? How do you express that? Same rule applies - each are expressed as two
quotes, so it ends up in the file as four quotes. This means a row of three
quotes in the data is expressed as six in the file; four as eight, five as ten,
and so on. In this way, we can export *any* data that's in the table, no
matter how many commas, newlines or quotes are in it, to a file and have
it read back in without any ambiguity.

By way of example, let's add one more record to our birthdates and birthplaces
file, for someone called Charlie "Chuck" Bloggs. The file should look like this:

````
"Alice","Oxford",1982-05-10
"Bob","Reading, Berkshire",1980-01-01
"Fred","Swindon",1981-03-31
"Charlie ""Chuck"" Bloggs","Exeter",1978-04-18
````

Note how the quote characters that are part of the field are distinguished from
the quotes that start and end the string by the fact that they're repeated. When
the parser is inside a quoted string and it reads a quote, it looks at the next
character as well - if that's also a quote, both quotes are consumed and it's
taken as a quote that's part of the data; otherwise the quote is consumed and
it's taken as the end of the quoted string. 

#### Summary

To recap, CSV should be formatted as follows:

Records are terminated with line breaks.
Fields within a record are separated with commas.
Any field may be quoted; that is, enclosed within double-quote characters.
A field must be quoted if the field contains the field separator
character (comma), a line break, or a double-quote character.
Double-quote characters within a quoted field must be represented
as a pair of double-quote characters.

Reference implementation


#### Writing a CSV field

The following C function takes a string as an argument, formats it as a quoted
CSV field, and writes the result to standard output.

```
#include &lt;stdio.h&gt;

void
write_csv_field(const char *data) {
    /* Write opening quote */
    putchar('\"');

    /* Write each character in the string to standard output. But if we
       find a quote in the string, repeat it so it's not mistaken for a
       closing quote. */
    while (*data) {
        if (*data == '\"')
            putchar('\"');
        putchar(*data);

        ++data;
    }

    /* Write closing quote */
    putchar('\"');
}
```

Reading a CSV field


The following C function reads a single CSV field from standard input, and
writes the data contained in it to the buffer pointed to by dest, up to
dest_max bytes. It reads until it finds a field separator (specified by
field_sep) or record terminator (assumed to be a Unix or Windows line
break) which is not inside a quoted string. *stop is set to the
consumed character which caused the function to stop. This will be
field_sep, '\n' or EOF, indicating that we found the
end of the field, record or file respectively.

It returns the number of characters that would have been written to
dest (not including the trailing NUL character) had enough
space been available. A trailing NUL is always written unless
dest_max is 0.

```
#include &lt;stdio.h&gt;

int
read_csv_field(char *dest, int dest_max, char field_sep, int *stop) {
    int in_quote = 0;   // are we inside a quoted string?
    int c;              // character we've read
    int outchar;        // character we'll write to dest on each iteration
    int dest_len = 0;   // number of chars we've written to dest

    /* Prerequisites:
     *     field_sep is not 0
     *     if dest_max is greater than 0, dest points to a buffer to which at
     *        least dest_max bytes may be written
     *     stop points to valid memory
     */

    /* Keep *stop as 0 until we finish */
    *stop = 0;
    
    /* Read character by character until we reach the end of the file or
       we find a character we should stop on */
    while (*stop == 0 &amp;&amp; (c = getchar()) != EOF) {
        if (in_quote) {
            /* We're inside a quote: only stop if we see a closing quote */
            if (c == '\"') {
                /* Peek at the next character */
                int next = getchar();
                if (next == '\"') {
                    /* Literal quote */
                    outchar = '\"';
                }
                else {
                    /* It's a real closing quote.
                       Put the peeked character back. */
                    ungetc(next, stdin);

                    /* We're no longer in a quoted string, and don't write
                       this quote character out. */
                    in_quote = 0;
                    outchar = -1;
                }
            }
            else {
                /* Whatever this character is, it's inside a quote so
                   write it as-is. */
                outchar = c;
            }
        }
        else {
            /* Not inside a quoted string */
            if (c == '\"') {
                /* We're inside a quoted string now, and don't write this
                   quote character out. */
                in_quote = 1;
                outchar = -1;
            }
            else if (c == field_sep || c == '\n') {
                /* End of field or end of record. */
                outchar = -1;
                *stop = c;
            }
            else {
                /* Ordinary character. */
                outchar = c;
            }
        }

        /* If we have a character to write, and there's enough space,
           then write it. */
        if (outchar != -1) {
            if (dest_len &lt; dest_max) {
                dest[dest_len] = outchar;
            }
            dest_len++;
        }
    }

    if (c == EOF)
        *stop = EOF;

    if (dest_max &gt; 0) {
        /* If the last character in dest was \r and we stopped on \n, then
           remove the \r - this means we got a Windows newline, and the \r
           isn't really part of the data. */
        if (dest_len &gt; 0 &amp;&amp; dest[dest_len-1] == '\r' &amp;&amp; *stop == '\n') {
            dest_len--;
        }
           
        /* NUL-terminate dest. If we can't because there isn't enough space in
           dest, then put the NUL over the last character. */
        if (dest_len &lt; dest_max)
            dest[dest_len] = '\0';
        else
            dest[dest_max - 1] = '\0';
    }

    return dest_len;
}
```</t>
<t tx="jonathanhudson.20201007113221.1">@language md

Debugging WX2 crashes
1) Getting the core file information 

There are various tools that can be used to debug wx2 crashes. 

#### Normal Debugging

The preferred option is "wxdgtool -D" which generates info, history and wxd files, with the wxd file containing the relevant core files. If the system has a dedicated AP (e.g. TCG) you can usually do "wxdgtool -D" from there as APs tend to have lots of disk space which is what you will need, especially if one or more RS nodes have crashed. You can specify the -O option to force the output to a specific directory. 

#### What to do id no AP in the system

If no node in the system has a lot of disk space you can use ssh from another node which does have disk space e.g. here I'm using ngpc12 to dump off from a WX2 system which includes the node 10.6.7.62: 

```
ssh wcsadmin@10.6.7.62 wxdgtool -D -o - &gt; poc5-dgtool.wxd 
```

You can use 

```
wxdumpx -XF history &lt;wxd file&gt;
wxdumpx -XF info &lt;wxd file&gt;
```
to extract the history and info files from a wxd file obtained in this way. 

Sometimes 

```
wxdgtool -D
```

runs out of time and does not capture a full core file for any crashed process. You should see something like the following at the end of the output in this case: 

```
WARNING -- NO COMPLETE CRASHED CORES DUMPED!
Dump complete in 496 secs. 
```

You should use wxdgdump to extract at least one relevant core file if this happens. The syntax is: 

```
wxdgdump -t &lt;target host&gt; -p &lt;crashed process&gt; -C &lt;corefilename&gt; 
```

If there isn't enough space on any node that can run wxdgdump against the system to dump the file (e.g. it is an RS and there is no AP available), you can do: 

```
ssh user@victim wxdgtool -o - -c \'"target {172.99.xxx.xxx},pid xxx"\' 
-d &gt; /local/file/to/put/it/in 
```

Then use 

```
wxdumpx -Xc /local/file/to/put/it/in
```

to get the corefile out.

GETTING JUST AN INFO FILE 
To get an info file you would run 
wxdgtool -i 

GETTING JUST A HISTORY FILE 
To get a history file you would run 
wxdgtool -H 

2) OOM problems - when the Linux Out Of Memory killer gets in and kills off processes, including wxdb/wxsmd ones. 

Check /var/log/messages on all nodes to see if there have been any out of memory problems at the time of the problem (watch out for seeing lots of old OOM problems which aren't related to the current issue under investigation though). Something like the following should do the trick: 
wxtool -S 'grep Memory: /var/log/messages' 
You can then check it for the date you are interested in by doing something like: 
wxtool -S 'grep Memory: /var/log/messages | grep June'</t>
<t tx="jonathanhudson.20201007113949.1">All howto solutions in one page

HOWTO: How do I tell WX2 how much memory to use on each server?

HOWTO: How do I enable direct I/O on WX2 disk storage devices?

HOWTO: How do I add optimiser hints to WX2 SQL statements?

HOWTO: How do I update the statistics for all tables in a schema using one statement?

HOWTO: How do I select the top N values from a query?

HOWTO: How do I combine a date and time to create a timestamp?

HOWTO: How do I display a duration in HH:MM:SS format?

HOWTO: How can I quickly check the hardware configuration of all the blades in a WX2 system?

HOWTO: How can I check which tables have compressed indices on them?

HOWTO: How can I tell how much free memory is available in the WX2 server?

HOWTO: How do I copy a file to all blades simultaneously?

HOWTO: How can I execute a Linux command on all WX2 blades simultaneously?

HOWTO: How do I edit the WX2 global configuration file on all blades simultaneously?

HOWTO: How can I use a different editor such as emacs with wxviconf?

HOWTO: How do I make WX2 restart automatically in the event of a crash?

HOWTO: There does not seem to be a record of SQL statement start and end times in ‘ipe_command’. Is there a way of establishing these values?

HOWTO: How can I quickly check the overall status of the WX2 system?

HOWTO: I can’t seem to run more than N queries simultaneously in Kognitio. Is there a way of allowing more queries to run?

HOWTO: I can’t seem to have more than N simultaneous connections to WX2. Is there a way of allowing more connections?

HOWTO: How do I uninstall an old version of the WX2 software to free up space in the installation directory?

HOWTO: How do I abort a running query in WX2?

HOWTO: How do I pause and resume a query queue in WX2?

HOWTO: How can I list out the valid options for the wxserver command?

HOWTO: How can I check for data skew on a table-by-table basis?

HOWTO: How do I import a date using wxloader / wximport when the day or month components do not have leading zeroes?

HOWTO: How do I import only the date component of a combined date/time value using wxloader / wximport?

HOWTO: How do I import a compressed file using wximport?

HOWTO: How do I generate sequence numbers in WX2?

HOWTO: How can I restrict the amount of RAM that WX2 is allowed to use on each node?

HOWTO: How can I simulate the functionality of the Oracle dummy table DUAL in WX2?

HOWTO: How can I list out all parameters per user in WX2?

HOWTO: How can I monitor the status of the WX2 queues?

HOWTO: How do I pass values into wxsubmit variables from the command line?

HOWTO: How do I embed a single quote (') character in a SQL string?

HOWTO: How do I list the amount of temporary RAM used per session?

HOWTO: How do I insert the contents of a wxsubmit string variable into a table using insert?

HOWTO: How do I convert fixed-format files with no line ends such that line-oriented tools like wximport can handle them?

HOWTO: How do I check the return status of a query in a wxsubmit script and branch conditionally as a result?

HOWTO: How do I generate subsecond precision with current_time and current_timestamp?

HOWTO: How do I disable the use of SSL when connecting to the WX2 server?

HOWTO: How do I enable additional debug tracing output for the WX2 ODBC driver?

HOWTO: How do I apply a Kognitio patch release?

HOWTO: How do I stop or restart all SMDs simultaneously across all nodes?

HOWTO: How do I perform a reclaim on a particular disk slab?

HOWTO: How do I install just the WX2 client tools on a Linux or Solaris server?

HOWTO: How do I chain commands together in wxadmin?

HOWTO: How do I generate a random number in WX2?

HOWTO: How do I enable WX2 Software RAID-5?

HOWTO: How can I find out the total used and free disk space on a WX2 server?

HOWTO: How do I stop and start the SMD daemons on individual nodes?

HOWTO: How do I run a SQL query to reference a specific point-in-time?

HOWTO: How do I completely uninstall WX2?

HOWTO: How do I use SSH keys with Kognitio?

HOWTO: How do I restrict the number of concurrent queries that can be executed by a query queue?

HOWTO: How do I set a priority on a query in a queue?

HOWTO: How do I use 'wxbackup' to back up just the system metadata and not the user data?

HOWTO: How do I prevent users running theta joins?

HOWTO: How do I remove disk from a WX2 system?

HOWTO: How do I review issues with lock timeouts/deadlocks from log files?

HOWTO: How do I load tables from Access into WX2

HOWTO: How do I determine the disk usage in bytes by user and schema

HOWTO: How do I enable users to run commands like wxprobe

HOWTO: How do I run the shape tests?

HOWTO: How do I disable the RS on one node

HOWTO: How can I restrict user access to WX2?

HOWTO: How do I setup the recovery script wxmon.pl to cope with certain failures on WX2?

HOWTO: Install Perl DBI

HOWTO: How can I specify target strings for commands such as wxprobe, wxtool and wxsync?

HOWTO: How can I change the WX2 system_id?

HOWTO: How can I modify a dependent view definition if "create or replace view" fails

HOWTO: How do I restrict WX2 logins as a non-SYS user?

HOWTO: How do I identify how much disk space is reclaimable?

HOWTO: How do I permanently disable Linux/Solaris swap on WX2?

HOWTO: Test read/write disk performance

HOWTO: How do I grant a non-SYS user the privileges to change their default queue?

HOWTO: Turn off all output streams (except for result rows) in wxsubmit

HOWTO: View/restore historical data

HOWTO: Configure the 'pocadm' system for resilience testing

HOWTO: Install and test unixODBC

HOWTO: Perform a resilience test on pocadm

HOWTO: How to do a reconfigure

HOWTO: How do I change the timezone on Linux and WX2

HOWTO: How do I configure the management link to run on a different network

HOWTO: What to do if queries against ipe_command return a CG0214 error

HOWTO: resolving backup problems

HOWTO: use disk repair/check to fix disk corruption

HOWTO: Re-integrate a repaired node back into a WX2 database

HOWTO: how to resume an interrupted restore

HOWTO: how to deal with out of string space errors

HOWTO: recover disk space when reclaim gives DP0001 out-of-disk error

HOWTO: How do I configure WX2 to use additional network links for internal traffic?

HOWTO: Investigate an unabortable session

HOWTO: Run the disk soak test

HOWTO: How do I add or introduce a standby disk to an existing Kognitio instance?

HOWTO: How do I determine if Kognitio is down?

HOWTO: More information on collecting information after a DB crash/hang

HOWTO: Get started with backups

HOWTO: preceding query with "debug" to get timings, etc

HOWTO: Reclaim space from slab 2 (the logging slab)

HOWTO: How do I find header information from a row in the database?

HOWTO: How do I use the IPE_MPK* tables?

HOWTO: Generate a metadata backup from a full backup

HOWTO: Deal with CI0012 errors

HOWTO: How do I drop a queue from a KAP system

HOWTO: remove a node from a system for repair/diagnostics

HOWTO: Replace a faulty node in a Kognitio database

HOWTO: Change the port used by the Kognitio Database for ODBC connectivity

HOWTO: write efficient queries for Kognitio

HOWTO: set disk slab size

HOWTO: stop Kognitio from using a specific network interface for internal traffic

HOWTO: deal with RPC port clashes

HOWTO: Diagnosing timestamp problems on appliances

HOWTO: how do I remove a runtime parameter setting from the config file

HOWTO: How to collect the replay log for a query

HOWTO: reintegrate a repaired node into a system

HOWTO: Perform a node failure test on KAP

HOWTO: Reset an enclosure switch

HOWTO: How do I stop automatic restart of the KAP database

HOWTO: commission a system on Amazon Web Services / AWS

HOWTO: How do I tell WX2 how much memory to use on each server?

Solution 00000037 by Stuart Watt at 2012-03-21T16:30:13.000+0000

By default, WX2 will allocate all the free RAM on each server to the database system. If necessary, a fixed amount of memory can be ring-fenced for non-WX2 purposes by adding the following section to the global WX2 configuration file: 

[boot options] 
max_fixed_pool=&lt;memory size in bytes to be ring-fenced&gt;


 


If, however, a value below 100 is used it becomes a % of total system RAM so  'max_fixed_pool=25' will leave 25% of RAM for non-WX2 purposes.



HOWTO: How do I enable direct I/O on WX2 disk storage devices?

Solution 00000057 by Stuart Watt at 2008-01-29T16:35:31.000+0000

WX2 will use direct I/O on its disk devices by adding the following section to the global configuration file:


[disks]

direct_io=yes


Kognitio currently recommend enabling direct I/O in write-intensive environments and future versions of WX2 will have direct I/O enabled by default.


Please note that not all Linux filesystems support direct I/O.

HOWTO: How do I add optimiser hints to WX2 SQL statements?

Solution 00000069 by Stuart Watt at 2016-05-23T14:46:41.000+0000

From version 8 onwards, optimiser hints allow the distribution of an intermediate table to be specified. This allows users to override default query plans when they know specific information about an intermediate object; for example the object will be heavily skewed on an attribute, and so should be partially hashed rather than just hashed.


The distribution can be specified in four places:


Optimiser Hints After a Table Name
The table will be reshaped to the given distribution. For example:


SELECT * FROM bigtable b HASHED (id) RANDOM VALUES ((0))
JOIN lookup l ON b.id = l.id


This specifies that bigtable should be loaded with a hash-random distribution; Kognitio will then use a hash-replicated distribution for lookup to perform the join. 


Optimiser Hints after a Derived Table
The derived table will always be created as-is and placed in the given distribution.


SELECT postcode, SUM(cnt) FROM
(SELECT id, COUNT(*) cnt
FROM people GROUP BY 1) pcnt HASHED (id),
addresslookup a HASHED (id)
WHERE a.id = pcnt.id
GROUP BY 1
Redistribution will not take place if predicates can be pushed down into the derived table to simplify it; for example:
SELECT * FROM
(SELECT * FROM a WHERE a1=1) b HASHED (a1)
WHERE a1=0


In this case the optimiser will not redistribute A as the predicates "a1=1" and "a1=0" are contradictory and so an empty table can be used instead.


Optimiser Hints After a WITH Definition
WITH addresscount AS
(SELECT address, COUNT(*) AS cnt FROM addresses) REPLICATED
SELECT…


Any WITH clauses referred to more than once in a query will be instantiated in memory. If no memory distribution is specified for the WITH, the instantiation will be done using a random distribution. This change could lead to higher memory usage for a query, although that query will typically complete more quickly than before. To switch off the behaviour, set the ci_reuse_with parameter to 0.


Optimiser Hints after a VALUES Table Definition
SELECT * FROM a,
VALUES (1), (2), (3) AS b (b1)
HASHED (b1) REPLICATED VALUES ((1), (2))
WHERE…


Optimiser Hints: Caveats
Kognitio always ensures that the join will be evaluated correctly. For example, if the user tries to join a hashed table to a random table, Kognitio will change the distribution of one table to allow the join to be performed.


CREATE TABLE a (a1 int) DISK;
CREATE TABLE b (b1 int) DISK;
SELECT * FROM a HASHED(a1) JOIN b RANDOM ON a1=b1


In this case both tables will be hashed immediately; table B will not be created randomly and then hashed!


HOWTO: How do I update the statistics for all tables in a schema using one statement?

Solution 00000073 by Stuart Watt at 2008-01-30T10:01:14.000+0000

It is not currently possible in WX2 to update the statistics for all tables in a schema using one statement. Separate ‘update statistics for…’ statements need to be run for each table in turn.

HOWTO: How do I select the top N values from a query?

Solution 00000074 by Stuart Watt at 2008-01-30T10:49:41.000+0000

As of WX2 v6.1.6, the 'select top N'  clause can be used as follows to find the five largest sales values:


select top 5 sale_value from sales order by sale_value desc;


This is equivalent to the following which uses SQL windowing functions instead:


select

  sale_value

from

  (select        

     sale_value,

     row_number() over (order by sale_value desc) ranking

   from sales

  ) dt

where ranking&lt;=5

order by sale_value desc;

HOWTO: How do I combine a date and time to create a timestamp?

Solution 00000075 by Stuart Watt at 2008-01-30T10:52:39.000+0000

Assuming the following table definition:


create table tab(startdate date, starttime time)


a timestamp can be created as follows:


timestamp(tab.startdate,tab.starttime)


A timestamp can also created from date and time constants:


timestamp(date ‘2006-02-27’,time ’09:55:41’)

HOWTO: How do I display a duration in HH:MM:SS format?

Solution 00000076 by Stuart Watt at 2008-01-30T10:55:11.000+0000

An expression of the form:


time '00:00:00'+cast(&lt;duration&gt; as interval second(8))


is required.


For example, to display start and end timestamps, along with the duration between the two:


select

  timestamp(t.sdate,t.stime) as startdate,

  timestamp(t.edate,t.etime) as enddate,

  time '00:00:00'+

    cast(timestamp(t.edate,t.etime)-timestamp(t.sdate,t.stime) as interval second(8)) as dur

from tab t;

HOWTO: How can I quickly check the hardware configuration of all the blades in a WX2 system?

Solution 00000085 by Stuart Watt at 2008-01-30T11:30:30.000+0000

Use the ‘wxprobe –H’ command, which produces output similar to the following:


Kognitio WX2 Hardware Discovery Tool v6.01.05-g on xxxx

(c)Copyright Kognitio Ltd 2001-2007.


WX2 system has: 58 nodes in 8 groups.

Disk resources: 7840GB in 56 disks.

System has 4 unique types of node.

System has 1 unique type of disk.

System RAM 898121MB, 890344MB for data processing.

224 CPUs available for data processing.


Detected node classes:

   cp: 2 nodes

   full: 56 nodes


Detected Operating platforms:

   Linux-2.6.16.21-0.8-bigsmp: 1 node

   Linux-2.6.16.53-0.16-smp: 56 nodes

   Linux-2.6.5-7.111.30-bigsmp: 1 node

HOWTO: How can I check which tables have compressed indices on them?

Solution 00000086 by Stuart Watt at 2008-01-30T11:32:30.000+0000

select b.name,

       c.name

from sys.ipe_allcol_img a,

     sys.ipe_alltable b,

     sys.ipe_allcolumn c

where a.table_id=b.id and

      b.id=c.table_id and

      a.seq=c.seq and

      a.indexed=1

HOWTO: How can I tell how much free memory is available in the WX2 server?

Solution 00000087 by Stuart Watt at 2016-05-23T11:45:30.000+0000

If using Kognitio Console, pressing ctrl+F2 in a query tab should bring up the relevant SQL, which can then be executed.  From wxsubmit, run '$f2;'In either case, the relevant SQL is:

select
cast(to_char(st.usedgb+freegb,'999,990.9') as varchar(9)) as "RAM GB",
cast(to_char(st.usedgb,'999,990.9') as varchar(9)) as "Used GB",
cast(to_char(st.freegb,'999,990.9') as varchar(9)) as "Free GB",
cast(to_char(st.availgb,'999,990.9') as varchar(9)) as "Avail GB",
cast(to_char((st.usedgb*100)/(st.usedgb+st.freegb),'990.9') || '%' as varchar(6)) as "% Used",
cast(to_char(((st.freegb-st.availgb)*100)/st.freegb,'990.9') || '%' as varchar(6)) as "% UnFr",
cast(to_char(st.rssizegb,'999,990.9') as varchar(9)) as "RS GB",
cast(to_char(st.rsno,'9,999') as varchar(5)) as "RS No"
from
(select
    sum(pr.data_stored)/power(1024,3) as usedgb,
    sum(pr.ram_free)/power(1024,3) as freegb,
   (min(pr.ram_free)*count(pr.mpid))/power(1024,3) as availgb,
    avg(pr.ram_size)/power(1024,3) as rssizegb,
    count(pr.mpid) as rsno
 from sys.ipe_process pr
where pr.type='rs') st(usedgb,freegb,availgb,rssizegb,rsno);






HOWTO: How do I copy a file to all blades simultaneously?

Solution 00000088 by Stuart Watt at 2008-01-30T11:36:17.000+0000

wxsync -S &lt;filename&gt;

HOWTO: How can I execute a Linux command on all WX2 blades simultaneously?

Solution 00000089 by Stuart Watt at 2008-01-30T11:38:39.000+0000

wxtool –E ‘&lt;command&gt;’


or


wxtool -S '&lt;shell command&gt;'

HOWTO: How do I edit the WX2 global configuration file on all blades simultaneously?

Solution 00000090 by Stuart Watt at 2008-01-30T11:45:14.000+0000

Use the command ‘wxviconf’, which will open a copy of the WX2 global configuration file in ‘vi’ by default and then copy the resultant file to all blades after the alterations have been saved.


Please see "How can I use a different editor such as emacs with wxviconf?" about the use of an alternative editor with 'wxviconf'.

HOWTO: How can I use a different editor such as emacs with wxviconf?

Solution 00000091 by Stuart Watt at 2008-01-30T11:44:02.000+0000

In the WX2 global configuration file, add the following section:


[wxconftool]

config_editor=/usr/bin/emacs


replacing ‘/usr/bin/emacs’ with the appropriate path to the chosen editor.

HOWTO: How do I make WX2 restart automatically in the event of a crash?

Solution 00000096 by Stuart Watt at 2016-05-23T12:03:37.000+0000

There are two methods. The first is to enable reliability features in the [wxsmd] section of the config file:


[wxsmd]
reliability_features=yes



The second method is to add the following to the config file under [wxsmd events]:


[wxsmd events]

on_server_crash=exec /tmp/serverdown


then create a file /tmp/serverdown on one of the blades that looks something like this:


#!/bin/sh

/opt/kognitio/wx2/current/bin/wxdgtool -i -o /tmp/wx2_crashinfo_`date +%F`_`date +%T`.txt

wxserver start


And finally synchronise the file /tmp/serverdown across all the blades with:

wxsync -S /tmp/serverdown


Whichever method is followed, in the event of a crash the master smd daemon should dump off a small amount of info just specifying which nodes crashed, but not anything large such as core files. Then the server should restart automatically.

HOWTO: There does not seem to be a record of SQL statement start and end times in ‘ipe_command’. Is there a way of establishing these values?

Solution 00000101 by Stuart Watt at 2008-01-30T13:26:55.000+0000

The WX2 system table ‘ipe_transaction’ records the start and end times of all transactions and is persistent across database restarts. When running in auto-commit mode, which is the default, each statement in ‘ipe_command’ is also a transaction in ‘ipe_transaction’, thus allowing start and end times to be established.

The ‘ipe_transaction’ table has two entries per transaction, one representing the start time and one representing the end time. To establish the start and end times of SQL statements in ‘ipe_command’, a SQL statement similar to the following can be used:

select
  ts.session_id,
  ts.user_id,
  timestamp(ts.tdate,ts.ttime) as startdate,
  timestamp(te.tdate,te.ttime) as enddate,
  time '00:00:00'+cast(timestamp(te.tdate,te.ttime)-timestamp(ts.tdate,ts.ttime) as interval second(8)) as duration,
  case c.status
    when 0 then 'OK'
           else 'Error'
  end as outcome,
  c.command
from
   ipe_transaction ts
 inner join
   ipe_command c
 on ts.tno=c.tno and ts.operation=2
 inner join
   ipe_transaction te
 on te.tno=ts.tno and te.operation in (3,4)
order by 1,2,3

HOWTO: How can I quickly check the overall status of the WX2 system?

Solution 00000108 by Stuart Watt at 2008-01-30T13:44:53.000+0000

The ‘wxprobe’ command will return 'No problems found.’ if everything is OK, otherwise the relevant WX2 error messages will be displayed.

HOWTO: I can’t seem to run more than N queries simultaneously in Kognitio. Is there a way of allowing more queries to run?

Solution 00000110 by Stuart Watt at 2012-08-28T08:58:33.000+0000

The number of simultaneously executing queries in Kognitio can be constrained by a couple of factors:


1. The maximum number of concurrent queries (maxrun) allowed for a given query queue. The WX2 Guide explains how queues are used to control concurrent access, and how they can be configured. 

2. A previously compiled query in Kognitio is actually executed by an interpreter process, which is exclusively allocated to that transaction for the transaction's duration. Therefore, if there are N interpreters, then there can only be N concurrent transactions. The number of allocated interpreters can be controlled by an entry in the Kognitio global configuration file via the following setting (note that a server restart is required for the setting to take effect):

[boot options]

num_int_nodes=&lt;maximum number of concurrent queries&gt;


It is also possible to increase the number of compilers for compiling queries, and the number of client connections allowed per Kognitio nodes by setting the num_comp_nodes and sessions_per_cp parameters respectively.

Note that it is possible to detect that the server has run out of interpreters by searching `wxlogd smd`/serverdbg* on a Kognitio node for the string "more interpreters required". If the server has run out of interpreters you will see output like the following:

...
28-08_11:19:12_UTC: WARNING: more interpreters required
as 32 queued now



28-08_11:19:13_UTC: WARNING: more interpreters required
as 33 queued now



28-08_11:19:20_UTC: WARNING: more interpreters required
as 34 queued now

...





It is often the case that modifying client applications is the way to resolve this problem, rather than increasing the number of interpreters in the system. 

For example
•ensuring that clients do not run lots of concurrent transactions if not required.
•ensuring that clients do not leave transactions open after they have finished retrieving results.
•using security classes to abort idle sessions (see the documentation for further details).


HOWTO: I can’t seem to have more than N simultaneous connections to WX2. Is there a way of allowing more connections?

Solution 00000111 by Stuart Watt at 2008-02-04T11:33:50.000+0000

The number of connections is, in part, controlled by an entry in the WX2 global configuration file as follows:


[boot options]

sessions_per_cp=&lt;up to 58 concurrent connections per node&gt;


This will set the number of Association Managers (AMs or sessions) per I/O process (per CP), which controls how many connections can be made to that I/O process.

HOWTO: How do I uninstall an old version of the WX2 software to free up space in the installation directory?

Solution 00000120 by Stuart Watt at 2016-05-19T16:09:42.000+0000

It is possible to uninstall previous versions of the WX2 software using 'wxserver uninstall &lt;version directory&gt;'. 


run 'wxserver info versions' to see a list of installed versions:

wxadmin wxadmin@wxsupp03-wx2-rack1-enc2-13:~&gt; wxserver info versions
Kognitio WX2 System Controller v8.01.00-rel160415 on wxsupp03
(c)Copyright Kognitio Ltd 2001-2016.
ver80100rel150916: state Installed.
ver80100rel151201: state Installed.
ver80100rel160218: state Installed.
...



Then run 'wxserver uninstall &lt;version&gt;', for example :


wxadmin wxadmin@wxsupp03-wx2-rack1-enc2-13:~&gt; wxserver uninstall ver80100rel151201





HOWTO: How do I abort a running query in WX2?

Solution 00000121 by Stuart Watt at 2008-01-31T15:54:28.000+0000

To abort a query running on the WX2 server, run the following command from another session:


update sys.ipe_cursessions set abort=1 where session=&lt;session_id&gt;


or


update sys.ipe_cursessions set abort=1 where last_command like '%&lt;command_you_want_to_abort&gt;%'


to avoid the need for the session_id.


An entire session can be aborted by using 'abort=2' instead in the queries above.

HOWTO: How do I pause and resume a query queue in WX2?

Solution 00000123 by Stuart Watt at 2008-01-30T16:34:31.000+0000

To pause and resume a query queue, run the following commands as the SYS user:


  - Pause: set parameter q&lt;queue_number&gt;_pause to 1

  - Resume: set parameter q&lt;queue_number&gt;_pause to 0


Example: set parameter q5_pause to 1 will pause queue number 5.

HOWTO: How can I list out the valid options for the wxserver command?

Solution 00000125 by Stuart Watt at 2015-04-29T09:44:07.000+0000

Using the command 'wxserver syn', output similar to the following will be produced:


Kognitio WX2 System Controller v6.01.05-g on xxxx

(c)Copyright Kognitio Ltd 2001-2007.


info | set | force | wait | background | bg | start | halt | stop | push | 

pop | upgrade | smd | install | inst | uninstall | uinst | config | 

hwmap | syn | abort | daemon | syntax | dumplist | dump | monitor | quit | 

exit | &lt;Complete command&gt;



Using the command 'wxserver -h', the following output will be produced:


&lt;options&gt; are:


Targeting arguments
-s &lt;server&gt;     :       Connect to &lt;server&gt; (default localhost:3000)
-S &lt;sysid&gt;      :       Connect to server with &lt;systemid&gt; (only works on a cp)


Command options
-i              :       Start an interactive SMD session
-x &lt;command&gt;    :       Execute &lt;command&gt;


Display arguments
-R              :       Raw connection with smd
-G              :       Show global output
-A              :       Show alerts
-U              :       Show state changes
-l &lt;num&gt;        :       Set alert level





HOWTO: How can I check for data skew on a table-by-table basis?

Solution 00000126 by Stuart Watt at 2008-01-30T17:07:43.000+0000

You can query 'ipe_ram_images' which has a row for each table and RAMstore (RS) combination, so for a given table there will be one row per RS. You can then look at the discrepancy between minimum and maximum RAM used to see potential skew.


The following SQL statement should be helpful in looking for skew:


select * from (select table_id, (max(ram_used) / min(ram_used)) skew from sys.ipe_ram_images group by 1) dt where skew &gt; 1.3 order by 2 desc

HOWTO: How do I import a date using wxloader / wximport when the day or month components do not have leading zeroes?

Solution 00000127 by Stuart Watt at 2016-05-25T14:09:14.000+0000

Note: wximport is deprecated in version 8. Wxloader should be used instead if possible.



In wxloader:


To import a date of the form '2/3/2008' use the 'D' and 'M' date formats as follows: 

-D 'D/M/YYYY'



In wximport:



To import a date of the form '2/3/2008' use the 'D' and 'M' date formats as follows:


-fd "D/M/YYYY'


For the full list of allowable date formats, see "FAQ: What date formats do wxloader/wxunloader/wximport/wxexport support?"

HOWTO: How do I import only the date component of a combined date/time value using wxloader / wximport?

Solution 00000128 by Stuart Watt at 2016-05-25T14:13:28.000+0000

note: wximport is deprecated in version 8. Use wxloader instead if available.



with wxloader:


To import only the date component of a combined date/time value such as '2008-03-05 08:11:04', use the '?' date format as follows:


-D "YYYY-MM-DD ??:??:??"


with wximport:

To import only the date component of a combined date/time value such as '2008-03-05 08:11:04', use the '?' date format as follows: 

-fd "YYYY-MM-DD ??:??:??" 


For the full list of allowable date formats, see "FAQ: What date formats do wxloader/wxunloader/wximport/wxexport support?"


HOWTO: How do I import a compressed file using wximport?

Solution 00000129 by Stuart Watt at 2008-01-31T09:49:47.000+0000

In the example below, a source file that has been compressed using 'gzip' is fed into 'wximport' via a pipe:


gzip -dc source_file.txt.gz | wximport -s myserver myuser -p mypass -f csv -t -ro -cs '|' -nd -opt -d - mytable


The key point is that the filename in wximport is replaced by a '-' to show that its input should be taken from a pipe/stdin.

HOWTO: How do I generate sequence numbers in WX2?

Solution 00000135 by Stuart Watt at 2008-01-31T11:35:53.000+0000

WX2 has a function called 'generate_key' which generates a unique key for each row in a query. The results are not typically contiguous, but are guaranteed to be unique for an individual SELECT statement.

From the SQL Guide:

GENERATE_KEY
For an individual SELECT statement the GENERATE_KEY function generates a unique key for each row.

Usage
GENERATE_KEY()

Notes
The result is an INT8 data type with a value greater than or equal to zero. No parameters are required, but you must enter the opening and closing parentheses.

The results are not typically contiguous, but are guaranteed to be unique for an individual SELECT STATEMENT.

GENERATE_KEY can only be used in the SELECT list, it cannot be used in WHERE, HAVING, GROUP BY or ORDER BY clauses.

Within the SELECT list you can perform arithmetic on the columns containing the GENERATE_KEY function.

Multiple occurrences of GENERATE_KEY in a SELECT list will all produce the same result within a single row.

The values generated are dependent upon the number of WX2 nodes and the distribution of the data. Rerunning a query may not generate the same results.

The function is primarily provided to help support WX2 ETL solutions.

Example 1- Generate key during INSERT-SELECT
Here we create a table with a key column and insert data into it.

CREATE TABLE keyed_telco_data FROM 
    SELECT GENERATE_KEY() k, t.* 
    FROM telco_data t

HOWTO: How can I restrict the amount of RAM that WX2 is allowed to use on each node?

Solution 00000136 by Stuart Watt at 2008-01-31T12:05:30.000+0000

Edit the WX2 configuration file as follows:


[system]

memsize=0x400000000


In this example, the memsize setting will make WX2 behave as though only 16GB of RAM is available to it. Running 'wxprobe -H' (or -HN) should show the node reporting itself as a 16GB node.


This setting can be applied to just one node by editing its local configuration file or to all nodes by editing the global configuration file. 


Common values are:


0x100000000 - 4GB

0x200000000 - 8GB

0x300000000 - 12GB

0x400000000 - 16GB

HOWTO: How can I simulate the functionality of the Oracle dummy table DUAL in WX2?

Solution 00000140 by Stuart Watt at 2009-01-05T14:37:43.000+0000

-- This script creates a WX2 table that simulates the functionality

-- of the 'DUAL' dummy table in Oracle.


drop table dual;

create table dual

as

select

  'X' as dummy

from sys.ipe_system;

HOWTO: How can I list out all parameters per user in WX2?

Solution 00000141 by Stuart Watt at 2008-01-31T13:14:57.000+0000

Run the following command as SYS:


select u.name, u.id, p.pname, p.user_id, p.session_id, p.pvalue

from ipe_alluser u, ipe_all_param p 

where u.id = p.user_id

order by u.name

HOWTO: How can I monitor the status of the WX2 queues?

Solution 00000142 by Stuart Watt at 2016-06-07T09:23:38.000+0000

The status of queues can be examined using the 'sys.ipe_query_queues' and 'sys.ipe_query_queue_stats' virtual tables.


ipe_query_queues has an entry for each current query associated with a queue, indicating items such as:

* its position in the queue

* whether it is running (as opposed to queued)

* how long it was queued for

* how long it has been running to date.


ipe_query_queue_stats has an entry per queue, indicating items such as:

* how many queries are on that queue

* how many are running and how many are queued

* how many queries have been through the queue since server start time

* how many queries are runnable at one time through the queue

* the load over the last 5 minutes, 1 hour, and 5 hours, expressed in terms of the average length of the queue including running queries over that period (averages calculated in the same way as UNIX-like load averages)

* the average queue time in seconds for the last 10, 50 and 100 queries

* the average run time in seconds for the last 10, 50 and 100 queries


In addition, 'select * from sys.ipe_cursessions where command_running=4' will also display queued queries.

HOWTO: How do I pass values into wxsubmit variables from the command line?

Solution 00000143 by Stuart Watt at 2008-01-31T13:28:25.000+0000

If in the command shell:

export var1=ldate 

export var2=ltime 


then the following command in wxsubmit:

select $var1, $var2 from ipe_login 


would be translated to:

select ldate, ltime from ipe_login


The lack of any 'set var' for $var1 and $var2 means that wxsubmit would retrieve the values of $var1 and $var2 from the shell's environment variable list instead.


This can also be done in a single line as follows:

var1=ltime var2=ldate wxsubmit -s myserver myuser -p mypass myscript.sql 


where myscript.sql contains references to $var1 and $var2 as appropriate.

HOWTO: How do I embed a single quote (') character in a SQL string?

Solution 00000146 by Stuart Watt at 2008-01-31T13:49:52.000+0000

The single quote character needs to be doubled-up as follows:


insert into mytable values ('Benny''s')

HOWTO: How do I list the amount of temporary RAM used per session?

Solution 00000147 by Stuart Watt at 2014-10-17T11:06:15.000+0000

The following query will work with version 7, 7.1, 7.2, and 8.1:

select cs.session, sum(ram_used) from sys.ipe_ram_images ri,

sys.ipe_cursessions cs, sys.ipe_transaction t

where cs.session = t.session_id

and t.tno = crtrans

and t.operation = 2

and table_id not in (select id from sys.ipe_table)

and (table_id &lt; 10000000 or table_id &gt;= 20000000)

group by 1

order by 2 desc

at full_history


System-created temporary tables have IDs above 20,000,000 whereas view images are between 10,000,000 and 20,000,000 (up to and including version 8.1 of Kognitio software).

HOWTO: How do I insert the contents of a wxsubmit string variable into a table using insert?

Solution 00000150 by Stuart Watt at 2008-01-31T14:26:42.000+0000

wxsubmit will not evaluate a variable if it is in single quotes hence:


insert into mylog values('$mystring')


will actually insert '$mystring' into the table and not the current value of the $mystring variable.


To work around this, do the following:


set var sq ';

insert into mylog values(${sq}${mystring}${sq});

HOWTO: How do I convert fixed-format files with no line ends such that line-oriented tools like wximport can handle them?

Solution 00000151 by Stuart Watt at 2016-05-24T14:24:12.000+0000

Note: wximport has been superceded by wxloader as of version 8. wxloader should be used instead, if available.


For fixed-format data files with no line ends, i.e. one continual stream of data, which need to be turned into 'normal' fixed format files with a line end every n characters, use:


fold -b -w &lt;line-length&gt; &lt;input-filename&gt;  &gt;  &lt;output-filename&gt;

HOWTO: How do I check the return status of a query in a wxsubmit script and branch conditionally as a result?

Solution 00000152 by Stuart Watt at 2008-01-31T14:45:32.000+0000

select * from mytab1;

if wcserror = ok goto nextquery;

if wcserror &lt;&gt; ok quit 5;

 

nextquery: 

select * from mytab2;

if wcserror = ok quit 0;

if wcserror &lt;&gt; ok quit 5;

HOWTO: How do I generate subsecond precision with current_time and current_timestamp?

Solution 00000158 by Stuart Watt at 2008-01-31T15:24:20.000+0000

Use current_time(p) and current_timestamp(p) where the precision p can be between 0 and 5. The default is a precision of 0.


Although WX2 will accept values of p between 0 and 5, currently only the first 2 digits of precision are significant, hence:


current_timestamp - 2008-01-31 15:16:18

current_timestamp(0) - 2008-01-31 15:16:18

current_timestamp(1) - 2008-01-31 15:16:18.4

current_timestamp(2) - 2008-01-31 15:16:18.45

current_timestamp(3) - 2008-01-31 15:16:18.450

current_timestamp(4) - 2008-01-31 15:16:18.4500

current_timestamp(5) - 2008-01-31 15:16:18.45000

HOWTO: How do I disable the use of SSL when connecting to the WX2 server?

Solution 00000159 by Stuart Watt at 2008-07-25T09:53:23.000+0000

You can disable SSL in the client by adding the following section to the odbc.ini file that the ODBC driver is using:

[ODBC]
SSLEnabled=N

Note this is likely to significantly improve the performance of operations which transfer a lot of data between the WX2 server and any client.

HOWTO: How do I enable additional debug tracing output for the WX2 ODBC driver?

Solution 00000167 by Stuart Watt at 2011-01-28T11:54:38.000+0000


If requested by WX2 Support, set the following parameters in the WX2 odbc.ini section in the Windows registry, or edit the Linux/Solaris odbc.ini file, under the entry for the WX2 system that needs to be traced:


Under Windows:

[&lt;server_name&gt;]

WX2ODBCDbg=1

WX2ODBCDbgFilename=c:\wx2odbc.log


Under Linux/Solaris:


[&lt;server_name&gt;]

WX2ODBCDbg=Y

WX2ODBCDbgFilename=/tmp/wx2odbc.log


If the WX2ODBCDbgFilename is not specified, it defaults to c:\wx2odbc.log on Windows, and /tmp/wx2odbc.log on Linux.

On Linux/Solaris, usually the odbc.ini file is found in /opt/kognitio/wx2/etc, but check to see where the variable WCS points to first.

NOTE : The above entries are case sensitive, please enter the above exactly as you see them


HOWTO: How do I apply a Kognitio patch release?

Solution 00000168 by Stuart Watt at 2012-08-13T12:02:30.000+0000

Periodically Kognitio
generate patch releases containing bug fixes, these releases will not be
required to be applied by all users. 

A patch release can be applied to a system
if the release number (e.g. 7.2.1) remains the same, but the label after the release
number has changed. Patch builds are standalone, complete releases and do not
depend on the previous version installed on the machine. Patch builds are
applied with the following syntax:
1.wxserver install &lt;pathname to patch file *.wxpkg&gt;
2.wxserver set current_version &lt;patch version - e.g. ver70201rel120803&gt;
3.wxserver smd all restart
4.wxserver start


HOWTO: How do I stop or restart all SMDs simultaneously across all nodes?

Solution 00000169 by Stuart Watt at 2008-02-01T15:20:21.000+0000

wxserver smd all restart | exit


Note: It is not possible to use 'wxserver smd' to start all SMDs simultaneously, as 'wxserver' requires the SMDs to already be present in order to function. In these circumstances, it will be necessary to use 'wxsvc' on each node individually. See "HOWTO: How do I stop and start the SMD daemons on individual nodes?"

HOWTO: How do I perform a reclaim on a particular disk slab?

Solution 00000171 by Stuart Watt at 2008-02-01T11:13:05.000+0000

reclaim for partition &lt;space separated list of slab ids&gt; to now

HOWTO: How do I install just the WX2 client tools on a Linux or Solaris server?

Solution 00000172 by Stuart Watt at 2012-02-03T11:58:31.000+0000

LINUX install


The tar file 'wx2-linux-clients.tar.gz' containing just the WX2 client tools and ODBC driver for Linux can be used to perform a basic client install:


1. Create the directory '/opt/kognitio/wx2/current' on the client;

2. Execute 'tar -C /opt/kognitio/wx2/current -xzf wx2-linux-clients.tar.gz' on the client.


It will then be necessary to set up an odbc.ini file. A template file can be found in '/opt/kognitio/wx2/current/etc' . Then set the WCS environment variable to point to the directory containing this file.


Man pages for 'wximport', 'wxexport' and 'wxsubmit' can also be made available by executing 'export MANPATH=$MANPATH:/opt/kognitio/wx2/current/man'.


Note that unlike the full server installation, there can only ever be one version of the client files and they are never updated as part of a server upgrade elsewhere.


SOLARIS Install


As above but using 'wx2-solaris86-clients.tar.gz' instead of 'wx2-linux-clients.tar.gz' 



Search for 'downloads' in Solutions to find the latest Linux and Solaris install files.

HOWTO: How do I chain commands together in wxadmin?

Solution 00000174 by Stuart Watt at 2008-02-01T12:09:02.000+0000

The way to chain commands together in 'wxadmin' is to use 7+10 rather than 7 10. You can then pass parameters to a particular step by leaving a space between the step and its parameter.


It's also recommended that the name of the step rather than the number is used, to avoid problems should the numbering change in future e.g. "server+newsys hardtoguesspassword"

HOWTO: How do I generate a random number in WX2?

Solution 00000182 by Stuart Watt at 2015-10-16T11:55:21.000+0000

There is no random number function in WX2, as its functions need to be deterministic, i.e. if you evaluate a function multiple times for one row, it must always return the same result.


Possible workarounds:

1. Use the 'hash_value' function to generate something that looks random. To sample 10% of customers, for example, use 'where hash_value(customer_id) %10 = 3' or similar;


2. Order by irrelevant fields and use 'row_number', e.g. to get 10% of the results, use 'select * from (select row_number() over (order by &lt;some fields&gt; x, * from mytab) dt where x mod 10=3'.

3. Generate a single random number between 0 and 100 with: "select mod(hash_value(current_timestamp(2)),101)"




HOWTO: How do I enable WX2 Software RAID-5?

Solution 00000186 by Stuart Watt at 2008-02-01T14:26:46.000+0000

WARNING: * Enabling Software RAID-5 will erase all the existing contents of the database *

In order to enable RAID-5 on WX2, it is necessary to edit the global configuration file and then re-initialise ('newsys') the database:

1.) Log on as the 'wxroot' WX2 administrative user;

2.) Run 'wxviconf', which will open a copy of the global configuration file. Any changes made to this file will automatically be replicated to all the nodes when 'wxviconf' exits;

3.) Add the following lines to the configuration file:

[boot options]
raid_cluster_size=4
virtual_diskstores=1

[wxsmd]
tolerate_changes=missing_node, missing_link, missing_disk

4.) Save and exit the editor in the normal way, which should result in:

Completed.
Restarting edited nodes.

5.) Run 'wxadmin', then choose 'option 7: server', then 'option 10: newsys'. Alternativey, run 'wxserver start newsys syspass ' from the command line.

At the end of this process, WX2 will be available again with RAID-5 enabled. The available disk capacity with have been reduced by some 25%.

HOWTO: How can I find out the total used and free disk space on a WX2 server?

Solution 00000189 by Stuart Watt at 2016-05-31T14:24:19.000+0000

In console, pressing ctrl+F1 in a query window will bring up the relevant SQL which can then be executed. From wxsubmit, run '$f4' as a query. In either case, the SQL which will be run is:


select
  cast(to_char(st.usedgb+st.freegb,'999,990.9') as varchar(9)) as "Disk GB",
  cast(to_char(st.usedgb,'999,990.9') as varchar(9)) as "Used GB",
  cast(to_char(st.freegb,'999,990.9') as varchar(9)) as "Free GB",
  cast(to_char(st.availgb,'999,990.9') as varchar(9)) as "Avail GB",
  cast(to_char((st.usedgb*100)/(st.usedgb+st.freegb),'990.9') || '%' as varchar(6)) as "% Used",
  cast(to_char(((st.freegb-st.availgb)*100)/st.freegb,'990.9') || '%' as varchar(6)) as "% Unav"
from
(select
    sum(xe.data_stored*xe.cu_size)/power(1024,3) as usedgb,
    sum(xe.free_space*xe.cu_size)/power(1024,3) as freegb,
   (min(xe.free_space*xe.cu_size)*count(xe.mpid))/power(1024,3) as availgb
 from sys.ipe_xor_element xe) st(usedgb,freegb,availgb);


HOWTO: How do I stop and start the SMD daemons on individual nodes?

Solution 00000190 by Stuart Watt at 2008-02-01T15:22:15.000+0000

wxsvc stop | start | restart


To stop or restart all SMDs simultaneously, see "HOWTO: How do I stop or restart all SMDs simultaneously across all nodes?"

HOWTO: How do I run a SQL query to reference a specific point-in-time?

Solution 00000194 by Stuart Watt at 2016-05-31T14:46:33.000+0000

Example:


select * from t1 where

wx_create_tno() &lt;

(select tno from sys.ipe_transaction where …) at full_history;

Every row in every table has an associated transaction number (tno) for the transaction that created it and an associated tno for the transaction that updated or deleted it. Transaction numbers are sequential, low numbers are old, high numbers are young. The wx_create_tno() and wx_update_tno() functions allow the row transaction numbers to be exposed and addressed. The ‘at full_history’ modifier forces all rows historical and current to be addressed by the query.


Qualifying in time requires a suitable tno-to-time reference. Transactions are logged in the system table IPE_TRANSACTION with an associated timestamp so all tnos are time referenceable. An application could also manage a tno reference lookup table by triggering unique identifiable commands at appropriate marker times (e.g. start of day, start of process etc.) and copy the associated tnos into a reference table for later use.

HOWTO: How do I completely uninstall WX2?

Solution 00000198 by Stuart Watt at 2015-06-23T10:29:57.000+0000

This is achieved by logging on to each node, including the AP, and running the following as root:

/opt/kognitio/wx2/current/bin/wxinstaller -D


This will remove the wxroot and wxadmin accounts, the /opt/kognitio/wx2 tree and anything associated with autostart. Only items that were created during an install will be removed, so if wxadmin existed before the install it will remain after the uninstall.


Note that the uninstall does not affect the disk resources themselves.

HOWTO: How do I use SSH keys with Kognitio?

Solution 00000199 by Stuart Watt at 2017-02-21T09:07:52.000+0000

There are several form posts covering this topic which can be found here at http://kognitio.com/forums/viewtopic.php?f=4&amp;t=52#p92




Specifically http://www.kognitio.com/forums/Passwordless%20authentication%20with%20KAP.pdf explains configuration prerequisites for using passwordless authentication, and covers other details relevant to this topic.









HOWTO: How do I restrict the number of concurrent queries that can be executed by a query queue?

Solution 00000205 by Stuart Watt at 2016-06-06T14:51:33.000+0000

Run the following command as SYS:


alter queue &lt;queue-name&gt; set maxrun to &lt;number&gt;

Example


alter queue q8 set maxrun to 4

will restrict query queue q8 to four concurrently executing queries.



HOWTO: How do I set a priority on a query in a queue?

Solution 00000206 by Stuart Watt at 2012-04-04T15:45:54.000+0000

It is possible to alter the priority of a query when it is submitted to a queue, which means that it may be inserted into the queue list above/below existing entries. By default, all entries are priority 100, so setting a higher priority of, say 50, will place a query above all existing priority 100 queries in the queue. 

The priority can be set for the current session: 

set current_session parameter queue_pri to 50; 

or for a user: 

set user parameter queue_pri to 50; 

Setting the priority at the user level may only be useful if there are two or more users associated with one queue, as WX2 does not look at priorities across different queues. However, it is possible to do this: 

create user qnormal; 
set user parameter queue_num to 11; 
set user parameter queue_pri to 100; 

create user qnormalpri; 
set user parameter queue_num to 11; 
set user parameter queue_pri to 50; 

which would allow two priorities of jobs on the normal queue, depending on which username was used to submit the query.

HOWTO: How do I use 'wxbackup' to back up just the system metadata and not the user data?

Solution 00000208 by Stuart Watt at 2016-06-06T14:53:32.000+0000

To back up just the metadata, use the '-M 0' option for 'wxbackup'.

HOWTO: How do I prevent users running theta joins?

Solution 00000212 by Kognitio Support at 2008-02-05T09:43:44.000+0000

To prevent users from erroneously generating theta joins (also known as cartesian joins), you can set the error_on_theta parameter to 1. As with many parameters, this can be set for the entire system:


set parameter error_on_theta to 1


...or for a particular user when logged in as that user):


set user parameter error_on_theta to 1


...or by SYS for a particular user:


alter user fred set parameter error_on_theta to 1


...or for a session:


set current_session parameter error_on_theta to 1


The parameter prevents theta joins except in the case when one side of the join contains 0 or 1 rows.

HOWTO: How do I remove disk from a WX2 system?

Solution 00000213 by Kognitio Support at 2015-11-04T16:57:18.000+0000

It may occasionally be desirable to remove some nodes (and their associated WX2 disk storage) from a WX2 system.

Before starting, note that if software RAID is on, you can only remove a whole number of clusters from the system. So each cluster must either be removed completely or left alone. Also, a reconfigure will delete all compressed data maps along with their statistics. New statistics will need to be gathered, and data maps rebuilt once the reconfiguration is complete. In time, the reconfigure down operation will be supported from the wxadmin command line utility, but currently the method is defined as that below.  It is important to execute every step to ensure the process completes succesfully.

0) ensure there is going to be enough space on the reduced-size system for all the data. WX2 will check that there is sufficient total disk space, but it will not verify that e.g. tables/schemas assigned to specific slabs can be held on those slabs with the reduced number of disks.

1) identify the mpids of the disk stores controlling the disks to be removed. You can do this with "wxprobe -l", and identify the Diskstore processes on the nodes which are having disks removed.

2) set the login mode with "set parameter adm_login_mode to 5" so that only localhost sessions connecting as SYS can login. This will prevent sessions getting in and writing to disk after the reconfigure down has completed. 

3) ensure there are no other connections to the system and run "reconfigure down &lt;space-separated list of mpids from (1)&gt;". This should take an amount of time comparable to a reclaim on the same amount of data.

4) when the reconfigure finishes, do "wxserver halt" to stop the WX2 server software.

5) Ensure that the removed disk resources are not going to be used by WX2 - typically by stopping the smd on the nodes being removed by running "wxsvc stop" on each of those nodes. However, if only a subset of disks on each node is being removed, or if the nodes are going to remain as RAM-only nodes in the system, edit the local config file on each node and provide a list of the remaining disk resources in [system] with "partitions=&lt;comma separated list of remaining disk resources&gt;". 

6) "wxserver start" to bring the downsized system back up. This will automatically set adm_login_mode to 0 removing the restrictions set in step 2.

7) remove the nodes no longer being used / change their system_id and restart the smds.


8) After reconfiguring down, any relevant DSN's will need to be updated to remove the old IP addresses, for example odbc.ini on the AP.  


Caveat: you must be running at least version 6.0.3 to be able to perform a reconfigure down operation.

Note that reconfigure down removes historic information, so it is not possible to run an incremental backup after this operation until another full backup has been taken.





HOWTO: How do I review issues with lock timeouts/deadlocks from log files?

Solution 00000216 by David Wild at 2017-05-19T12:29:13.000+0000

This solution concentrates on reviewing locking problems that have occurred in the past. If you suspect there are current locking issues on your system which are preventing queries from making progress, search for "Investigating performance issues" in the support portal rather than using the contents of this solution.

There are two sorts of locking problems:


1) lock timeouts - these occur when a lock request has been pending for longer than the lock timeout period. The default lock timeout period is one day, but on some machines it is set to a much lower value e.g. 30 seconds. The "lkti" parameter specifies the lock timeout period in seconds. If a lock timeout occurs, the client receives the DA0003 lock timeout error code.


2) deadlocks - here, two sessions have conflicting locks. For example, session A has an exclusive table lock on T1 and is trying to get an exclusive table lock on T2, while session B is holding an exclusive table lock on T2. If B now tries to get an exclusive table lock on T1, the deadlock will be detected and one of the queries generating a lock request will get the deadlock error code, DA0005. In this simple case, only two sessions are involved, but clearly there could be many more.


In case (1), an entry will appear in SYS.IPE_ERRORLOG looking something like this:


Lock Manager: session 213669 transaction 452609 requested EX TAB on table 12039. Conflicts with session 213668 transaction 452608 SH TAB on table 12039 


Now it is possible to look in ipe_command to see which transactions were being run for the transactions listed above, and abort one of the sessions if appropriate, or wait for one session to complete then resubmit the query.


In case (2), the SYS.IPE_ERRORLOG contains an entry of the following form:


Lock Manager: session 63506 transaction 291265 requested SH TAB on table 3727. Deadlock detected: dependency cycle of sessions 63506, 62908.


In this case, look in ipe_command for commands run by transactions from the relevant sessions to see which queries caused the problems, and resolve as for case (1).


Note that locks are held for the duration of a transaction, so the completion of a command is not sufficient - the transaction must be rolled back/committed to free any locks held. Check in ipe_transaction to see if the relevant transaction has two entries - if it has only one it is still open and still holding locks.

HOWTO: How do I load tables from Access into WX2

Solution 00000218 by Kognitio Support at 2008-02-28T13:16:51.000+0000

Attached is a macro that automatically exports a create table script and data from an Access database.
 
The code figures out all the tables in the database and optionally does the following:
  Drop table script creation.
  Create table script creation. (tables are created on disk only)
  Create table image script creation.
  Export tables as csv with column headings.

There is also an import script that automatically imports all generated files into the WX2 - to use this you will need to change the connection details to match those of your WX2 server.
 
To use:
Open the Access database you want to export. 
Open the VB editor. 
Add the .bas file as a module (or create a new module and copy the text from the .bas file) 
If needed, edit the .bas file to change the path indicating where the files will be created. 
Run macro. 
Both .sql and .csv files will appear in the defined directory. 
Optionally run the drop script on WX2. 
Run the create table script on the WX2. 
Copy csv files and import script to a WX2 node. 
Run the import script. 
Run the create image script on the WX2.

Note: This is not a fully tested product.

Attachment: modExportTables.bas,

Attachment: import_to_wx2.sh,

HOWTO: How do I determine the disk usage in bytes by user and schema

Solution 00000221 by Kognitio Support at 2008-10-02T07:57:08.000+0000

As SYS, run:

select u.name username, s.name schemaname, 
sum(f.nrows * (24 + (4 * ((mins + 3) / 4)) + (4 * ((c + 31) / 32)))) / (1024.00 * 1024 * 1024) AS minsizeGB,
sum(f.nrows * (24 + (4 * ((maxs + 3) / 4)) + (4 * ((c + 31) / 32)))) / (1024.00 * 1024 * 1024) AS maxsizeGB
from ipe_alluser u, ipe_allschema s, ipe_alltable t, ipe_ftable f,
(SELECT 
  table_id, 
  SUM(CASE WHEN datatype = 209 AND numtype = 304 
           THEN scale + 8 
           ELSE length END),
  SUM(CASE WHEN datatype = 209 AND numtype = 304 
           THEN 8 
           ELSE length END), 
  COUNT(*) 
  FROM ipe_allcolumn  GROUP BY 1) AS x(t, maxs, mins, c)
where t.owner = u.id and t.schema_id = s.id and f.table_id = t.id and t.id = x.t
group by 1,2
order by 3 desc

This should give you a range because WX2 does not have a quick and easy way to deal with varchar data - so the minsize shows the value if all the varchar fields have 0 bytes of data in them, and the maxsize shows what the disk usage would be if all rows had all their varchars filled to capacity.

The query takes account of deleted records, but not truncated or dropped tables.

Alternatively, the following query should give a reasonable indication of how much space is used by assuming that each 8KB disk page contains data from only one table - effectively true for non-trivial tables with the exception of system logging tables. It also does account for dropped tables:

select trim(s.name) || '.' || trim(t.name) as name, f.table_id, sum(block_count * 8) / (1024.00 * 1024) as GBused
from ipe_ftable f left outer join  (ipe_alltable t join ipe_allschema s on t.schema_id = s.id) on f.table_id = t.id
group by 1,2
order by 3 desc

HOWTO: How do I enable users to run commands like wxprobe

Solution 00000229 by Kognitio Support at 2012-07-20T13:19:32.000+0000

To run commands like wxprobe the user must be in the wxadmin group at the time the system manager daemons (smds) were started.
By default, the wxadmin and wxroot users created at commission time are in this group, but other users need to be added by an administrator.

To do this, as root run the following command on nodes which need to have the new user running commands like wxprobe (typically this will be on APs only):


usermod -Gwxadmin &lt;username&gt;


...then restart the smds by issuing the following as wxadmin/wxroot/root:


wxserver smd all restart


Note that if the user running wxprobe is not in the wxadmin group when the smds were last started, attempts to run wxprobe will typically result in the following:

FATAL ERROR: Timeout on mop packet - aborted before operation.

Hwmap not collected due to error.




HOWTO: How do I run the shape tests?

Solution 00000235 by Kognitio Support at 2015-10-19T08:07:13.000+0000

initiating a shape test




A good default for running shape tests is:

wxtester -s DSN -u USER -p PASSWORD -Ishape 5000 5000 1

 

(note that the option used in the command line above is an upper case "i")

To run a longer term shape test, change to "5000 9000 1" for the last three parameters.

For machines with not much RAM per node, you will need to use smaller values for the first and second parameters, and optionally add in a fourth parameter which is a repeat count for each value. So e.g. "3000 6000 1 10" would be reasonable values. A shape test should be run in the background with 'nohup', so that logging out of the shell does not kill the process. For example:




nohup wxtester -s DSN -u USER -p PASSWORD -Ishape 5000 5000 1 &amp;


To calculate a bandwidth rate per node, work out nrecs * reclen / (time * number of nodes). Note that for systems with very small nodes counts this is misleading - e.g. for 2 nodes half the traffic stays on node, so the rate you get should be halved as you should only be looking at off-node traffic. Typically we calculate the rate for replicating 64 byte records. A useful command line for doing this is shown below - you need to change the numberofnodes to be the actual number for the system concerned:



cd `wxlogd wxtester` 
grep TSTSHN results | gawk '{ if ($5==64) print ((($3*$5)/$6)/numberofnodes)/1048576 }' | head -20 | sort




As a very rough rule of thumb, we used to aim for about 
40MB/s/core with Kognitio-produced appliances, so if your system had 32 cores per node you would expect to
 get a result of about 1280 from the above, providing you have enough 
network bandwidth for this. 

With current HP Gen 9 kit, the rates are a lot higher - a system made of 40 core nodes with 2 x 56Gb/s links per node showed a rate of 3500MB/s/node, which is around 90MB/s/core.

In other environments you will not see this rate (e.g. Amazon) as the networking infrastructure cannot support it.

stopping a shape test


When you are satisfied that the results of the shape test are clear, you can stop the process by running 'ps -ef | grep wxtester' and taking a note of the process ID, then running 'kill -9 &lt;pid&gt;'
Remember that shape tests can leave a significant amount of data in RAM, so you should try to get rid of the relevant images at the end of the run. To do this, login as the user who ran the shape tests and run "drop table qbgseed cascade".
 






HOWTO: How do I disable the RS on one node

Solution 00000260 by Kognitio Support at 2008-10-02T11:29:25.000+0000

To stop Ram Store (RS) processes running on a node, edit the local config file with "wxviconf -l" on the node in question, and add the following entry:


[capabilities]

db_rs=no

HOWTO: How can I restrict user access to WX2?

Solution 00000261 by Kognitio Support at 2008-10-03T08:47:29.000+0000

The adm_login_mode parameter can be used to restrict login access to WX2. It consists of a number of bitfields:


0:    only allow connections from localhost

1:    only allow connections as root

2:    only allow connections as SYS


For example, setting adm_login_mode to 5 only allows SYS connections to localhost. This permits maintenance activity whilst preventing other users connecting.


You still need to deal with existing connections. One method is to let them run for some time then abort via the ipe_allcursessions table.


When the maintenance is complete, setting adm_login_mode back to 0 allows all users to connect as usual.

HOWTO: How do I setup the recovery script wxmon.pl to cope with certain failures on WX2?

Solution 00000297 by Simon Darkin at 2009-04-23T08:17:17.000+0000

To allow WX2 to tolerate certain failures you can setup a recovery script that will regularly check the state of the system and take appropriate recovery action if something like a node should fail.  

Prerequisites and recommendations
i) Wxmon will only work with a WX2 system that has software raid enabled.
ii) Wxmon must be installed on an AP node that is part of the system that you want to monitor, preferably one that can send emails to allow alerts to be sent. It uses the wxadmin user under Linux by default but the user can be changed if necessary.
iii) Wxmon can be installed on two AP nodes so that monitoring can continue should one AP fail.  It cannot currently run on more than two AP nodes or a DB node although future versions may support this. 
iv) Wxmon needs to be able to ssh as whatever user you are going to run it as, from any node (for now just the APs it will run on) to any other node non-interactively; it therefore needs ssh keys to be set up because it won't be possible to send a password.  If SSH access is not setup you can generate a DSA SSH key pair using the following command; then add the public key from id_dsa.pub to the authorised keys file on each node (~wxadmin/.ssh/authorized_keys for most Linux versions or /etc/ssh/keys/wxadmin for the production image) for 'wxadmin@*'.

  ssh-keygen -t dsa -f ~wxadmin/.ssh/id_dsa -P ""

In the script, it disables strict host key checking, so that it won't prompt about keys it hasn't seen before.  However, ssh will still refuse to connect if the host key has changed (most likely because there is now a different blade using the same IP) and then wxmon will probably go wrong.

You may need to generate a key for the wxmon user using ssh-keygen, as shown above.  If you add the public key to the authorised keys file on one node, you could use wxsync to propagate that across all the nodes (as long as it is supposed to be the same on all nodes).



The following steps explain how to install and configure the script.

1) Edit the WX2 config file to ensure the correct parameters are present
i) run wxviconf from the command line as user wxadmin.  You should already have an entry indicating the raid_cluster_size which is typically set to 2 or 4. To the same section add:

  virtual_diskstores=1

This allows WX2 to place a disk handling process on a node without a disk resource, which is what is required if a node with a disk resource is removed.

Add a [wxsmd] section with the following:

  tolerate_changes=missing_node, missing_link, missing_disk

This allows WX2 to tolerate a missing disk resource when the system is started - otherwise the default is to fail to start with an error reporting which disk resources are missing.


2) Installation
i) Download the latest version of wxmon.pl and timeout from this solution's attachment and install onto the AP somewhere in your path e.g. /usr/local/bin
ii) Make the timeout file executable e.g. chmod 744 timeout.
iii) Alter the following variables at the start of wxmon.pl

$dump_dir        Directory where debug dumps are saved. If seting up on two APs try to setup identical dump areas so that the same wxmon.pl can be used on both.
$submit_user        User to connect as using wxsubmit
$submit_passwd        Password for user for wxsubmit
$use_two_aps       Set to 0 for one AP, and 1 for two APs
$mail_to       Email address for alerts (multiple recipients should be space separated). 

Note you must escape the @ e.g. $mailto = "john.smith\@kognitio.com";

iv) The following parameters should be reviewed and set as appropriate to tailor the recovery script behaviour 

@submit_alert_mins       Interval in minutes for subsequent submit attempts once a failure is discovered
$submit_restart_server_mins       Interval in minutes between final check following failure and restart. So if set to 5, it will be at least 5 minutes from submit first failing until the server restarts (and the restart will only be done if all the intervening submit attempts fail);  -1 = no restart
$primary_monitor_override         Hostname of the primary monitor (overrides the default method of assignment)
$secondary_monitor_override          Hostname of the secondary montor (overrides the default method of assignment)
$restart_with_sysimage       0 = "create image", 1 = "create system image"
$restart_if_dump_failed         0 = Don't start if dump fails, 1 = restart even if dump fails


v) Edit the alert routine in wxmon.pl to e.g. send an email or SMS. Below is an example string for emailing an alert.  

my $c="echo | $run_timeout $ssh wxadmin\@ mail -s \"\\\"\'$alert_str ($ft)\'\\\"\" -r \"\\\"\'$mailfrom\'\\\"\" $mailto";


3) Stopping and starting the recovery script
You start and stop the script on a node using "wxmon.pl start|stop".  It will only actually run on two of the APs (namely, the first two when their names are sorted alphabetically, or of course just one node if only one AP exists), otherwise this command will do nothing.  This is intended so that you can do the following and not worry about nodes (or that we might change the behaviour in the future to run on more nodes).  This should be run as wxadmin.

    wxtool -E "wxmon.pl start"
    wxtool -E "wxmon.pl stop"

Please note that the script won't start using wxtool -E if the location of wxmon.pl is not on your $PATH

Always check the output log after starting wxmon.pl to ensure that wxprobe and wxsubmit are able to return ok.  If you see a message alerting that either of these can't be run then double-check that both wxmon.pl and timeout are on the PATH 

Secondly if you are running the recovery script from two APs then check the log to make sure the primary and secondary monitors can detect each other  If you see a message along the lines of  "ALERT:  Primary|Secondary monitor is broken" then check that wxmon.pl is running on both APs and check that timeout is on the PATH

4) How it works
Wxmon initially uses "wxprobe -wF" to work out which nodes are in the system and their MAN link 0 IP addresses to use for ssh.  It also chooses some which it will use for wxsubmit.  The primary monitor checks every so often that the secondary monitor is running and for the problems below.  The secondary monitor checks every so often that the 
primary monitor is running and only if it isn't, then it will take over and check for the problems below.

It tries to connect to the database using wxsubmit, regarding any response from the server as a success (for example, "sessions inhibited" doesn't imply a server problem).  

It also runs wxprobe to check for problems.   There are several possible things that can go wrong here:

  1.  Wxprobe reports crash; couldn't run wxsubmit:   do a dump and restart database
  2.  Wxprobe reports dead link; successfully run wxsubmit:   try to restart smd
  3.  Wxprobe reports dead link; couldn't run wxsubmit:   try to restart smd, then try to restart database
  4.  Couldn't run wxsubmit:   note how long this has happened and send some alerts (after say 5, 10 and 20 minutes), then eventually restart it (after say 30 minutes)
  5.  Wxprobe reports node added or removed:   send alert

The database is not restarted if there is a diskstore crash disk as that might result in data integrity issues on a restart. It attempts to avoid a repeated loop of restarts.  Database restarts create a full image but there is an option to create system image.

All script output is logged by default to ~wxadmin/wxmon.log* and alerts are sent for important events.  It rotates the log file after it grows to a large size, by default 1 Mib.


5) Why it works the way it does
Wxmon was designed with the assumption that only one node will fail at a time, although anything can fail.  It is intended to recover from easy, common problems, but without making them worse.  It also keeps users notified by sending alerts.

It doesn't attempt to cope with fundamentally difficult problems such as what happens if the primary and secondary monitors can't see each other and try to restart the database at the same time.


6) How to see what it is currently doing
You need to look at the nodes running the primary and secondary monitors, which are those given by "wxtool -R | head -n 2".  You can look in the most recent log file "~wxadmin/wxmon.log" to see what has happened recently.  Wxmon logs all the monitoring actions it performs and the alerts.  You can search for alerts by looking for "ALERT" in this file.  Alerts are also emailed, but delivery may take time or fail so it is best to check the log directly if in doubt.

Attachment: wxmon.tgz,

HOWTO: Install Perl DBI

Solution 00000302 by David Wild at 2009-06-30T14:08:12.000+0000

For the installation you will need these three things...
 
1. The perl DBI module. You can get this from:

http://search.cpan.org/~timb/DBI-1.607/DBI.pm

2. ODBC driver for Perl DBI. You can get this from:

http://search.cpan.org/~mjevans/DBD-ODBC-1.21/

Note: Please avoid using version 1.17 of the DBD::DBC drivers as it contains a bug preventing retrieval of varchars &gt; 81 bytes in length.

3. An ODBC driver manager (e.g. UnixODBC)

The rest of this solution will assume that you install the first two components on a server with an ODBC driver manager already installed and with WX2 access via that driver manager proven.

So (as root) ...

1. Download 1 + 2 to /tmp

2. Extract the tar file for the DBI module...

&gt; cd /tmp
&gt; tar -xvf ./DBI-1.607.tar.gz
&gt; cd DBI-1.607

3. Follow the instructions in the README file for building the DBI module. In the case of the version being used here:

&gt; perl Makefile.PL
&gt; make
&gt; make test
&gt; make install 

4. Now build/install the ODBC driver...

&gt; cd /tmp
&gt; tar -xvf ./DBD-ODBC-1.21.tar.gz
&gt; cd DBD-ODBC-1.21

5. Follow the instructions in the README file for building the ODBC module. In the case of the version being used here, the first thing to do is setup the environment variables. So as an example:

export DBI_DSN='dbi:ODBC:test1'
export DBI_USER='sys'
export DBI_PASS='syspass'
export ODBCHOME=

leave ODBCHOME blank and "perl Makefile.pl" will auto-detect the unixODBC driver (if it is installed)
test1 is a DSN setup in the unixODBC .ini file, like this:

&gt; cat /etc/odbc.ini 
[ODBC Data Sources] 
test1=test1

[test1] 
Driver=/opt/kognitio/wx2/current/lib/Linux/libwcsodbc.so 
Description=test1
ServerAddress1=10.2.3.4
ServerPort1=6550 
Timeout=15 
ForceConnect=N 

Now make two modifications to the test scripts to ensure they can run succesfully against WX2.  The first modification ensures that the select statement targets a table name as we don't support "select 1".   

The second modification specifies a VARCHAR instead of a LONGVARCHAR in the ODBCTEST module.  Historically VARCHAR had a maximum length of 255, but now they are both around 32KB so it's ok to use VARCHAR as an alternative for the test. 


A) Modify t/02simple.t to select a single row from ipe_system

    if ($drv =~ /Oracle/i) {
        $sql = q/select 1 from dual/;
    } else {
        $sql = q/select 1 from sys.ipe_system/;            [SQL_SMALLINT,-5, SQL_TINYINT, SQL_NUMERIC, SQL_DECIMAL, SQL_FLOAT, SQL_REAL, SQL_INTEGER],
                     'COL_B' =&gt; [SQL_VARCHAR, SQL_CHAR, SQL_WVARCHAR, SQL_WCHAR],
                     'COL_C' =&gt; [SQL_VARCHAR],
                     'COL_D' =&gt; [SQL_TYPE_TIMESTAMP, SQL_TYPE_DATE, SQL_DATE, SQL_TIMESTAMP ],
                    );


Now build and install the ODBC module:

&gt; perl Makefile.PL
&gt; make
&gt; make test
&gt; make install

6. Tidy up

&gt; cd /tmp
&gt; rm -r ./DBD-ODBC-1.21
&gt; rm -r ./DBI-1.607

7. Test DBI works by making a connection from a perl script.

Create conn.pl and put in the following lines:

use DBI;
use DBD::ODBC;
if (DBI-&gt;connect("dbi:ODBC:", "sys", "")) { print "SUCCESS connecting to WX2\n";}

Then to test: 
&gt; perl -w conn.pl 

If this comes back successfully the installation is working.

HOWTO: How can I specify target strings for commands such as wxprobe, wxtool and wxsync?

Solution 00000303 by Simon Darkin at 2009-02-06T17:30:49.000+0000

TARGET FILTERS

By default wxprobe and friends will target all the nodes that are running an SMD for a given system, however commands such as wxprobe, wxtool and wxsync can be supplied with a target string allowing a subset of nodes to be targeted.

The  can either be the name of a node (usually 'hostname', but doesn't have to be), or it can be a list of target tokens enclosed in {}'s.  Target tokens can either be one or two words long depending on what the token is, these include:

can 		each node has a set of capabilities which define what it can do.  wxprobe -w lists these.  
   	double word token targets nodes with a given attribute.  Each node has attributes defined for it's configuration file
  		single word token which targets the node with the supplied IP address
  		single word token which matches nodes with the supplied name
ukey   		targets all nodes which have the user defined key  set.

Using node capabilities as an example the 'can' token will target only those nodes which have the capability .  The capabilities of a node can be viewed in the output from wxprobe -w , which also serves as a useful reminder of how  must be specified in the target string. This formatting consideration also applies to single and double word tokens whereby the format of the node name, IP address or attribute value within the search string must match the format of that items as viewed in wxprobe output. wxprobe -wF lists all node attributes so can be used as a useful reference.

Each target string can be preceded with a '!' which negates it. For example 'can DB' would target DB nodes only, whereas '!can DB' would target non DB nodes only, useful if you want to target say APs.

Tokens can be put together. A space separator will perform a logical AND, a comma seperator will perform a logical OR.

Examples
	wxprobe -w -a '{hardware HPBlade}'		target HPBlade hardware only
	wxprobe -w -a '{can DB}' 			target DB nodes
	wxprobe -w -a '{rack rack3}' 			target all nodes in rack 3
	wxprobe -w -a '{enclosure enc6}' 		target all nodes in enclosure 6
	wxprobe -w -a '{bay 1}'				target all nodes in bay 1 
	wxprobe -w -a '{rack rack2 bay 1}'		target nodes in rack 2 bay 1 (if two nodes have both rack=2 and bay=1 they'll both be returned).
	wxprobe -w -a '{rack rack2, bay 1}'		target all nodes in rack 2 and all nodes in bay 1 of other racks (i.e. return nodes with rack=2 OR bay=1)
	wxprobe -w -a '{bl-f0v2r3e2b1, bl-f0v2r3e3b1}'	target the two nodes listed
	wxprobe -w -a '{10.2.2.16}'			target the node listed
	wxprobe -w -a '{!enclosure enc6}'		target all nodes except enclosure 6
	wxprobe -a '{mpid 100}' -l  			target the node that is running mpid 100
	wxtool -R -a '{updated}'			target the nodes which need an SMD restart because the config file has changed (or whatever). Useful check to see if anyone forgot to restart an SMD
	wxtool -w -a '{aa:bb:cc:dd:ee:ff}' target the node listed		

USER DEFINED TARGET FILTERS

User defined keys can be set with the wxtool -F  command, which sets the key on all nodes targeted.  So to assign target strings to a key:

	wxtool -a '{bl-f0v2r3e2b1}' -F key1		Add the listed node to key1
	wxtool -a '{bl-f0v2r3e2b2}' -F key1		Add the listed node to key1
	wxprobe -w -a '{ukey key1}'			target nodes assigned to key1

wxtool -C clears previously defined keys, so to clear a key:

	wxtool -a '{ukey key1}' -C key1			clear key1

HOWTO: How can I change the WX2 system_id?

Solution 00000308 by Deborah Martin at 2018-04-09T14:22:54.000+0000

You cannot use wxviconf to change the system_id for a WX2 system, because wxviconf relies on the existing system_id to work. Therefore, you must stop the smds and manually update the config file to change the system_id. You must also run a newsys after changing the system_id. 

If you are splitting an existing system and require the data from the nodes being removed from the original system, you will first need to run a "reconfigure down" process before
following this solution. For instructions on reconfigure down use this link: https://emea.salesforce.com/50120000000Ciq7?srPos=1&amp;srKp=501.

To change the system_id do the following:

1) ensure a full backup is taken, in case something goes wrong.
2) stop WX/2 with "wxserver halt".

3) run "wxprobe -i" and save the results somewhere.

4) stop the smds with "wxserver smd all exit".

5) on the first node  to be changed, as the root user, take a backup copy of the config file which resides in /opt/kognitio/wx2/etc.
Edit /opt/kognitio/wx2/etc/config and change the system_id as required. Make sure the system_id is all lowercase. 
Save the change. 

6) scp the config file to all other nodes, ensuring it is written to /opt/kognitio/wx2/etc. Ensure the owner and group of the file is set to root on all nodes. The results from step 3 will contain a list of all node's IP addresses.

7) restart the smds. Typically by running "wxsvc start" on all nodes via ssh, as either wxadmin or wxroot.

8) check with "wxprobe -H" that all the relevant nodes are now present with the new system_id.

9) when all nodes are present, ensure a license for the new system is installed using "wxlicense -Ak &lt;licence key&gt;"


10) run newsys as wxroot (options 7, then 10 in wxadmin tool)


11) The disks will now zero - this may take a few hours. When zeroing is complete, you may restore from backup to the new system, using wxrestore. Restoring from backup can take several hours. 


12) update odbc.ini on the AP so that it contains a DSN for the new system ID. 


13) ensure that other systems are updated to reflect the new system_id. For example, Nagios. 




HOWTO: How can I modify a dependent view definition if "create or replace view" fails

Solution 00000310 by Simon Darkin at 2016-06-16T10:59:48.000+0000

The "Create or Replace View" (CORV) command is the preferred method for altering a view on which other views depend, as it has the ability to cascade changes through to the child views.  CORV has however been a problematic area and there has been a couple of occasions where the CORV command has failed due to underlying bugs in the code.  Naturally any such issues should be raised as a CASE but if a fix is not available in time then there is a workaround.


If a situation arises where CORV can't be used but the definition of a dependent view needs to be changed then it is possible to replace entire trees of dependent views using wxbackup and wxrestore as long as the signature of the dependent view is not being changed.

A typical candidate might be a view that unions several months of data and you want to remove the reference to the table containing the oldest data and then union in data for the latest month. The signature of such a view would be unchanged in this example and therefore it could be replaced using this workaround.

  

Note you CANNOT use wxbackup/wxrestore method if the signature of the dependent view is being altered.


The steps to replace the dependent views are as follows:


1) Take a metadata backup of the entire system  e.g.  wxbackup -s &lt;server&gt; -u &lt;user&gt; -p &lt;pwd&gt; -d &lt;backup dir&gt; -M 0 -z 


2) Create a file called dep_tmp.txt in the area where the backup was created


3) From Console (or similar) run "explain &lt;dependent view name&gt; dependent views" 


4) Paste the results from step 3 into dep_tmp.txt


5) Remove any duplicate view names from dep_tmp.txt and create a restore script with the following command:


echo "[structure]" &gt; restore_script.txt; sed s'/ //g' dep_tmp.txt | sort | uniq &gt;&gt; restore_script.txt


6) Drop cascade and then re-create the dependent view with any modifications as appropriate, remembering that the signature must be identical to the view it replaces.


7) Restore the remaining views using the restore script created in step 5 e.g. wxrestore -s &lt;server&gt; -u &lt;user&gt; -p &lt;pwd&gt; -d &lt;backup_dir&gt; -f restore_script.txt


8) Check there are no errors from wxrestore, remembering that it's possible to drop the dependent view and restore back to the original state if required.

HOWTO: How do I restrict WX2 logins as a non-SYS user?

Solution 00000316 by Simon Darkin at 2009-02-20T11:00:52.000+0000

The SYS user is required to grant the nominated admin user some initial privileges, once that's done the admin user will be able to restrict any other user from accessing WX2.  
There are two ways of setting this up. The first method utilises IPE_LOGIN_RESTRICT, and is documented in the SQL guide.  
The second method allows you to restrict logins by modifying the value of STATUS in IPE_ALLUSER.    



METHOD ONE - IPE_LOGIN_RESTRICT (See WX2 guide for further details) 
------------------------------------------------------------------------------------------------

--As SYS
grant all on ipe_login_restrict to ;
grant all on ipe_system to ;

--As admin-user
--restrict logins for specific user by setting PROHIBIT to 1
insert into sys.ipe_login_restrict values (, NULL, 1, NULL, NULL, NULL);

--Activate the restriction with
update sys.ipe_system set login_restrict = 1;

--De-activate the restriction with
update sys.ipe_system set login_restrict = 0;

--enable logins for specific user by setting the value in PROHIBIT back to 0
insert into sys.ipe_login_restrict values (, NULL, 0, NULL, NULL, NULL);



METHOD TWO - IPE_ALLUSER
-------------------------------------------

--As SYS
grant create system image to 
grant all on ipe_alluser to 
grant abort sessions on  to ;  (repeat for all users)
grant abort queries on  to ;   (repeat for all users)

--As admin user
--Restrict logins
update sys.ipe_cursessions set abort = 2 where session ', '');
disconnect;

--Restore user logins
update sys.ipe_cursessions set abort = 2 where session ', '');
disconnect;

HOWTO: How do I identify how much disk space is reclaimable?

Solution 00000321 by Simon Darkin at 2016-06-10T13:45:53.000+0000

The following command shows how many megabytes (MB) will be reclaimed from a system, using 'reclaim to now':

select sum(t) MB_reclaimable from (select (case when drop_tno=2147483647 then del_words+trunc_words else words+del_words+trunc_words end) /(1032*1032) as t from ipe_ftable) df

explanation: The 'del_words' field shows the space taken up in bytes by deleted rows for a particular table id and slab. trunc_words is similar, but for truncated tables. the 'words' field is space taken by live or dropped rows. Thus 'words' is only reclaimable space when the table has been dropped.

The above command will be accurate most of the time. Occasionally you may find that less space gets reclaimed than the command predicted. This is usually caused by one of the following;

1. discardable data (meaning deleted, dropped or truncated ) are interleaved with current data.  Reclaim runs in two phases. in phase 1 (which is fast), reclaim will read buffers from disk until a buffer is found which either a) exceeds a threshold on discardable data (default 80%) or b) contains some truncated records.   Once one of those conditions are met, then phase 2 will begin. In phase 2 (which is slow), reclaim will remove all discardable data from the buffer. So if a buffer contains 70% discardable data, then NONE of that disk space will be recovered, unless it contains truncated rows. 

2. If the most recent backup on the system had the 'expect-incremental' option set, and the rows you're trying to reclaim were deleted/truncated/dropped after that backup, then those rows will not be reclaimed. This is because the system needs to preserve information on what data was added / removed since the last backup, so it can be included in the next backup. If such rows were reclaimed, then that information would be lost. In this case, you need to run an incremental or full backup, then run a reclaim.







 



HOWTO: How do I permanently disable Linux/Solaris swap on WX2?

Solution 00000324 by Simon Darkin at 2009-03-11T14:37:55.000+0000

Kognitio recommend that swap is disabled on DB nodes running WX2.  Swap can be permanently disabled using the following steps:

1) Confirm that swap is in use by running 'free'.  If the total swap is non-zero then swap is enabled.  The default output is in KB so in the example below we can see swap is set to 2Gbytes.

wxadmin wxadmin@system1:/etc&gt; free
             total       used       free     shared    buffers     cached
Mem:      66012628     470260   65542368          0      46516     315340
-/+ buffers/cache:     108404   65904224
Swap:      2104504          0    2104504


2) As root, disable swap with the command:

	swapoff -a


3) edit /etc/fstab and comment out (with a #) or remove the 'swap' line or lines.  

Please note the order is important -- if you do it in the other order (edit /etc/fstab first) then swapoff -a, then on certain systems the swap partition may not be disabled as there is no longer a reference to it in /etc/fstab.


4) Finally, check that swap is set to zero by running free.

wxadmin wxadmin@system1:~&gt; free
             total       used       free     shared    buffers     cached
Mem:      66012628     170572   65842056          0       5756      74428
-/+ buffers/cache:      90388   65922240
Swap:            0          0          0

HOWTO: Test read/write disk performance

Solution 00000328 by Simon Darkin at 2015-06-05T09:50:44.000+0000

Below are some example tests that can be used to check the read/write performance of a disk on a system running WX2 on Linux.


Some example timings have been included which were taken from the following drive specification:


Model:		HP DG146BB976

Capacity: 		136Gb 

Drive Type:		SAS 

Rotational Speed:	10k rpm



The following tests assume to have access to a linux account and that you have a filesystem,


--Write out a 2Gbyte file using fsync to flush the writeback cache

--48-50Mb/s

for i in `seq 1 3`; do echo "Loop $i"; dd if=/dev/zero of=2gb_file.out bs=400000 count=5000 conv=fsync; echo ""; done



--Read a 2Gbyte file using sync to flush the read buffer

for i in `seq 1 3`; do echo "Loop $i"; sync ; echo 3 &gt;/proc/sys/vm/drop_caches; dd if=&lt;path&gt;/2gb_file.out of=/dev/null bs=400000 count=5000; echo ""; done



--Read 2Gb of data from the WX2 partition adjusting the offset on each iteration of the loop and using sync to flush buffers

--77-90Mb/s

for i in `seq 1 3`; do let skip=$i*5000; echo "Loop $i - skip = $skip"; sync ; echo 3 &gt;/proc/sys/vm/drop_caches; dd if=/dev/cciss/c0d0p2 of=/dev/null bs=400000 count=5000 skip=$skip ;echo ""; done



--Meaure how fast the drive can sustain sequential data reads under Linux, without any filesystem overhead.  Ensure the buffer cache is flushed during the processing buy including the -t switch

--82-86Mb/s

for i in `seq 1 3`; do echo "Loop $i"; hdparm --direct -t /dev/cciss/c0d0p2; echo ""; done





Note that whilst a single hdparm test may not necessarily reveal any performance issues, concurrent hdparm tests against a single logical disk could reveal poor seek performance or in the case of a hardware RAID array, a failed disk.   Case 15847 provides an example of this and explains two distinct ways in which a RAID controller deals with accesses. For example, does it try to split each large access into two and use both disks (which we've seen with some HP kit in the past), or does it do a large IO from one disk to allow any subsequent IO to a different part of the logical disk to use the other disk rather than have both drives having to do large seeks.  Cleary the effect on performance in the vent of a failed disk fail will be different depending on which approach the RAID controller uses with regard to accesses and whether or not the performance of single or concurrent accesses are being measured.


--In addition if you suspect that a disk is underperforming in WX2 you can scan a disk based table then monitor wxtop and look for diskstores that finish a significant time after others have completed.


select *

from &lt;large disk based table&gt;

where &lt;condition unlikely to be true&gt;;


If you do discover one or more diskstores that lag behind the majority then check the MPID(s) in question to see if they contain significantly more rows than the other MPIDs for the table being scanned as tha twill naturally affect scan times.


select mpid, sum(nrows) nrows

from ipe_ftable

where table_id = &lt;table_id being scanned&gt;

group by 1

order by 2 desc;



HOWTO: How do I grant a non-SYS user the privileges to change their default queue?

Solution 00000329 by Simon Darkin at 2009-03-26T11:17:50.000+0000

As soon as the "QUEUE" domain is documented in the privilege section of the SQL guide then that should be used in preference to this solution. 


--as SYS

grant alter on queue default to &lt;user&gt;;

grant alter on queue &lt;new-queue&gt; to &lt;user&gt;;


--as non-SYS user toggle between default and the new queue

alter user &lt;user&gt; set queue to default;

alter user &lt;user&gt; set queue to &lt;new-queue&gt;;

HOWTO: Turn off all output streams (except for result rows) in wxsubmit

Solution 00000330 by Simon Darkin at 2009-03-27T13:37:59.000+0000

This is useful if your building SQL statements using SQL. 


wxadmin wxadmin@hp-rack1-enc1-7:~&gt; cat s.sql

set out all off;

set header off;

set out rows s.out;

select 'drop view blah' from ipe_limit;


wxsubmit -s &lt;server&gt; &lt;user&gt; -p &lt;pwd&gt; s.sql


wxadmin wxadmin@hp-rack1-enc1-7:~&gt; cat s.out

drop view blah

drop view blah

drop view blah

drop view blah

drop view blah

drop view blah

HOWTO: View/restore historical data

Solution 00000333 by Simon Darkin at 2009-04-03T11:44:24.000+0000

This method can be used as a starting point to restore a table back to a previous state (assuming a backup doesn't exist).  Useful if someone has updated or deleted from a table and they need to view or restore the original contents.  
Note this method relies on historical data so it can only be used if there has not been a reclaim run between the events that changed the table and the point in time when the table was in the state you want to get back to.

The following example starts of with a 3 row table, which is then deleted from and updated.  It then shows how to view the contents of the table prior to those delete and update operations.

drop table t1 cascsde;
create table t1 (c1 int) disk;

insert into t1 values (1),(2),(3);

--deleting a row will update wx_update_tno() changing 2147483647 (most pos int) to the tno of the delete command
delete from t1 where c1= 1;


--updating a row will delete the row where a value is changing ,updating wx_update_tno() from 2147483647 to the tno of the update command
--and a new row is created with wx_create_tno() set to the tno of the update
update t1 set c1 = 4 where c1 = 2;

--Get transaction numbers for the delete operation (delete where C1=1) and the update operation (where C1 was updated from 2 to 4)
select wx_create_tno(), wx_update_tno(), c1
from t1 order by 1
at full_history;

WX_CREATE_TNO()|WX_UPDATE_TNO()|C1
          25065|          25068| 1
          25066|          25069| 2
          25067|     2147483647| 3
          25069|     2147483647| 4

--select original data (i.e. before the update and the delete)
select c1
from t1
where (wx_update_tno() = 2147483647 and wx_create_tno() &lt; 25069)
or wx_update_tno()

HOWTO: Configure the 'pocadm' system for resilience testing

Solution 00000342 by Simon Darkin at 2009-12-03T17:06:40.000+0000

The 4DB + 1AP node WX2 system 'pocadm' is primarily intended for resilience demonstrations to customers and must be configured in a consistent way.

From time to time the system may be configured for other types of testing, so this solution describes how to get the system back into 
the 'resilience' configuration. You should allow about 20 minutes to complete this process.

Repeat steps 1a-1g and step 2 for each of the 4 DB nodes. 

1a) as root remove any existing WX2 partitions (if they're greater than 4Gb, and create a new 4Gb partition for WX2 usage. 

fdisk /dev/cciss/c0d0


1b) print existing partition information with 'p'

Command (m for help): p

Disk /dev/cciss/c0d0: 146.8 GB, 146807930880 bytes
255 heads, 32 sectors/track, 35139 cylinders
Units = cylinders of 8160 * 512 = 4177920 bytes

           Device Boot      Start         End      Blocks   Id  System
/dev/cciss/c0d0p1   *           1        1028     4194224   83  Linux
/dev/cciss/c0d0p2            1029       34538   136720800   60  Unknown


1c) identify the wx2 partition (type 0x60) and delete it with 'd'.  BE CAREFUL NOT TO DELETE THE LINUX PARTITION.

Command (m for help): d
Partition number (1-4): 2


1d) Create a new 4Gb primary partion with 'n' -&gt; 'p' -&gt; enter -&gt; +4000M 

Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4): 2
First cylinder (1029-35139, default 1029): 
Using default value 1029
Last cylinder or +size or +sizeM or +sizeK (1029-35139, default 35139): +4000M


1e) Set partion type to 0x60 with 't' -&gt; 60 


Command (m for help): t
Partition number (1-4): 2
Hex code (type L to list codes): 60
Changed system type of partition 2 to 60 (Unknown)


1f) Check your changes with 'p'

Command (m for help): p

Disk /dev/cciss/c0d0: 146.8 GB, 146807930880 bytes
255 heads, 32 sectors/track, 35139 cylinders
Units = cylinders of 8160 * 512 = 4177920 bytes

           Device Boot      Start         End      Blocks   Id  System
/dev/cciss/c0d0p1   *           1        1028     4194224   83  Linux
/dev/cciss/c0d0p2            1029        1986     3908640   60  Unknown



1g) write the changes with 'w'

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table.
The new table will be used at the next reboot.
Syncing disks.


2) Back at the command prompt as root, re-read the partition table with 'partprobe'


3) restart the SMDs with 'wxserver smd all restart'


4) Check that there are 4 disk reources each 4Gb in size (3817Mb if you specified +4000M in fdisk)

wxroot wxroot@wx2-f0v6r6e5b2:~&gt; wxprobe -HD
Kognitio WX2 Hardware Discovery Tool v6.01.07-pbet1 on pocadm
(c)Copyright Kognitio Ltd 2001-2008.

4 disks like this one:
  Disk WXD:4A072507:00000003: on_nodes 1, Status Up.
           sid &gt;&gt;, seq 0x0, Location wx2-f0v6r6e5b4.
           SIZE 3817Mb, nsecs 7817280, ssize 512.
             Attributes Part-Zero-DirIO-Align


5) Ensure that the config file contains the following entries...

[general]
system_id=pocadm

[system]
external_net=eth0

[boot options]
raid_cluster_size=2
virtual_diskstores=1

[wxsmd]
tolerate_changes=missing_node, missing_link, missing_disk

[runtime parameters]
ds_gsr_percent=80


6) As wxroot run a newsys from the wxadmin tool ensuring that the SYS password matches the entry in the password file


7) As SYS create the WXAMS user

HOWTO: Install and test unixODBC

Solution 00000343 by Simon Darkin at 2017-01-10T11:18:21.000+0000

For the installation you will need two things...


1) The unixODBC install package available from www.unixodbc.org


Note that version 2.2.12 of unixODBC contains a bug in the 64-bit driver which prevents you selecting more than one column using isql.  At the time of writing  2.2.14 is the latest version available and includes a fix for the aforementioned bug.


2) An install of the latest WX2 drivers, available from the Solutions area of the support portal.     


The rest of the solution assumes that the WX2 drivers have been installed.



As root...


1) Download the unixODBC install package to /tmp



2) Extract the tarfile


&gt; cd /tmp

&gt; tar xvf unixODBC-2.2.14.tar.gz

&gt; cd unixODBC-2.2.14/

 


3) Follow the instructions in the README and INSTALL file.  In the case of the version being used here:


&gt; ./configure --enable-gui=no

&gt; make

&gt; make install

&gt; make clean



4) Set environment variables as required.


The files are installed to /usr/local/bin and libraries are installed to /usr/local/lib. If either of these are not in the path then add them in with...


&gt; export PATH=$PATH:/usr/local/lib

&gt; export PATH=$PATH:/usr/local/bin


Ensure the unixODBC libraries can be found at runtime

 

&gt; export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib



5) Create a ~/.odbc.ini file (note the leading dot) and add a datasource. 


[test1]

Description             = Kognitio WX2 ODBC

Driver                  = WX2

ServerAddress1          = localhost

ServerPort1             = 6550

ServerAddress2          =

ServerPort2             =

ServerAddress3          =

ServerPort3             =

ForceConnect            = N

Timeout                 = 3

PrivateKey              =

SslEnabled              = Y

SslAllowFallback        = Y

WX2PreserveCursors      = N

WX2AutoCommit           = Y

WX2KeepAlive            = Y

WX2Directory            = .

WX2Locale               = 4

WX2ODBCDbg              = N

WX2ODBCDbgFileName          = /tmp/wx2odbc.log




6) Check the location of the odbcinst.ini file and then add an entry for the WX2 odbc driver.  Note the driver section header, in this case "WX2" must match the driver reference in the ~/.odbc.ini file 


&gt; odbc_config --odbcinstini

/usr/local/etc/odbcinst.ini


contents of /usr/local/etc/odbcinst.ini

 

[ODBC]

Trace           = No

TraceFile       = /tmp/unixODBC.log


[WX2]

Driver          = /usr/local/lib64/libwcsodbc.so




7) Test database connectivity using isql


&gt; isql test1 &lt;user&gt; &lt;pwd&gt;

+---------------------------------------+

| Connected!                            |

|                                       |

| sql-statement                         |

| help [tablename]                      |

| quit                                  |

|                                       |

+---------------------------------------+

SQL&gt; select count(*) from sys.ipe_system

+---------------------+

| COUNT(*)            |

+---------------------+

| 1                   |

+---------------------+

SQLRowCount returns -1

1 rows fetched



If you receive the error "isql: symbol lookup error: /usr/local/lib64/libwcsodbc.so: undefined symbol: SQLGetPrivateProfileString", then it's likely that you are using a wx2 driver prior to version 6.1.7q, in which case you will need to obtain the latest wx2 drivers from the support portal.

HOWTO: Perform a resilience test on pocadm

Solution 00000346 by Simon Darkin at 2010-01-12T09:26:28.000+0000

NOTE 12/01/10: POCADM DOES NOT EXIST AT THE MOMENT, AS BLADES WERE USED TO RECOVER MOBIUS2 MACHINE, SO DO NOT TRY ANY OF THESE STEPS BEFORE THOSE BLADES HAVE BEEN REPLACED

System details
--------------------
pocadm 	(1 AP + 4 DB nodes) 
IP:	10.6.6.25/35/45/55		# R6 E5
AP:	10.6.6.15 			# R6 E5
sys:	check with support for the password


Initial setup
----------------
The pocadm system should be ready to run a resilience test at any time, but it's worth running the following three quick checks to ensure the system is configured correctly and ready to use. 

1) Check the config file includes the following entries

&gt; wxviconf 

[general] 
system_id=pocadm 

[system] 
external_net=eth0 

[boot options] 
raid_cluster_size=2 
virtual_diskstores=1 

[wxsmd] 
tolerate_changes=missing_node, missing_link, missing_disk 


2) Check that wxprobe reports four disks present, and that they are 3817Mb in size.

&gt; wxprobe -HD
Kognitio WX2 Hardware Discovery Tool v6.01.07-q on pocadm
(c)Copyright Kognitio Ltd 2001-2008.

4 disks like this one:
  Disk WXD:4A37B74F:00000001: on_nodes 1, Status Up.
           sid &gt;&gt;, seq 0x0, Location wx2-f0v6r6e5b2.
           SIZE 3817Mb, nsecs 7817280, ssize 512.
             Attributes Part-Zero-DirIO-Align


3) Check that the system has four good disks 

&gt; wxsubmit -s pocadm sys -p resilience

Connected to pocadm CLI Version 6.01.07-q  Server Version 06.01.0007
&gt;$f3
MPID   |CLID|Disk Status       |MPID GB  |Used GB  |Free GB  |% Used
     43|   1|good              |      1.7|      0.0|      1.7|  0.1%
     14|   0|good              |      1.7|      0.0|      1.7|  0.1%
     29|   0|good              |      1.7|      0.0|      1.7|  0.1%
     57|   1|good              |      1.7|      0.0|      1.7|  0.1%
Query           1               4 rows     ----   0:00.2   0:00.2     ----


The monitoring script is called 'wxmon.pl' and is located on the pocadm AP in /usr/local/bin.  Check that the script exists and that it includes the following variables and associated values.

$sleep_secs = 15;
$test_timeout_secs = 15;
$run_timeout_secs = 15;
$restart_smd_wait_secs = 15;

$submit_user = "wxmon";
$submit_passwd = "wxmon";
$dump_dir = "/opt/kognitio/wx2/home/wxadmin";

@submit_alert_mins = ();
$submit_restart_server_mins = 1;
$restart_if_dump_failed = 1;
$restart_with_sysimage = 1;
$log_free_mem = 0;
$use_two_aps = 0;

It's advisable to run through a test prior to a customer demonstration to ensure everything works as expected.  It takes less than 15 minutes to do the test and get the system  back to the pre-test state.

Before you start it's worth noting that there are two common mistakes that can lead to a failed resilience test...

1) Not powering off the node quickly enough after resetting it.  If you keep this gap to under 10 seconds then you shouldn't have a problem.

2) Forgetting to stop the monitoring script before restarting the server with the all nodes back in.  

So keep these in mind and follow the instructions below carefully to ensure the test completes successfully.



STAGE 1 - Resetting/Powering down a node 
------------------------------------------------------------

1) ssh to the AP as wxadmin

&gt; ssh wxadmin@10.6.6.15 


2) make sure the monitoring script is stopped  

&gt; /usr/local/bin/wxmon.pl stop
&gt; ps -ef | grep wxmon


3) List out the DB nodes and chose which one will be shut down. 

&gt; wxtool -a '{ can db }' -R | sort

wx2-f0v6r6e5b2
wx2-f0v6r6e5b3
wx2-f0v6r6e5b4
wx2-f0v6r6e5b5		


4) run a system image to ensure restarts are as fast as possible

&gt; wxserver start sysimage


5) Start the monitoring script and tail the log file to check that the wxprobe and submit attempts are all running successfully.

&gt; /usr/local/bin/wxmon.pl start
&gt; tail -f ~/wxmon.log_


6) rdesktop into the RDP server 10.6.0.2 as rdp.admin.  Check with support for the password.


7) deployment console -&gt; expand out pocadm, right-click the node to be powered down -&gt; Power Control -&gt; RILOE/iLO - Interface -&gt; login (no username or password required) -&gt;  Remote Console Tab -&gt; Remote Console (this is for visual confirmation that node is powering down) -&gt; Virtual Devices tab -&gt; Virtual Power -&gt; select 'Reset System' radio button -&gt;  click 'Virtual Power' -&gt; click OK, you should receive a visual confirmation.  Briefly check the remote console for messages that indicate the node is resetting then immediately select the 'Press and Hold' radio button -&gt; click 'Virtual Power' -&gt; click OK, you should receive a visual confirmation.  The final step to power the node down ensures that  
restart doesn't try to include the reset node.


8) The failed submit attempts, crash detection and restart messages can be viewed in the wxmon log file.

&gt; tail -f ~/wxmon.log_


9) Look out for the message "Restarting database (with sysimage)" in the wxmon log file, wait 20 or so seconds for the new startup directory to be created, open a second rxvt window, ssh to the AP then navigate to the startup directory to monitor progress of the server restart.

&gt; ssh wxadmin@10.6.6.15
&gt; cd `wxlogd startup'
&gt; tail -f output


Previous tests have shown that it takes 3 - 4 minutes from node reset to completion of the CSI following the restart.





STAGE 2 - Restore the system to it's original state
--------------------------------------------------------------------

1) kill off the wxmon script 

&gt; /usr/local/bin/wxmon.pl stop


2) check with ps to confirm the process is no longer running 

&gt; ps -ef | grep wxmon


3) Return to your rdesktop connection to power up the server that was previously powered off.


4) right-click the node to be powered up -&gt; Power Control -&gt; RILOE/iLO - Interface -&gt; login (no username or password required) -&gt; Remote Console Tab -&gt; Remote Console (this is for visual confirmation that node is powering up) -&gt; Virtual Devices tab -&gt; Virtual Power -&gt; select 'Manual Override for BL p-Class' radio button -&gt; click 'Virtual Power' -&gt; click OK


5) Periodically check to see if the node is detected by the SMD.  Expect to see one AP node and four DB nodes.

&gt; wxprobe -H


6) Restart the server as soon as all four DB nodes are present.

&gt; wxserver start sysimage


7) Connect to the database, and identify the mpid of the bad disk, recreate the bad disk, then check that the disk status has changed to 'recreating data' for the disk being recreated.

&gt; wxsubmit -s pocadm sys -p resilience

Connected to pocadm CLI Version 6.01.07-q  Server Version 06.01.0007
&gt;$f3
MPID   |CLID|Disk Status       |MPID GB  |Used GB  |Free GB  |% Used
     43|   1|bad               |      1.7|      0.0|      1.7|  0.1%
     14|   0|good              |      1.7|      0.0|      1.7|  0.1%
     29|   0|good              |      1.7|      0.0|      1.7|  0.1%
     57|   1|good              |      1.7|      0.0|      1.7|  0.1%
Query           1               4 rows     ----   0:00.2   0:00.2     ----


&gt; recreate disk ; 


then $f3 again to see the recreate:

&gt;recreate disk 43;
Query           2             Complete     ----   0:00.1     ----     ----
&gt;$f3
MPID   |CLID|Disk Status       |MPID GB  |Used GB  |Free GB  |% Used
     14|   0|good              |      1.7|      0.0|      1.7|  0.1%
     29|   0|good              |      1.7|      0.0|      1.7|  0.1%
     57|   1|good              |      1.7|      0.0|      1.7|  0.1%
     43|   1|recreating data   |      1.7|      0.0|      1.7|  0.1%
Query           3               4 rows     ----   0:00.2   0:00.2     ----


The disk status will change to 'good' once the recreate has completed. 



Repeating the node failure test with a node already out
--------------------------------------------------------------------------

If for any reason you need to repeat the procedure because the initial node failure test didn't go to plan, you can still do this even if the system has been reduced to three  nodes from the initial test.   


1) Identify which raid cluster is running a virtual diskstore, then nominate a node from the other raid cluster for the second shutdown test. In the example below either  wx2-f0v6r6e5b2 or wx2-f0v6r6e5b3 are suitable candidates for powering down.

&gt; cat clustermap

       Uid            |     Raid    | state | mpid | cost | local | node
                      |(cluster,pos)| state |      |      |       | 
WXD:4A37B74F:00000001 |(      0,  0)|     1 |    6 |    0 |     1 | wx2-f0v6r6e5b2
WXD:4A37B74F:00000002 |(      0,  1)|     1 |   28 |    0 |     1 | wx2-f0v6r6e5b3
WXD:4A37B74F:00000003 |(      1,  0)|     1 |   50 |    0 |     1 | wx2-f0v6r6e5b4
WXD:4A37B74F:00000004 |(      1,  1)|     1 |    5 |  | wx2-f0v6r6e5b2

2) Assuming you still have all the connection windows open from the initial test then repeat the test from "STAGE 1, STEP 7" above, remembering that you will need to power up two  nodes and recreate two disk when you're restoring the system to it's original state.

HOWTO: How to do a reconfigure

Solution 00000468 by Kognitio Support at 2014-12-16T15:24:00.000+0000

See forum topic http://kognitio.com/forums/viewtopic.php?f=4&amp;t=250 for details on reconfigure.


Performance figures

1) lmgmigros01 06/01/12
number of disk resources before deskew: 36
number of disk resources added: 12
disk resources per node: 1 of 133GB
software version: 7.1.2z111111
RAID cluster size: 4
max disk usage on old disks beforehand: 68.6%
max disk usage on new disks afterwards: 48.8%
start time for deskew: 06/01/12 18:38:39
end time for deskew: 07/01/12 05:20:32
time takes: 10:41:53
processing rate for old disks scanning data: (133 * 0.686 / 10.68) 8.5 GB / hour
processing rate for new disks adding data:(133 * 0.488 / 10.68) 6.1 GB / hour





HOWTO: How do I change the timezone on Linux and WX2

Solution 00000549 by Simon Darkin at 2015-10-26T11:27:29.000+0000

The timezone needs to be set in two places to ensure that both Linux commands and WX2 use the same zone.  It is best to make these changes when the system is idle as the process requires WX2 to be restarted and you would also want to avoid introducing odd timings into logs that are written to by any shell scripts that you have.  


The example below demonstrates how to set the timezone to CET, however you will want 
to select your desired timezone from /usr/share/zoneinfo 

Stage 1 - As the root user redefine /etc/localtime on all nodes so that Linux commands such as 'date' report the correct timezone


1. Make a backup copy of /etc/localtime

wxtool -S "mv /etc/localtime /etc/localtime.bak"


2. Check the backup copy has been created

wxtool -S "ls -l /etc/localtime.bak"


3. Redefine the symbolic link /etc/localtime to point to "Rome".  
wxtool -S "ln -sf /usr/share/zoneinfo/Europe/Rome /etc/localtime"


4. Run "date" to check the the timezone is reported as CET   (note that "wxtool -S date" will not yet reflect the change) 
date 



Stage 2 - As root redefine the value for TIMEZONE in /etc/sysconfig/clock so that WX2 picks up the correct zone


In this example the nodes are currently set to "Europe/London" which maps to GMT. 


You can see this by using wxtool to query all nodes as follows:-


wxadmin wxadmin@system1:~&gt; wxtool -S 'grep ^TIMEZONE /etc/sysconfig/clock'

Kognitio WX2 Administration Utility v7.01.00-f on system1

(c)Copyright Kognitio Ltd 2003-2010.


Results:

For node system1-wx2-f0v3r1e1b1 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b2 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b3 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b4 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b5 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b6 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b7 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"

For node system1-wx2-f0v3r1e1b8 (ecode 0, 59 bytes):

TIMEZONE="Europe/London"



Change the timezone setting with the following steps:-


1. Backup the existing clock file

wxtool  -S "cp /etc/sysconfig/clock /etc/sysconfig/clock.bak"


2. Check the backup file exists

wxtool  -S "ls -l /etc/sysconfig/clock.bak"


3. Create a new clock file specifying "Europe/Rome" as the TIMEZONE

wxtool  -S "cat /etc/sysconfig/clock | sed 's/\"Europe\/London\"/\"Europe\/Rome\"/g' &gt; /etc/sysconfig/clock.new"


4. Check that the TIMEZONE has been set as intended within in the new file

wxtool  -S "grep ^TIMEZONE /etc/sysconfig/clock.new"


5. Overwrite the existing clock file with the new one

wxtool  -S "cp /etc/sysconfig/clock.new /etc/sysconfig/clock"


6. A final check to see that TIMEZONE is now set to "Europe/Rome" in the current clock file

wxtool -S "grep ^TIMEZONE /etc/sysconfig/clock"


7. Restart the SMDs so that commands like "wxtool -S date" pick up on the change
wxserver smd all restart 


8. Restart KAP so that the database picks up on the change
wxserver start [sysimage] 


9. Restart the cron daemon on all nodes
wxtool -S "/etc/init.d/cron stop"
wxtool -S "/etc/init.d/cron start"



HOWTO: How do I configure the management link to run on a different network

Solution 00000554 by Simon Darkin at 2010-11-16T14:32:28.000+0000

This solution documents how a recent networking issue was investigated on lmg03, and a subsequent workaround implemented by way of switching the management link from one NIC/network to another.  Although this example is fairly specific to the LMG case the general principle could be applied on other systems where multiple network devices are installed. 

The symptoms included:

* failed backup (due to failed SMD commands)
* errors from wxprobe commands i.e. Resends then DEAD LINK message from ap1 to all other nodes even though all nodes were up and had been for weeks
* Restarting the SMDs on all nodes resulted in each AP only able to see itself, and the DB nodes able to see themselves but not the APs

These suggested a network issue relating to the link over which the SMD communicates i.e. the "MAN link", specifically on the APs. 


STAGE 1 - IDENTIFY WHICH INTERFACE AND NIC ARE AT FAULT

1. Check to see which network the management link is running on (10.4 in this case). 

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; wxprobe -wF | grep "MAN link" | more
Kognitio WX2 Hardware Discovery Tool v6.01.08-z20101109 on lmg03
(c)Copyright Kognitio Ltd 2001-2009.

MAN link  0: IP 10.4.11.1, MAC 00.26.55.7D.B5.B8, MPK Y, Status Up.
MAN link  0: IP 10.4.11.10, MAC 00.26.55.7D.C6.78, MPK Y, Status Up.
..

2. Run ifconfig to see which network interface is configured on that address. 

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; /sbin/ifconfig
..
eth0      Link encap:Ethernet  HWaddr 00:25:B3:21:A8:06  
          inet addr:10.3.29.1  Bcast:10.3.255.255  Mask:255.255.0.0
..
eth2      Link encap:Ethernet  HWaddr 00:26:55:56:21:D8  
          inet addr:10.4.29.1  Bcast:10.4.63.255  Mask:255.255.192.0
..

The 10.4 network is configured on eth2 and the 10.3 network in on eth0.

3. Try pinging a database node on the 10.4.network

4.  If the ping fails then check /var/log/messages for evidence of any issues with network devices, paying particular attention for messages relating to the interface over which 

the management link has been configured.   This was the message seen on the lmg03 AP:   

Nov  6 00:28:28 lmg03-ap1-rack2-enc9-1 kernel: nx_nic[eth2]: Device temperature 100 degrees C exceeds maximum allowed. Hardware has been shut down.

The message indicates that the device using eth2 shutdown due to overheating, which explains why SMD commands that ran over the "MAN link" had stopped working. You now need to find out what that network device is called so that it can be investigated further.

5.  Note the physical location for the interface identified in step 2.  eth2 maps to 0000:07:00.0

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; ls /sys/bus/*/devices/*/net:*/address |sort
/sys/bus/pci/devices/0000:02:00.0/net:eth0/address
/sys/bus/pci/devices/0000:02:00.1/net:eth1/address
/sys/bus/pci/devices/0000:03:00.0/net:eth2-mbd/address
/sys/bus/pci/devices/0000:03:00.1/net:eth3-mbd/address
/sys/bus/pci/devices/0000:07:00.0/net:eth2/address
/sys/bus/pci/devices/0000:07:00.1/net:eth3/address

6.  Lookup the name of the ethernet device referencing the physical location identified in step 3. 0000:07:00.0 maps to NetXen 1/10 Gigabit Server Adapter

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; /sbin/lspci | grep Ethernet
02:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
03:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
03:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
07:00.0 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10 Gigabit Server Adapter (rev 42)
07:00.1 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10 Gigabit Server Adapter (rev 42)

Clearly identifying an alternative interface means finding one that's not configured to use the failed NIC. We can see from the output above that eth0 is configured on the 

Broadcom device and so it's a potential alternative.  The earlier output from ifconfig showed that eth0 was on the 10.3 network so logon to a database node, identify the IP address for eth0 and try pinging that address from the AP.  If that works then switch the "MAN link" over to eth0.   


STAGE 2 - SWITCHING MANAGEMENT LINK TO A DIFFERENT NIC/NETWORK

WARNING - It's strongly advisable to perform an initial test with one or two servers to confirm the move has worked ok before making the change to all servers.

1. Connect to the rdp server, take two copies of the local_config file, one as a backup, one for editing. 

logon to the rdp server -&gt; Deployment Console -&gt; right-click LMG03 -&gt; execute -&gt;
	cp /opt/kognitio/wx2/etc/local_config /opt/kognitio/wx2/etc/local_config.old
	cp /opt/kognitio/wx2/etc/local_config /opt/kognitio/wx2/home/wxadmin/local_config.new

2. Edit local_config.new file on both APs and one 'test' DB node specifying the new interface over which the management network should run.    

[system]
management_net=eth0

3. If the local_config is identical on all DB nodes then use wxsync to synch the file edited in the previous steps across all DB nodes 

wxsync -a '{can DB}' -S /opt/kognitio/wx2/home/wxadmin/local_config.new

If the local_config file is not the same on all DB nodes then edit the files manually one by one, or better still look at why the files are different and try to get them into a consistent state where they can be synchronised.
  
4. Stop the smd on the two APS and test DB node only with "wxsvc stop" 

5. Copy the edited local_config file over the live one on the two APs and test DB nodes

cp /opt/kognitio/wx2/home/wxadmin/local_config.new /opt/kognitio/wx2/etc/local_config

6. Start the smd on the two APs and test DB nodes with "wxsvc start"

7.  Run "wxprobe -H" from one of the APs to check that it can see the two APs and test DB node 

8. If the previous step was succesful then make the same change on the remaining nodes via RDP.  

logon to the rdp server -&gt; Deployment Console -&gt; right-click WX2 -&gt; execute -&gt;
	/opt/kognitio/wx2/current/bin/wxsvc stop 
	cp /opt/kognitio/wx2/home/wxadmin/local_config.new /opt/kognitio/wx2/etc/local_config
	/opt/kognitio/wx2/current/bin/wxsvc start

9.  Run "wxprobe -H" and check that all nodes are present 

10.  Restart the server to prevent expired TCP connections timing out and causing a MISC node crash down the line.  This may not be necessary if running with a version that includes  I10537

11.  Note that some operations such as debug dumps and logfile writes may still try to use the old network if that's what the MPK link is configured on and so this may only be suitable as a temporary workaround until the faulty NIC is fixed.

In the case of lmg03 the fix was to set "Thermal Configuration -&gt; Increased Cooling" in the system bios after ensuring firmware was up-to-date to prevent the NetXen NICs from overheating.

HOWTO: What to do if queries against ipe_command return a CG0214 error

Solution 00000591 by Simon Darkin at 2011-02-08T08:41:56.000+0000

The error "Invalid string representation CG0214" is sometimes seen when querying IPE_COMMAND.COMMAND on a system that has been upgraded from version 6 to version 7 . As part of this upgrade IPE_COMMAND.COMMAND is set to UTF-8 because commands from version 7 are inserted in that format. Therefore the upgrade changes system table rows to change the character set (since TREAT AS isn't possible on system tables). 

There might be existing rows using high characters in LATIN-1, which aren't valid in UTF-8, so we want to convert these to UTF-8. However, there might also be UTF-8 rows which we don't want to convert. Fortunately the wxunicheck() function allows us to distinguish between the two, and there is a command to do the conversion just before altering the character set. 

To fix the issue with ipe_command run the following as SYS (having ensured there are no other connections to the system). Once the sequence completes, you need to disconnect to allow other connections to be made: 

lock system; 

update IPE_ALLCOLUMN set CHARSET=1 where TABLE_ID = 541 and SEQ = 2; 

update ipe_command set command = cast(cast(cast(command as varchar(32000) character set utf8) as varbinary(32000)) as varchar(32000) character set latin1) where substring(wxunicheck(command) from 4 for 1) = 'N'; 

update IPE_ALLCOLUMN set CHARSET=1021 where TABLE_ID = 541 and SEQ = 2; 


Invalid utf-8 characters may also find their way into ipe_command on a version 7 server via a client that connects through a pre-version 7 ODBC driver. I10737 wil fix this but at the time of writing is not yet incorporated into a release.

HOWTO: resolving backup problems

Solution 00000704 by Kognitio Support at 2014-11-27T13:46:22.000+0000


The wxbackup command line tool allows full, incremental and metadata backups to be performed for WX2. Full details of the options available can be seen from the associated man page, although an introduction to the various options can be found here.

Considerations include:

1) Prior to version 8, excluding large tables from backup, and exporting them separately with the parallel export tool, wxpexport. Backup prior to version 8 uses a single export stream for each individual table, so exceptionally large tables can be handled faster with wxpexport. From version 8, backup is the fastest way to backup tables, but still consider ways to exclude large tables if the backup window is too small to include them, and capture them less frequently with a strategy for replaying changes since their capture should a restore be required.

2) If running out of memory on the backup client
•if this happens when running from cron, but not when running from the command line, check here for advice.

•consider reducing the number of streams used by backup via the -J flag. The default number of streams is 10. Note that "wxtool" imposes a limit on the amount of RAM that can be used by programs it runs, so if using wxtool to initiate backup it is likely you will have to reduce the number of streams. Our current recommendation would be to use a value of 5 in these situations if wxtool is required to initiate the backup, or remove the use of wxtool if it is not essential.

3) Prior to version 8, consider whether or not compression is required on the backup files, and what trade-off to make between how much compression is achieved and how long the compression takes to perform.

4) It is possible to resume a failed backup which was using a manifest by marking all the incomplete jobs as outstanding. This allows the backup to be rerun immediately (do not do this if data has changed since the original backup was initiated). 

UPDATE SYS.IPE_MANIFEST SET JOB_STATUS = 1
WHERE JOB_STATUS &lt;&gt; 3
AND MANIFEST_ID = &lt;manifest number for original backup&gt;

In the backup manifest table, 1 means waiting to be processed, and 3 means done. The above query will set all failed jobs (status 4) and jobs in progress (status 2) back to 1 so they get retried.

Then rerun the original backup command e.g.

wxbackup -s server -u user -p password -d /path/to/backup/directory -m &lt;manifest number&gt;



HOWTO: use disk repair/check to fix disk corruption

Solution 00000705 by Kognitio Support at 2019-02-08T09:01:27.000+0000


If disk corruption occurs on a system, you can use "invoke disk_check" to determine the extent of the corruption, and "invoke disk_repair" to fix that corruption. You can see the nature of corruption detected by looking in the serverdbg file whilst or after the check/repair runs.

Check/repair work by looking at the headers of objects on disk, and applying various sanity checks to them. When a corrupt header is found, they search forward until they find two contiguous good headers, and patch out the intervening data.

In serverdbg that might look like this:

14-07_23:50:30_BST: DS 257.1: Check: error 0: found garbage at address 0xc00878, len 77, tag 0x1
14-07_23:50:30_BST: DS 257.1: Check: error 0: first 10 words of msg are: 0x1004d, 0x3bea7f, 0x3f, 0x53, 0xc0198c, 0xffffffbb, 0x3bea02, 0x3bea13, 0xc9, 0x310c0
14-07_23:50:30_BST: DS 257.1: Check: patched after finding good tags len pairs (0, 600), (1, 8) from 0xc0092c
14-07_23:50:30_BST: DS 257.1: Check: writing patch of length 720 at 0xc00878

If records for table A were in the corrupt area, normally there will be a subsequent message for that table indicating that the record in A prior to the corruption has now been linked in to the first record after the corruption. For example:

14-07_23:50:30_BST: DS 257.1: Check: error 1: record lost (file 2, addr 0xc007e2, recchainptr 0xc00896, correct chainptr 0xc0092c

There is the likelihood that the record immediately before a patch will also be corrupt, but the tables affected can be determined from the tables listed in serverdbg output (e.g. table 2 above), and then a decision can be taken on whether the customer can tolerate a small amount of corruption to that table, or whether it should be dropped and reloaded from backup. Note that in certain cases the corruption occurs in e.g. a varchar column which can make the server crash when scanning that record, so in this case the offending record(s) must be deleted using a carefully chosen predicate (you will need development help for this), or by dropping and reloading the table. We are currently working on a 7.2 fix which will patch out the corrupt record in this case, but there is a chance that will not be in the initial 7.2 GA release.

Up to 7.1.2 inclusive, if the system has one bad disk (e.g. failed node), then setting the "repd" parameter to the mpid of the bad disk store in the [runtime parameters] section of the config file, then restarting with "wxserver start noimage" and ensuring that is still the mpid of the bad disk store, will allow the check/repair to run much faster. Ensure the repd parameter is commented out of the file after the repair work has been completed.

To do the above in 7.2 onwards, you can set the "disk_to_repair" parameter. The new "slab_to_repair" parameter allows you to specify one slab to be repaired rather than the default of all slabs. Note these parameters apply for both check and repair.

Note that after a repair operation the PTFRIEB structure on each slab needs to be rebuilt on the ensuing create [system] image, and this is a slow process, so that imaging will take a lot longer than usual. In the past we have patched out the setting of the "newp" parameter by disk check/repair on systems which are known to have no compressed indices to speed up the restart - development can do this for you if required, but it will not be possible from 7.2 onwards as the new disk train code relies on the PTFRIEB structure for normal disk scans.








IMPORTANT: After the disk repair you must *restart* the database in order to run a sysimage or full image.  If you try to run either of those without restarting the database will crash.





HOWTO: Re-integrate a repaired node back into a WX2 database

Solution 00000706 by Simon Darkin at 2011-07-20T15:22:38.000+0000

Solutions only have a 32k limit so see attachment for the details of this solution



Attachment: HOWTO re-integrate a repaired node back into WX2.txt,

HOWTO: how to resume an interrupted restore

Solution 00000707 by Kognitio Support at 2011-07-27T15:32:45.000+0000


To resume an interrupted restore, use the following process:
     
1) Determine the manifest_id for the interrupted restore by reviewing the restore log file. You should see a line like the following (in this case the manifest_id is 1):

 -&gt; Successfully uploaded jobs to manifest 1.

2) Set all jobs that were running at the time of the "interruption", back to non-executed:

wxrestore -s &lt;dsn&gt; -u &lt;user&gt; -p &lt;password&gt; -m &lt;manifest_id&gt; -c resume 

3) Run all remaining jobs:
     
wxrestore -s &lt;dsn&gt; -u &lt;user&gt; -p &lt;password&gt; -m &lt;manifest_id&gt; -c run 
     
     
Note that the above steps are not the same as rerunning failed jobs. To do that, specify "-c retry" instead after taking steps to allow those failed jobs to run.





HOWTO: how to deal with out of string space errors

Solution 00000710 by Kognitio Support at 2011-09-20T14:08:32.000+0000


Sometimes string manipulation queries will return with an error like "RS0103: Insufficient string space for operation ".

This indicates that WX2 did not have enough string space to satisfy the query. Often this is a result of having to convert a string to use 32-bit characters for some operation.

Ways to work around this include:
1.try to use substring to only analyse a portion of the string if this is a suitable approach for the task in hand.
2.cast the string to e.g. LATIN1 before further manipulation, to ensure it is using single-byte characters. If the string contains characters which cannot be converted to LATIN1, set the charconv_err_unk parameter to 0 (typically set per-user or per-session) to force such characters to be mapped to '?'


HOWTO: recover disk space when reclaim gives DP0001 out-of-disk error

Solution 00000715 by Kognitio Support at 2011-11-03T09:18:48.000+0000


Reclaim does some deletes and updates to tidy up system table entries before actually attempting to recover space.

If slab 1 is really full, these deletes/updates can fail with an out-of-disk error.

In this case you need to stop reclaim from tidying up the old entries by setting the parameter createimagepurge to 0 as follows:

set parameter createimagepurge to 0

Now retry the reclaim - if it still fails with an out-of-disk error, you need to set the rcus parameter in the config file to 1, then restart the server, set the createimagepurge parameter to 0 again, and run the reclaim again. 

Once the reclaim has run, you need to set all the parameters back to their old values (1 for createimagepurge, 10 for rcus), or drop them so the default values take effect.



HOWTO: How do I configure WX2 to use additional network links for internal traffic?

Solution 00000721 by Simon Darkin at 2013-02-27T16:30:56.000+0000

This solution demonstrates how to increase the number of network interfaces used by WX2 for internal traffic (sometimes referred to as message passing).   It is assumed that the re-configuration is being performed on an existing WX2 instance and that the new network interfaces have already been configured.

 

In the example below WX2 will be configured to utilise an additional network interface "eth3".   

 

1. Stop WX2 with wxserver stop

 

2. List out the interfaces assigned to the default_net parameter located in the local config file.  At this point WX2 is configured to use eth2 only i.e. just one link is being used by the management network and message passing network.

 

&gt; wxtool -a '{can DB}' -S "cat /opt/kognitio/wx2/etc/local_config"
Kognitio WX2 Administration Utility v7.01.02-z110909 on wxsupp01
(c)Copyright Kognitio Ltd 2003-2011.

 

Results:
For node wxsupp01-wx2-rack4-enc1-1 (ecode 0, 89 bytes):
[system]
default_net=eth2

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-2 (ecode 0, 89 bytes):
[system]
default_net=eth2

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-3 (ecode 0, 89 bytes):
[system]
default_net=eth2

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-4 (ecode 0, 89 bytes):
[system]
default_net=eth2

 

[attributes]
nodeclass=full

 

 

3.  Run wxprobe -HN against the database nodes and confirm that the number of reported links ties in with findings from step 1.

 

&gt; wxprobe -a '{can DB}' -HN
Kognitio WX2 Hardware Discovery Tool v7.01.02-z110909 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2011.

 

4 nodes like this one:
  bay 1: ID wxsupp01-wx2-rack4-enc1-1, Status Up
         sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 1
         ram 33754533888, mrpp 3921674240, swap 0, cpus 8, end L.
         Caps DB-IO-DS-RS-MI-DBG-MASTER.
         MPID range from -1 to -1 (bootstamp 0)
         Uptime 20312234 s (860810 s busy)
         Node has been unmapped 0 times

 

 

4. As root edit the local config file on the database nodes with wxviconf -L '{can DB}' and add the extra interfaces as a comma separated list.  In this example... 

 

    default_net=eth2

 

becomes

 

    default_net=eth2,eth3

 

 

5.  Double-check that all nodes have been updated with the changes


&gt; wxtool -a '{can DB}' -S "cat /opt/kognitio/wx2/etc/local_config"
Kognitio WX2 Administration Utility v7.01.02-z110909 on wxsupp01
(c)Copyright Kognitio Ltd 2003-2011.

 

Results:
For node wxsupp01-wx2-rack4-enc1-1 (ecode 0, 94 bytes):
[system]
default_net=eth2,eth3

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-2 (ecode 0, 94 bytes):
[system]
default_net=eth2,eth3

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-3 (ecode 0, 94 bytes):
[system]
default_net=eth2,eth3

 

[attributes]
nodeclass=full
For node wxsupp01-wx2-rack4-enc1-4 (ecode 0, 94 bytes):
[system]
default_net=eth2,eth3

 

[attributes]
nodeclass=full

 

 


6.  Run wxprobe -HN against the database nodes and check that the number of links being used by the SMD has increased to the anticipated number (two in this case)

 


&gt; wxprobe -a '{can DB}' -HN
Kognitio WX2 Hardware Discovery Tool v7.01.02-z110909 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2011.

 

4 nodes like this one:
  bay 1: ID wxsupp01-wx2-rack4-enc1-1, Status Up
         sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 2
         ram 33754533888, mrpp 3921674240, swap 0, cpus 8, end L.
         Caps DB-IO-DS-RS-MI-DBG-MASTER.
         MPID range from -1 to -1 (bootstamp 0)
         Uptime 20312143 s (860809 s busy)
         Node has been unmapped 0 times

 

 

7. List out the links for each database node.  The output shows that there are now two management and message passing links per node

 


&gt; wxprobe -a '{can DB}' -wF | egrep '(ID|link)'
Kognitio WX2 Hardware Discovery Tool v7.01.02-z110909 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2011.

 

        bay 1: ID wxsupp01-wx2-rack4-enc1-1, Status Up
               sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 2
               MPID range from -1 to -1 (bootstamp 0)
            MPK link  0: IP 14.4.41.1, MAC 00.22.64.9E.A2.2E, MPK Y, Status Up.
            MPK link  1: IP 14.4.105.1, MAC 00.22.64.9E.A2.2F, MPK Y, Status Up.
            MAN link  0: IP 14.4.41.1, MAC 00.22.64.9E.A2.2E, MPK Y, Status Up.
            MAN link  1: IP 14.4.105.1, MAC 00.22.64.9E.A2.2F, MPK Y, Status Up.
        bay 2: ID wxsupp01-wx2-rack4-enc1-2, Status Up
               sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 2
               MPID range from -1 to -1 (bootstamp 0)
            MPK link  0: IP 14.4.41.2, MAC 00.22.64.9E.82.D2, MPK Y, Status Up.
            MPK link  1: IP 14.4.105.2, MAC 00.22.64.9E.82.D3, MPK Y, Status Up.
            MAN link  0: IP 14.4.41.2, MAC 00.22.64.9E.82.D2, MPK Y, Status Up.
            MAN link  1: IP 14.4.105.2, MAC 00.22.64.9E.82.D3, MPK Y, Status Up.
        bay 3: ID wxsupp01-wx2-rack4-enc1-3, Status Up
               sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 2
               MPID range from -1 to -1 (bootstamp 0)
            MPK link  0: IP 14.4.41.3, MAC 00.22.64.9E.52.CE, MPK Y, Status Up.
            MPK link  1: IP 14.4.105.3, MAC 00.22.64.9E.52.CF, MPK Y, Status Up.
            MAN link  0: IP 14.4.41.3, MAC 00.22.64.9E.52.CE, MPK Y, Status Up.
            MAN link  1: IP 14.4.105.3, MAC 00.22.64.9E.52.CF, MPK Y, Status Up.
        bay 4: ID wxsupp01-wx2-rack4-enc1-4, Status Up
               sys Linux-2.6.16.60-0.42.5-smp, Disks 1, links 2
               MPID range from -1 to -1 (bootstamp 0)
            MPK link  0: IP 14.4.41.4, MAC 00.22.64.9E.02.64, MPK Y, Status Up.
            MPK link  1: IP 14.4.105.4, MAC 00.22.64.9E.02.65, MPK Y, Status Up.
            MAN link  0: IP 14.4.41.4, MAC 00.22.64.9E.02.64, MPK Y, Status Up.
            MAN link  1: IP 14.4.105.4, MAC 00.22.64.9E.02.65, MPK Y, Status Up.



 

 

8.  Restart WX2 with wxserver start [sysimage] for the changes to take effect




HOWTO: Investigate an unabortable session

Solution 00000724 by Simon Darkin at 2019-02-15T14:23:09.000+0000


note: If the query is unioning together multiple external tables (7 or more), this may be a repeat of cases 21913, 21983





Once you have established that a session cannot be aborted you will need to go through the following steps in order to capture the correct debug information:-








1. Look in the serverdbg file for the session in question and check to see if there is any record of it not being aborted due to being a solo operation. If there are entries of that nature then there is no need to go through the remaining steps. 


 


2. Collect a history file


 


3. Run a diagnose of the query (note this could cause a second hung session so only do this if it doesn't matter)


 


4. Capture the entire contents of IPE_DISK_ACCESS and IPE_ALLRAM_ACCESS for the session in question and send to development straight away asking them if they can identify which ramstores should be dumped out. Usually you'll be required to dump out a ramstore and wait for it to be debugged in order to find out what else should be dumped off. This is likely to be an iterative process. Be aware that dumping out a WX2 process carries a small risk of crashing the server so this should be scheduled with the customer for a convenient time usually out-of-hours. 


If development are not available then look in IPE_ALLRAM_ACCESS for the session in question and try to find examples of ramstores that stand out as different, perhaps because they have a different number of accesses or they have a different state to the others. If there are no obvious candidates then try to categorise the ramstores into groups and dump out an example from each group. *EDIT*: Andy M suggests doing the following. Look in ipe_allram_access, and look at the num_children column. Usually the hung process will have 0 children, so dump off anything with 0 children. You can also look at the parent_mpid column - the hung process will often be the only one which is not the parent of any other row in ipe_allram_access, i.e. it's at the bottom of the tree formed by parent_mpid and should therefore be dumped off. Also each unique STATE string should be dumped off. 


 


5. Identify the MPID of the IO node through which the session connected and dump it out.


 


i. Retrieve net_if_name from IPE_ALLCURSESSIONS


 


select 


net_if_name 


from ipe_allcursessions


where session = &lt;unabortable-session&gt;;


 


ii. Take the last 4 digits from net_if_name and convert to decimal to get the MPID


 


iii. Run wxprobe -a '{mpid &lt;mpid&gt;}' -l to identify the appropriate pid and IP address then dump it out 


 


 


6. Identify the MPID of the interpreter and dump it out


 


i. Retrieve any request_id for the unabortable session


select 


max(request_id) request_id


from ipe_allram_access 


where session = &lt;unabortable-session&gt;;


 


ii. convert the request_id to hex then take the last four digits and convert to decimal to get the MPID


 


iii. Run wxprobe -a '{mpid &lt;mpid&gt;}' -l to identify the appropriate pid and IP address then dump it out 


 

7. dump off the mpid column from ipe_allcursessions.  The mpid column, when converted to hex, will give a number like 0x02000271. Take the non-zero digits from the end and convert back to decimal. This will be the mpid to be dumped off, and it will be either an interpreter or compiler. 

For example, if the mpid column is 33555089, convert this to hex ( 0x02000291) and take the last non-zero digits (291), convert this to decimal (657) and dump that off. 


 


8. Capture a full dump with wxdgtool -D


 


9. Send all of the output from the previous steps to development


 


10. Schedule a restart of WX2 in order to clear the session once you're satisfied that all of the required core files have been captured.  Note: sometimes a restart is not necessary, if the session is waiting for an external script to exit. These queries can be aborted by killing the relevant processes and their children. In this case you will see 'XIWait' in the state field of a row in ipe_allram_access for the session, which means it is waiting for an external process to exit. Take the mpid and request_id for that row, and run wxtool -a '{mpid &lt;mpid&gt;}' -S 'ps -o pid,pgid,cmd | grep &lt;request_id&gt;' to see the relevant external process, along with it's PGID (process group ID). Take the PGID (second column), and run wxtool -a '{mpid &lt;mpid&gt;}' -S 'ps -o pid,pgid,cmd | grep &lt;PGID&gt;' to see a list of processes on that node with the given PGID. The PGID is important as external scripts can start child processes which also need to be killed (KAP will wait for the external script and all it's children to exit before the query commits). From the resulting processes, kill any ones which are related to the given external script by running wxtool -a '{mpid &lt;mpid&gt;}' -S 'kill -9 &lt;pid&gt;'. Then in the same way kill any other processes with corresponding XIWait rows in ipe_allram_access.  Then the query should commit. 



HOWTO: Run the disk soak test

Solution 00000736 by Deborah Martin at 2012-01-13T14:56:12.000+0000

The idea of the disk soak test is to generate some data, insert into some tables, truncate the tables 


and start all over again looping as many times as you want it too. 


 


Untar the attached file to the wxadmin home directory. This will create ~wxadmin/dbsoak ~wxadmin/dbsoak/sql ~wxadmin/dbsoak/logs 
Run the dbsoak.sh script in the dbsoak directory and it will show you all the options available. 



Usage : ./dbsoak &lt;dsn&gt; &lt;sys password&gt; &lt;option&gt; 


where options are : 1) SETUPDATA | setupdata  = Setup the tables
                             2) BULKDATA  | bulkdata     = Bulk the data a bit
                             3) DISKSOAK  | disksoak    = Run the disk soak
                             4) CLEANUP   | cleanup       = Clean up
                             5) ALL       | all                    = All of the above



 


e.g ./dbsoak localhost albatros ALL 


The above example will connect to the database, setup what it needs and then run a full soak  


 


You can also target specific bits if you want to maybe test any changes you've made before running the whole lot in full 


./dbsoak localhost albatros SETUP 


./dbsoak localhost albatros BULKDATA


./dbsoak localhost albatros DISKSOAK


./dbsoak localhost albatros CLEANUP 


 


dbsoak.sh will run one or more of the following. The .SQL files are located in ~wxadmin/dbsoak/sql 



1_SETUPDATA.SQL


This will setup the tables and insert some data into tab1 which should always go on the highest numbered slab. 


You may want to change the slab allocations here as tab1 is always on the smallest user data slab with the 


rest of the tables allocated to the remaining user slabs. Also, for every other user slab you'll need to expand 


the number of tables being created and tested with So for example if you have 4 user slabs then create 


test1 , test2 , test3 and test4 and assign to their own separate slabs in 1_SETUPDATA.SQL 


Then amend 3_DISKSOAK.SQL and add additional lines for the test&lt;n&gt; tables as appropriate. 


You will also need to amend 4_CLEANUP.SQL to ensure all the relevant tables are then dropped. 



2_BULKDATA.SQL
This will insert-select data from tab1 into test1, test2, .... 



3_DISKSOAK.SQL
This will insert..select into test1, test2, .... then truncate the tables in a loop. 
The default is set to 10000 but you can edit the 3_DISKSOAK.SQL file and change that to run alot 


longer or even sooner.



4_CLEANUP.SQL


This will drop all the tables created so that space is reclaimed 


 






Attachment: dbsoak.tar,

HOWTO: How do I add or introduce a standby disk to an existing Kognitio instance?

Solution 00000747 by Simon Darkin at 2016-06-29T11:29:35.000+0000

Adding a standby disk to an existing Kognitio instance


 


1. ensure that the node with the standby disk has the same version of Kognitio software installed as the existing nodes, and (if identical hardware) the same drivers and firmware version. For example, use 'ethtool -i eth2' to check driver/fw version for eth2, and similarly for other interfaces.
2. edit the config file and add num_standby_disks=1 to the [wxsmd] section.  Add reliability_features=yes to the same section if it is not already present.
3. copy /opt/kognitio/wx2/etc/config file from an existing node onto the node with the standby disk 
4. start the SMD on the standby node with "wxsvc start" and check the new disk can be seen with "wxprobe -wD".  At this point the uid should be something like &lt;hostname:partition&gt;
5. run "wxtool -Z" to zero the standby disk




Note that B41278 which went into 8.1.0rel150709, increases the size of the write buffer to speed up disk zeroing.  With this change KAP can become unresponsive when zeroing disks on nodes that are running DB processes, and so it is recommended that disks be zeroed before RAM and CPU resources from those nodes has been introduced into the system.  


 


You can check progress of the disk format by reviewing the output file in the current SMD directory on the standby node. Once complete you will see a message indicating the uid has been replaced e.g.



...
30-05_14:34:38_BST: Stdout: Replaced DISK UID wxsupp02-wx2-rack4-enc1-5:/dev/cciss/c0d0p2 with WXD:4FC62185:00000001, uid_replaced 0.
30-05_14:35:11_BST: Stdout: Format uid wxsupp02-wx2-rack4-enc1-5:/dev/cciss/c0d0p2 resource /dev/cciss/c0d0p2 complete, rc 0.


 


6  run "wxserver start sysimage without recovery" to introduce the disk, RAM and CPU resource from the new node. Review the clustermap in the current startup directory to check the new disk uid is present as a standby disk.


 


&gt; cat clustermap 
       Uid             |     Raid    | state | mpid | cost | local | node
                       |(cluster,pos)| state |      |      |       | 
WXD:4FC622B1:00000001  |(     -1,  0)|     5 |      |&lt;standby disk&gt;| wxsupp02-wx2-rack4-enc1-5
WXD:4FC61E93:00000001  |(      0,  0)|     1 |    9 |    0 |     1 | wxsupp01-wx2-rack4-enc1-1
WXD:4FC61E93:00000002  |(      0,  1)|     1 |   19 |    0 |     1 | wxsupp01-wx2-rack4-enc1-2
WXD:4FC61E93:00000003  |(      1,  0)|     1 |   29 |    0 |     1 | wxsupp01-wx2-rack4-enc1-3
WXD:4FC61E93:00000004  |(      1,  1)|     1 |   39 |    0 |     1 | wxsupp01-wx2-rack4-enc1-4


 


The database is now running with a standby disk.




7. Consider running a shape test on the new system to ensure no problems have been introduced by adding the node. (This has been seen before on e.g. ica01)


 


Replacing a failed disk with a standby disk





The following steps are not part of the integration procedure but just demonstrates how the standby disk can be introduced so *DO NOT* do this unless you are performing a test or actually want to replace a failed disk with a standby.


 


1. ensure reliability features are on
2. stop the database with "wxserver stop"
3. stop the SMD on a non-standby node with "wxsvc stop" (Only required if testing otherwise skip this step)
4. start the database with "wxserver start" and assuming the default setting for the reliability_features are enabled expect to see three failed attempts to restart due to the missing disk and then a fourth succesful attempt as the missing disk is permanently replaced by the standby. 


 


The first three boot attempts will fail as by default reliability features will try to reboot three times looking for all hardware to be present before trying the standby disks


 


&gt; wxserver start
Kognitio WX2 System Controller v7.02.01-rel120509 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2012.


   --&gt;  Cleaning up unwanted files/processes.
   --&gt;  Examining system components.
   --&gt;  Configuring WX2 software.
Generation results:
ERROR: 1 disks missing.
Logging startup to /var/log/wx2/logs-wxsupp01/startup.T_2012-05-30_14:51:38_BST.
   --&gt;  Cleaning up unwanted files/processes.
   --&gt;  Examining system components.
   --&gt;  Configuring WX2 software.
Generation results:
ERROR: 1 disks missing.
Logging startup to /var/log/wx2/logs-wxsupp01/startup.T_2012-05-30_14:51:44_BST.
   --&gt;  Cleaning up unwanted files/processes.
   --&gt;  Examining system components.
   --&gt;  Configuring WX2 software.
Generation results:
ERROR: 1 disks missing.
Logging startup to /var/log/wx2/logs-wxsupp01/startup.T_2012-05-30_14:51:49_BST.
   --&gt;  Cleaning up unwanted files/processes.
   --&gt;  Examining system components.
   --&gt;  Configuring WX2 software.
Generation results:
WARNING: Attempting to replace disk uid WXD:4FC61E93:00000004 with WXD:4FC62185:00000001.
WARNING: Will permanently replace disk WXD:4FC61E93:00000004 with WXD:4FC62185:00000001.
   --&gt;  Initialising Database.
   --&gt;  Loading system tables, user tables and view images
   --&gt;  Starting automatic disk recreate from standbys
Completed crimage in 00:00:44.
Startup complete.  SERVER IS NOW OPERATIONAL.


 


The default behaviour for the reliability_features is to automatically recreate the standby disk so check ipe_xor_element to ensure the recreate is underway.



&gt;select mpid, status from ipe_xor_element order by 1;
       MPID|     STATUS
         18|          1
         37|          1
         55|          1
         73|          4
Query           1               4 rows     ----   0:00.2   0:00.2   0:00.2




If the recreate didn't start automatically, you can start it manually with the command 'recreate disk &lt;mpid&gt;'.  A disk status of 0 indicates a bad disk, 1 is a good disk, 4 is recreating, 5 is awaiting reconfigure, 7 is recreating parity. 



HOWTO: How do I determine if Kognitio is down?

Solution 00000757 by Kognitio Support at 2012-07-12T09:56:38.000+0000


If you are unable to connect to a Kognitio system or run queries on it, there are a number of possible causes. The most likely are:
1.There is a network connectivity issue
2.The DBA is performing an administration task and has locked out other users
3.The server has exhausted some resource (e.g. interpreters for running SQL, or client connections allowed)
4.The server is down
To determine what is causing the problem in your instance, there are a number of steps you can take:





1) Get a Linux command line prompt on a node in the 
system as the wxadmin user, or another user setup to use the Kognitio 
command line utilities. Then use "wxserver info state" to verify whether
 the server is running. If the following output appears, the server 
should be running - i.e. you are not in state (4) from the list above:


Current state: Booted.

Goal state: Booted.

Second Goal State: UNKNOWN.

Status: OK.

Current operation: .


If the output Status line is as follows the server is down. Follow the instructions here to collect information and restart the system:
Status: Fatal error entering state - ERROR: DES crash detected by monitor..


Note that it is possible to access the command line for a node but not connect to it via ODBC due to e.g. firewall restrictions, so being able to access the command line does not eliminate a networking problem as the cause of the original connectivity issue.

2) You
 can verify that all is well by using "wxsubmit -s localhost &lt;your 
Kognitio username&gt; -p &lt;your Kognitio password&gt;" from the Linux command line on a database node. This should 
bring up a prompt allowing you to run SQL.

If this connection attempt, or a connection attempt from your client, 
returns error "AH0001" this indicates an administration task in underway (i.e. state (2) from the list above) - contact your 
database administrator to find out what administration is being performed, and 
when the system is expected to be available.

If the wxsubmit mentioned above allows queries to be run but your original client did not, then it is most likely that your original problem was caused by a network issue (state (1) from the list above). Contact your network administrator for further investigation.

3) Try to connect as the SYS user - some resources such as interpreters for running SQL are reserved for SYS, so if you can connect and run queries as SYS but not as another user the problem is either that the administrator has locked other users out (using login restrictions / queue pausing), or that some resource has been exhausted. Check with your DBA for the first case (or if you are the DBA verify that login restriction / queue pausing are not causing the problem). For the second case, you can either identify which client is hogging resources by looking in IPE_ALLCURSESSIONS from your SYS connection, or you can collect information to allow later analysis of the problem and restart the server to clear the issue using the information here.

4) If you cannot resolve the problem, the instructions here explain how to collect information and restart the system.



HOWTO: More information on collecting information after a DB crash/hang

Solution 00000758 by Kognitio Support at 2017-01-05T08:33:22.000+0000

This solution is for INTERNAL USE ONLY. It details how to collect information on a server crash/hang to give the development team the best chance of identifying the underlying problem. It should not be made accessible to external users, as the steps involved can be somewhat convoluted.



1) DETERMINE WHAT TYPE OF CRASH/HANG HAS OCCURRED.



In the case of a ramstore, diskstore, interpreter or compiler crash, you will be
required to dump off debug information from one of the individual crashed
nodes. Debug information for other types of crashes can be extracted with a
standard hands-off dump.

First run wxprobe -s. This will give you a quick readout of the processes that have crashed, if any. If you see the following output from wxprobe -s then there are no crashed processes at this point in time.

&gt; wxprobe -s

Kognitio WX2 Hardware Discovery Tool v6.01.07-r on wx2-system1

(c)Copyright Kognitio Ltd 2001-2008.

No problems found.




However, if the system has hung you should go to step (2.2) below to capture general information on the state of the system before restarting. If there are crashed processes from wxprobe -s, then the output will look similar to the example below. However, wxprobe -s output is not always in the same order that the processes crashed in. To see the correct order of events (assuming there were any crashed processes), you will need to view the monitor file on the master smd node. Run wxserver info master to see which node is the current master, then ssh to that node and run less `wxlogd smd`/monitor*. After browsing to the correct time period you should see similar output to wxprobe -s. 

The important crash to look for is the first non-watchdog process that crashed. In the example below, this is process 28286. 

T_2015-07-07_10:34:57_BST: Detected a crash on node madison01-wx2-rack1-enc1-1: 
T_2015-07-07_10:34:57_BST: Process 28261: Name WXDB(WATCHDOG), nthreads 1, state T, size 15044608. 
T_2015-07-07_10:34:57_BST: : ppid 8609, pgid 28261, tracerpid 0, time 20(10,10)+0(0,0). 
T_2015-07-07_10:34:57_BST: : status Crashed(ERROR: Child pid 28286 stopped (probably crashed)). 
T_2015-07-07_10:34:57_BST: : mpid -1, type WATCHDOG 
T_2015-07-07_10:34:57_BST: SERVER CRASH! 
T_2015-07-07_10:34:57_BST: Detected a crash on node madison01-wx2-rack1-enc1-1: 
T_2015-07-07_10:34:57_BST: Process 28286: Name WXDB(0): Ramstore, nthreads 54, state T, size 3994587136. 
T_2015-07-07_10:34:57_BST: : ppid 28261, pgid 28261, tracerpid 0, time 464(203,261)+17769796(5453491,12316305). 
T_2015-07-07_10:34:57_BST: : status Crashed(ERROR: Signal SEGV at EIP 0x04DFE2FE data address 0x4dfe2fe: address not mapped to object). 
T_2015-07-07_10:34:57_BST: : mpid 0, type Ramstore



Look for the "type"
in the last line of the output to identify what type of process has crashed
e.g. ramstore, diskstore, io node etc.






2) RETRIEVING THE CORE FILE INFORMATION



Having established in step 1 that a crash has occurred
you now need to extract the debug files and compress them ready to be sent to
Kognitio. Follow the steps in 2.1 and 2.2 for ramstore, diskstore, compiler or interpreter crashes,
for any other type of crash go straight to step 2.2 .



If the monitor file shows multiple crashed processes, it's normally sufficient to only dump off the first instance of each type. The exception to this is if the status message points to another process. This is shown in the example below:

T_2015-06-20_18:33:52_BST: Monitor started. 
T_2015-06-21_15:47:54_BST: Detected a crash on node bbb01-wx2-rack1-enc1-13: 
T_2015-06-21_15:47:54_BST: Process 15430: Name WXDB(WATCHDOG), nthreads 1, state T, size 16502784. 
T_2015-06-21_15:47:54_BST: : ppid 1, pgid 15430, tracerpid 0, time 31(13,18)+0(0,0). 
T_2015-06-21_15:47:54_BST: : status Crashed(ERROR: Child pid 15522 stopped (probably crashed)). 
T_2015-06-21_15:47:54_BST: : mpid -1, type WATCHDOG 
T_2015-06-21_15:47:54_BST: SERVER CRASH! 
T_2015-06-21_15:47:54_BST: Detected a crash on node bbb01-wx2-rack1-enc1-13: 
T_2015-06-21_15:47:54_BST: Process 15522: Name WXDB(353): Interpreter, nthreads 33, state T, size 94523392. 
T_2015-06-21_15:47:54_BST: : ppid 15430, pgid 15430, tracerpid 0, time 6(3,3)+78785(37290,41495). 
T_2015-06-21_15:47:54_BST: : status Crashed(ERROR: Too many resends to 286. ). 
T_2015-06-21_15:47:54_BST: : mpid 353, type Interpreter 
...

In this example, the first non-watchdog process (pid 15522, mpid 353) crashed with "too many resends to 286", meaning that it tried repeatedly to send data to another process (mpid 286) and was unable to do so. In this case, both mpid 353 and mpid 286 would need to be dumped off, since mpid 286 may have been unresponsive and hence needs to be investigated. To find the Linux pid corresponding to mpid 286, you would run: wxprobe -l | grep -B 3 "mpid 286,". for example:

&gt; wxprobe -l | grep -B 3 "mpid 286,"

Kognitio WX2 Hardware Discovery Tool v8.01.00-rel141124 on wxsupp01

(c)Copyright Kognitio Ltd 2001-2014.

  Process 10532: Name WXDB(286): Ramstore, nthreads 56, state S, size 3935567872.

              : ppid 10502, pgid 10502, tracerpid 0, time 238(158,80)+72625(18973,53652).

              : status Running.



              : mpid 286, type Ramstore

To find the node that process was running on, run: wxprobe -a '{mpid 57}' -l | head -n1






2.1) CAPTURING SPECIFIC INFORMATION FOR LARGE PROCESSES
(Ramstore / diskstore crashes)



If any node in the system running an SMD contains a linux
partition that has more than 10GB disk free, then you can run the dump command
from there. This is referred to as a "local dump" . If none of the
nodes running an SMD have at least 10GB disk free in a linux partition then you
must dump the information to a remote node, referred to as a "remote
dump". Decide whether you are going to dump from a local or remote node
and chose either option i) or ii) below as appropriate.






i) WHEN THERE IS A NODE WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (LOCAL DUMP)



The syntax to target a crashed process from a node within
the system is: wxdgdump -t &lt;target-host-ip&gt; -p &lt;crashed-process-id&gt;
-C &lt;core-filename&gt;

The target-host-ip for any node can be found by running: wxprobe -i and looking at eth2 interface for that node.



It's helpful to specify the type of crash and mpid of the
crash in the output filename so using the monitor file output from step 1 as an
example you would run the following to dump off the crashed process:-



&gt; cd &lt;volume with at least 10Gb free disk space&gt;



&gt; wxdgdump -t 10.1.2.3 -p 28286 -C rs0.core



&gt; gzip rs0.core

Repeat the above steps for any other processes that need to be dumped, then go to step 2.2 to gather general debug information.






ii) WHEN THERE ARE NO NODES WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (REMOTE DUMP)



If no nodes in the system have sufficient disk space free
to store a core file (typically these can be several Gbytes) then you should
identify a node outside of the system which does have sufficient space and ssh
from there.



The syntax to target a crashed process from a remote node
is "ssh wxadmin@&lt;externally-accessible-node&gt; wxdgtool -o - -c
\'"target {&lt;target-host-ip&gt;},pid &lt;crashed-process-id&gt;"\'
-d &gt; &lt;core-filename&gt;



It's helpful to specify the type of crash and mpid of the
crash in the output filename so using the monitor file output from step 1 as an
example you would run the following:-



&gt; cd &lt;volume with at least 10Gb free disk space&gt;



&gt; ssh wxadmin@10.1.2.3
wxdgtool -o - -c \'"target {10.1.2.3},pid 28286"\' -d &gt; rs0.core



&gt; gzip rs0.core



Repeat the above for any other processes that need to be dumped, then go to step 2.2 to gather general debug information.






2.2) CAPTURING GENERAL DEBUG INFORMATION



First take a copy of the logging directory. Connect to
any node in the system, navigate to /var/log and tar the wx2 directory as
follows:



&gt; ssh wxadmin@any-node-in-system



&gt; cd /var/log



&gt; tar czvf /tmp/logfiles.tgz wx2



You will now need to use "wxdgtool -D" to
generate info, history and wxd files, with the wxd file containing the relevant
core files.



If any node in the system running an SMD contains a linux
partition that has more than 10GB disk free, then you can run the dump command
from there. This is referred to as a "local dump" . If none of the
nodes running an SMD have at least 10GB disk free in a linux partition then you
must dump the information to a remote node, referred to as a "remote
dump". Decide whether you are going to dump from a local or remote node
and chose either option i) or ii) below as appropriate.






i) WHEN THERE IS A NODE WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (LOCAL DUMP)



The "wxdgtool -D" command will create a
timestamped core, info and history file. These should be extracted, tar'd and
compressed as follows:-



&gt; cd &lt;volume with at least 10Gb free disk space&gt;



&gt; wxdgtool -D -O . 



&gt; tar czvf wxdgtool.tgz &lt;core-file history-file
info-file&gt;






ii) WHEN THERE ARE NO NODES WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (REMOTE DUMP)



If no nodes in the system have sufficient disk space you
should identify a node outside of the system which has sufficient space and ssh
from there, e.g. here I'm starting an ssh connection to 10.1.2.3 from a remote
node and running wxdgtool:



remote-node&gt; cd &lt;volume with at least 10Gb free
disk space&gt;



remote-node&gt; ssh wxadmin@10.1.2.3
wxdgtool -D -o - &gt; wxdgtool.wxd



remote-node&gt; gzip wxdgtool.wx






3) RESTARTING THE SERVER



Having extracted the debug information in step 2, the
Kognitio system can be restarted. However different systems need to be started in different ways. Some can be simply restarted with "wxserver start" which will
restart the database and run a "create image" restoring the system to
it's original state prior to the crash. In version 8 and above, "wxserver start" will attempt to start with image recovery, which is much faster than other methods, as it reuses data that are already in memory. However not all systems require this. At the time of writing, startup instructions are held in a 'password file' which is on a shared network drive. Ask a colleague if you don't know where this is. 




4) VIEWING THE HISTORY FILE

Often the proximate cause of a crash can be determined by looking at the last few queries run on the system leading up to the crash. These queries can be viewed in two places: system tables (ipe_command, ipe_transaction etc) and the history file (which was generated by wxdgtool -D). It's important to investigate this, as otherwise a customer might run the same query again after a restart, causing a second crash. Of course not all crashes are triggered by a query. The first thing to note is the exact time when the first crashed process showed up in the monitor file. In the example in step 1, this was 10:34:57. Obviously if a query caused this crash, it must have occurred before this. Note that the problem query will not always be the most recent one before 10:34:57, sometimes it will be a query started at say 10:19:00 which took some time to reach the stage that caused the crash. There's no easy way to be sure which query crashed the system, but the following techniques can help to make an educated guess, and any such guesses can then be relayed to the customer, so they can avoid running them:




wxsqlhist -A -f &lt;history file&gt;              -- this will send a listing of recent queries to stdout. with transaction numbers, session id's etc. These tno's can be looked up  in ipe_transaction and ipe_command to see when they occurred. 


wxsqlhist -f history.T_2015-07-08_22:02:02_BST -qer | grep "mp 0" | tail             -- this will show you which sessions and tno's were running for mpid 0. This is useful if mpid 0 was the first non-watchdog process that crashed. Often the last line of output will be the tno/session which caused the crash


wxsqlhist -f history.T_2015-07-08_22:02:02_BST -t 11289820 -qer                     -- this shows detailed information for tno 11289820

select * from ipe_alltransaction where tno = 11289820                                -- when run in the database, this will show you the time that the tno occurred. 




The nature of the query can also help, for example a "select * from ipe_allschema" is unlikely to have caused a crash, whereas a more complicated query, or a query using newly introduced features, or little-tested features is more likely to be the culprit.




5) MOVING FILES


After the above steps are complete, the dump files need to be moved to a location where the devs can access them for analysis. At time of writing, this is the support box (193.35.206.116) in /debug/helpdesk. So run ssh -A 193.35.206.116, then cd /debug/helpdesk. In this directory there are some directories of the form &lt;company name&gt; and some of the form &lt;system name&gt;. cd to the affected system name if it's there. Otherwise cd to the company name, and then cd to the system name, which should exist as a subdirectory beneath the &lt;company name&gt; directory. In either case, you will then want to create a new directory for that date. For example, mkdir 20150723; cd 20150723.  This will be the destination directory for the dump files. Before retrieving the files, make sure they are compressed. To retrieve the files, run rsync -P &lt;ip of affected system&gt;:/&lt;path to file&gt; ./ for each file. rsync -P will ensure partial files are kept, so if large file transfers get interrupted, you can resume from where you left off by running the same command again. When the files have been copied, run chmod +r ./* to make them readable by the devs. 












 



HOWTO: Get started with backups

Solution 00000761 by Deborah Martin at 2012-07-20T10:47:34.000+0000

wxbackup is a command line tool which can be used to backup your KAP data. 


 


There are a number of different options available :- 


 


Metadata backup 


This will backup the "infrastructure" of the database. So users, object definitions, privileges, etc. will be backed up but the actual data won't be.Useful if you want to recreate the structure of your database on another machine (e.g. a development / test system).



 


Full backup 


This will backup everything.


 


Incremental backup


You must have a full backup completed first to use this option, which will backup the changes since the last full / incremental backup. Note that you need to tie this in with disk recovery operations - the man page for wxbackup has more details.



 


Targetted backup 


You can specify specific schemas and objects in those schemas to be backed if required.


 


Attached is a script which caters for metadata, full and incremental backups, and could be setup in cron as a regular job if required, although be aware of generic issues with running script through cron as discussed here. 


 


A typical weekly backup schedule could look like the following, replacing &lt;YOURDSN&gt; with the DSN name of your KAP instance :- 


 


00 22 * * 0 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt; FULLBASE
00 22 * * 1 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL 
00 22 * * 2 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 3 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 4 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 5 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  LASTINCREMENTAL 


 


Note: LASTINCREMENTAL has --expect-incremental removed. This is so that a reclaim/repack can recover as much disk space as possible. The next backup after the reclaim must be a full backup. 


 


All the options available in the attached script can be run manually from the command line and is only designed to help setup a backup strategy.


 


Further details on options available to wxbackup can be found in the documentation or the man page for wxbackup. 






For restoring data again, there are details in the documentation and the man page for wxrestore. 







Attachment: WX2Backup.sh,

HOWTO: preceding query with "debug" to get timings, etc

Solution 00000763 by Kognitio Support at 2012-07-17T12:57:21.000+0000


It is possible to precede a query with the "debug" keyword to collect extra information on issues like query performance. In all cases, extra information is output in `wxlogd smd` to one or more cidebug files.

The following are all useful:

debug time &lt;query&gt;
shows how long the interpreter spends in each part of the SCode.

debug time 1 &lt;query&gt;
also includes a summary of compiler time before the interpreter time. In the cidebug file you will see something like the following:
17-07_13:24:16_BST: TME: Syntax     0.000092 secs (0.000092)   3.00%



17-07_13:24:16_BST: TME: Resolution 0.002571 secs
(0.002663)  83.72%



17-07_13:24:16_BST: TME: Semantics  0.000062 secs (0.002725)   2.02%



17-07_13:24:16_BST: TME: Generator  0.000341 secs (0.003066)  11.10%



17-07_13:24:16_BST: TME: Final      0.000005 secs (0.003071)   0.16%




debug time 10 &lt;query&gt;
provides information on every step through the query transformer too - do not go above 10 as then the time spent generating the debug output becomes significant.

debug {time [x]} &lt;query&gt;
allows the query to run correctly - the items in {} tell the ODBC driver this is part of the debug statement, and hence it knows to expect results. This is only supported in 7.2.1 and later ODBC drivers.

Be aware that with streaming switched on (i.e. the default mode), some of the interpreter times can be misleading - the interpreter can issue AI requests, with the underlying activity being deferred by the AI in many cases, so steps may appear to complete very quickly because they have been deferred, and their cost will appear under a later operation.



HOWTO: Reclaim space from slab 2 (the logging slab)

Solution 00000765 by Deborah Martin at 2016-10-24T09:26:11.000+0000




If the logging slab (slab 2) becomes full, logging of user connections, transactions and queries is prevented.
To re-enable logging, space needs to be freed up. The most effective way to do this is by copying any log data you want to keep to user tables, then truncating all the tables which reside on slab 2.

As SYS, switch off logging  and then disconnect to make the change effective for your next session:- 

set parameter llev to 0;
disconnect; 

Reconnect. 


Get a global session:- 

Switching off logging is only effective from the start of a session so if there are existing sessions already on the system, logging may still be active for them. You will need to therefore, abort all existing sessions and obtain a global lock to ensure nothing further is logged on slab 2 before truncation. 

update sys.ipe_allcursessions set abort = 2 where session &lt;&gt; current_session;
lock system;




Make a copy of the main logging tables to somewhere else. In this example, we'll create them in the LOG_ARCHIVE schema which needs to exist prior to running anything further. 




a) Find all the tables that reside on slab 2 currently. Occasionally, new tables are introduced and assigned to slab 2 and also some tables are moved from slab 2 to the user slabs. The query below will generate SQL that you can then run separately :- 

select 'CREATE TABLE LOG_ARCHIVE.'||trim(t.name)||' DISK FOR SELECT * FROM SYS.'||trim(t.name)||';'
from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2); 

Note - you may see errors such as "Table &lt;name&gt; already exists" if you've run the above before. However, the above may also catch any potentially new system tables that have been assigned to slab 2. 


b) Archive the data into the tables created in the LOG_ARCHIVE schema. The query below will generate SQL that you can then run separately :- 

select 'INSERT INTO LOG_ARCHIVE.'||trim(t.name)||' SELECT * FROM SYS.'||trim(t.name)||';'
from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2);

c) Truncate the tables that reside on slab 2. The query below will generate SQL that you can then run separately :- 

select 'TRUNCATE TABLE SYS.'||trim(t.name)||';' from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2)

The last truncate should start zeroing of all slab 2 instances which means the space in slab 2 has been recovered, although disk performance will be impacted for a while.




Re-enable logging
Still as SYS, do the following to re-enable logging

set parameter llev to 3;
disconnect;  -- You must disconnect for  the parameter change to be effective. 




Optionally, copy some of the logging rows back. If you have security classes which depend on logging rows (e.g. a check on how long a user has gone without logging in), it is best practice to copy relevant rows back into the logging tables. Note - when you copy rows back, you'll need to ensure that you are not introducing duplicates each time you do this. 





EXAMPLES:- 




insert into log_archive.ipe_command select * from sys.ipe_command where cdate &gt;= current_date - interval '30' day 
except select * from log_archive.ipe_command; 

insert into sys.ipe_command select * from log_archive.ipe_command where cdate &gt;= current_date - interval '30' day 
except select * from sys.ipe_command;



HOWTO: How do I find header information from a row in the database?

Solution 00000766 by Kognitio Support at 2012-07-20T09:13:46.000+0000

 



Each database row has a header that is used internally by
the database.




It can be useful to see this information for the purposes of problem diagnosis (e.g. finding out which transaction changed a row, then looking up the query and user for that transaction).




Header information is exposed in SQL by the following functions; the
transaction number functions have been available for a long time and the other
functions were added in 7.1.2.



 



WX_CREATE_TNO()

Transaction number when row was inserted






WX_UPDATE_TNO()

Transaction number when row was deleted
(or replaced by a new value, because we don't modify rows in-place). Note that deleted rows persist until their space is recovered by a reclaim/repack or other disk space recovery operation.







WX_RAMPROC()

MPID of Ramstore on where this row is located






WX_RAMADDR()

Memory address in Ramstore where this row is
located






WX_DISKADDR()

 Disk address where this row is located






WX_DISKMPID()

 MPID of Diskstore where this row is located






WX_DISKPTN()

Slab on disk where this row is
located



 



The RAM address functions do not apply to
disk-only tables and the disk address functions do not apply to RAM-only
tables.




The location of a row in memory is uniquely described using the
tuple (WX_RAMPROC(), WX_RAMADDR()) and a row on disk is uniquely described
using (WX_DISKMPID(), WX_DISKADDR(), WX_DISKPTN()).




If a table is
replicated then that row will occur once in every ramstore. If you select WX_RAMPROC() or WX_RAMADDR() from a
replicated table, the server will infer that you want to see each row from
every ramstore (i.e. pretend it's random).






HOWTO: How do I use the IPE_MPK* tables?

Solution 00000769 by Simon Darkin at 2016-10-24T09:19:20.000+0000

There are three tables: ipe_mpk_stats, ipe_mpk_link_stats and ipe_mpk_link_peer_stats.  





These provide information on the MPK at 3 levels of granularity:


ipe_mpk_stats has one row for every MPID in the system (every Kognitio heavyweight Linux process has a message passing ID called an MPID).


ipe_mpk_link_stats has one row for every link for every MPID, so a node with 3 links has 3 rows per MPID.



ipe_mpk_link_peer_stats has one row for each destination for each MPID and Link. This means that in a system with 2 links and 100 MPIDs each process will generate 200 rows for this table, 100 for each target MPID on link 0 and one for each target on link 1.





More detail on each of the tables is contained below, and finally some example queries are given to extract useful information from these tables.


 




ipe_mpk_stats
Most of the columns in here are self explanatory. It contains counters for things that have happened on that MPID. Many of these are counters for events that won't make much sense unless you understand the internals of the MPK but some are useful. Interesting values to look at are:
 


frames_resent 
this counts the number of times we had to resend something. This is handy for finding bottlenecks in the system.

noroute_frames

this counts the number of times we couldn't send a message because all links are broken. Should never happen but handy for finding problems.

send_rate and receive_rate


these are the approximate rate of message passing in bytes/second at that point in time. You can sum this over a node to get an idea of how much data is going in/out but this is approximate.


 


nacks_sent and nacks_received
these are an indication of flow control happening. When a node is getting data too quickly for it to handle it starts sending out nacks to tell other nodes to slow down. Things like hashing on a skewed value sometimes do this. By looking at the values in here you can often see where the skewed data is coming from and going too, but be aware there can be quite a bit of backscatter here; when a node is congested it often tells everyone who sends anything to back off rather than just the nodes which are flooding it.


 


deferred packets
The current length of the deferred message queue. This is an indication of memory fragmentation on the node. Messages are usually deferred because they have been received as separate frames but can't be assembled into the target address space. Usually these should be very transient, it is a problem if the same message stays deferred for long.


 


readqueue_toggles and using_readqueue
this happens in certain heavy-traffic situations on the MPK. When the MPK is having trouble delivering messages (because the recipient is busy, memory is fragmented, or whatever) we activate the readqueue which slows the MPK down a little but makes it easier to deal with nacks, defer messages, etc. Version 7.1 used the readqueue all the time.






 


ipe_mpk_link_stats
Stats and counters that relate to traffic for that MPID on a particular link. Here you have the transmission control values for the link:






windowsize
how many unacked messages can be outstanding at any time.





timeout
how long (in ms) do we wait before resending a message.





messages_outstanding
how many unacked messages sent from this mpid on this link are outstanding.
These get adjusted dynamically by the server. They can be an indicator of problems, particularly if the timeout ends up high and the resend count is going up a lot.


 


The link status values are here too:


active 
1 or 0 depending on whether or not we think the link works.





score 
This is used to measure the link's health. Retransmissions reduce the score and good acks received increase it. Below 0 for a period of time causes the link to be set to inactive.





yoyo_count
a link 'yoyo' is a link going down and then coming back up again. When the link goes from down to up the yoyo count increases. At 5 the link cannot be made active again. It goes down by 1 every 5 hours or so until it gets to 0.
Generally speaking these are not used much by the recent versions of the MPK, which just uses the ones in the peer_stats file instead.


 


Then there are counters that are breakdowns of the ones in ipe_mpk_stats above to provide more granularity.  The recent_XXX values are included in the total_XXX ones of the same name and periodically reset to 0,
ipe_mpk_link_peer_stats



This has one row for each 'route' in the MPK. A route is a directional connection between a sending and receiving MPID over a given link. This has route status values (score, active, yoyo_count) which mirror those in ipe_mpk_link_stats above but these ones are the primary ones which actually control which routes are considered 'working' and which aren't. You can get a lot of useful information from these values by aggregating with appropriate grouping to track down where problem links are.


 


peer_mpid
the MPID the route is sending to.




mpid
the MPID the messages come from.


 


out_frames_queued
the current number of frames queued for that destination. This value is actually shared across all links between a given pair of MPIDs because the output queue is shared.


 


messages_outstanding
this breaks down the messages_outstanding value from ipe_mpk_link_stats above. Often if there's a slow node lots of links will have messages outstanding to it and you can use this with aggregation to find things like that out.


 


frames_resent
the number of frames resent on that particular route. This is very useful for drilling down to pinpoint network faults.


 


nacks_sent and nacks_received
these are breakouts of the ones above and are useful for pinpointing the source and/or target of congestion.


 


Often you want to join one or more of these tables to others to perform aggregation. A useful join is between the mpid value (and/or peer_mpid) and the MPID column in ipe_process. This join gives the processor_nr value from the relevant ipe_process row, which is actually the physical node number of the node the process is on. You can use this to group values by physical node so you can, for example, get the count of resends from every physical node to every other physical node and order descending to see if there are excessive resends to a given node. You can also join processor_nr to the node_nr column in ipe_nodeinfo, which has 1 row per node and contains things like the enclosure name of the node. This allows you to aggregate MPK stats up to the enclosure level for finding switch problems, etc.


 


Some example queries:


 


-- find nodes containing outbound routes marked as bad

 select ni.wx2_node_name, link, count(*) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni 
where ni.node_nr = p.processor_nr 
  and p.mpid = s.mpid
  and active = 0
group by 1, 2


 


-- find nodes with bad routes going into them (more useful) 
select ni.wx2_node_name, link, count(*) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni 
where ni.node_nr = p.processor_nr
 and p.mpid = s.peer_mpid
 and active = 0
group by 1, 2


 


-- Sum resends by source and dest enclosure to see if there's a bottleneck


-- if you have a bad switch the top rows all have the same link and dest_enc which tells you which switch

 select ni.node_location as source_enc, p_ni.node_location as dest_enc, link, sum(frames_resent) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni, ipe_process p_p, ipe_nodeinfo p_ni 
where ni.node_nr = p.processor_nr
 and p.mpid = s.mpid
 and p_ni.node_nr = p_p.processor_nr
 and p_p.mpid = s.peer_mpid 
group by 1, 2, 3 
order by 4 desc




-- Check if one node is the target of excessive resends - in which case that node might be slow

-- ideally only do this after restart and shape test, as otherwise the cause might be e.g. skewed queries

select p_ni.wx2_node_name, link, sum(frames_resent)
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni, ipe_process p_p, ipe_nodeinfo p_ni
where ni.node_nr = p.processor_nr
 and p.mpid = s.mpid
 and p_ni.node_nr = p_p.processor_nr
 and p_p.mpid = s.peer_mpid
group by 1, 2
order by 3 desc




-- one way to check for a slow node if previous query suggests problems is to check all nodes can do something CPU intensive

-- in roughly the same time. For example, run the following from the Linux command line on a node and compare real times for consistency:

-- wxtool -S 'time dd if=/dev/zero bs=1024 count=1000000 | gzip &gt; /dev/null'




HOWTO: Generate a metadata backup from a full backup

Solution 00000770 by Simon Darkin at 2012-08-03T09:17:25.000+0000

If you ever need to obtain a metadata backup but find that it's not convenient to run one then you can generate one from an existing full backup by using the "exclude" option in tar to capture everything except for the data.wcb.gz files as follows:-  



tar czvf &lt;tarfile&gt; --exclude=data.wcb.gz &lt;backup-dir&gt; 


 


The resulting tarfile can then be extracted and restored as normal.



HOWTO: Deal with CI0012 errors

Solution 00000771 by Kognitio Support at 2012-08-03T09:34:28.000+0000

"CI0012: Internal memory allocation too large" is an error which occurs when the SQL compiler tries to do an excessively large memory allocation.

In some versions, the server's idea of "excessively large" is too small, which means queries which should be able to run can encounter this error.

To workaround those cases, set the ci_maxalloc current_session parameter to a larger value, and retry the query within that session, then drop the current_session parameter or disconnect. Typically this will be for e.g. altering an object with a lot of dependents, so it is best done in a maintenance window anyway. The SQL to set the parameter is:

set current_session parameter ci_maxalloc to 1000000000






HOWTO: How do I drop a queue from a KAP system

Solution 00000772 by Deborah Martin at 2017-05-15T10:27:33.000+0000

To drop a queue, run "drop queue queue-name;" as SQL.
The following criteria apply for dropping a queue:

•It must not have any members. 
•It is not possible to drop the NONQUEUED and DEFAULT queues created at commission time.




It is worth noting that there is no server overhead for having queues in existence with no associated members.





HOWTO: remove a node from a system for repair/diagnostics

Solution 00000779 by Kognitio Support at 2016-06-14T11:18:08.000+0000

If a node in a system has suspected hardware issues, it can be removed from the system to allow diagnostics to be run in isolation, and/or the node to be repaired.

To do this, use the following checklist:
1.ensure that the system has software RAID on - check in the `wxlogd startup`/clustermap file to ensure RAID clusters have &gt; 1 member each. If software RAID is not already on, you cannot remove a node with one or more disk resources on it.

2.use "wxserver halt" to stop the database processes on all nodes. This prevents processes on the problem node from interfering with the processes on the remaining nodes in future.

3.as root on the problem node, do "/opt/kognitio/wx2/current/bin/wxsvc stop" to stop the smd.
4.as root on the problem node, edit the config file with "vi /opt/kognitio/wx2/etc/config" and
1.set a different system_id under the [general] heading. e.g. "system_id=foo". This prevents the node from rejoining the rest if e.g. it is rebooted. THE SYSTEM_ID MUST BE 11 CHARACTERS OR LESS.

2.change the daemon_port setting under [general], e.g. "daemon_port=1634". This is to prevent users connecting through to any single node test/soak instance that you create. You need to do this even if the node does not have an external interface, as must ensure that e.g. the AP does not manage to connect to it when running ETL/backup.

5.if possible, edit ODBC DSNs everywhere to ensure the removed node is not included in the set of IP addresses for the system, but the list of ServerAddress&lt;x&gt; entries is still contiguous - e.g. if the missing node was ServerAddress5 out of 10, comment it out, and rename ServerAddress10 as ServerAddress5. Ensure this is done for DSNs used by backup, JDBC bridge, AP connections, ... Notify users to do the same if you do not have access to all DSN settings. The only downside if this is not done is that connection attempts may periodically be slow, as they wait for the connection to the missing node to time out. **IMPORTANT** these DSNs need to be changed back to their original settings when the node is reintroduced after repair. So make a note or set yourself a reminder for that.  

6.use wxviconf to edit the config file on the remaining nodes. Add virtual_diskstores=yes to the [boot options] section, to allow restarting with the node missing.
7.restart the database software on the remaining nodes, which will make the system usable again. Note that disk performance will be noticably worse as the database will be using software RAID to reconstruct contents of any missing disks. Performance from RAM will also be impacted by the loss of RAM and CPU.   if the system is running with standby disks then you can replace the failed disk with the standby at this step to quickly restore disk performance.     See the solution "HOWTO: How do I add or introduce a standby disk to an existing Kognitio instance?" for details.



When the node has been repaired, follow the steps in solution 780 (HOWTO: Replace a faulty node in a Kognitio database) to reintroduce it into the system.






HOWTO: Replace a faulty node in a Kognitio database

Solution 00000780 by Simon Darkin at 2019-06-10T07:05:43.000+0000

Prerequisites:-


•the database is configured to use software RAID -  check in the `wxlogd startup`/clustermap file to ensure RAID clusters have &gt; 1 member each. If software RAID is not already on, you cannot remove a node with one or more disk resources on it.

•the database is able to run with virtual diskstores either by having virtual_diskstores=1 in [boot options] and/or by having reliability_features=yes in [wxsmd] in the config file.


1. Remove the faulty node

This stage requires a database restart and so will require downtime to be scheduled if the database is up and running. 

Follow the steps in the solution entitled "HOWTO: remove a node from a system for repair/diagnostics" to ensure the node is safely removed and the database restarted.


2. Re-introduce the replacement node 

NOTE: If the replacement node was rebuilt e.g OS and KAP s/w reinstallation rather than having just h/w replaced, check that the KAP s/w version running on the remaining database nodes matches. If not, install it and change the current_pointer to that version on the replacement node otherwise there is a risk an older version may be used instead which could cause compatibility issues.




This stage requires a database restart and may therefore require some scheduled downtime. 

Note that B41278 which went into 8.1.0rel150709, increases the size of the write buffer to speed up disk zeroing.  With this change KAP can become unresponsive when zeroing disks on nodes that are running DB processes, and so it is recommended that disks be zeroed before RAM and CPU resources from those nodes has been introduced into the system.   

2.1 If the replacement node has identical hardware, ensure that drivers/firmware on the replacement node are identical to the other nodes. For example, use 'ethtool -i eth2' to check drivers/firmware on eth2, and similarly for other interfaces. 

2.2 ensure that the database software and the smd on the replacement node are stopped

2.3 copy the global config file from an existing node to the replacement node ensuring file ownership and permissions are retained
2.4 ensure that the current pointer on the replacement node is pointing at the same version of software as an existing node 
2.5 run wxsvc restart on the replacement node then wait a few seconds and run wxprobe -H to check that the full complement of nodes are present


RESTARTING WITH NEW DISKS - Only execute steps 2.5 to 2.8 if the replacement node has been fitted with new disks or if the UID on the original disks has changed i.e. the node was commissioned as a single node system for testing

2.5 zero the disk on the replacement node with wxtool -Z.   You can monitor the progress of the disk zeroing by viewing the output file in the current startup directory on the replacement node
2.6 once zeroing has completed run wxprobe -wD to confirm the status has changed to disk_is_zeroed 1 for the disk on the replacement node and note the new UID
2.7 view the clustermap for the current boot on the main system and note down the UID for the failed drive i.e. the drive marked as &lt;virtual ds&gt;
2.8 having noted the UID of the replacement disk(s) in step 2.6 (new UID) and the UID of the virtual diskstore(s) in step 2.7 (original UID), run wxserver start [sysimage] without recovery replace uid &lt;original UID&gt; with &lt;new UID&gt;

Typically you would include the sysimage option if the client has an imaging script or if the normal restart instruction for that system is a system image rather than a full image (which is usually the case if their create image time is prohibitively long ).


If you need to restart with multiple disk replacements then use wxserver start [sysimage] without recovery replace uid &lt;old-uid-1&gt; with &lt;new-uid-1&gt; uid &lt;old-uid-2&gt; with &lt;new-uid-2&gt; ...


RESTARTING WITH ORIGINAL DISKS - Only execute step 2.9 if the replacement node is using the original disks with unaltered Kognitio partitions

2.9 run wxserver start [sysimage] without recovery to restart the database 


3. Final checks


Connect to the database and run select mpid, status from ipe_xor_element order by 1; to check the status of the disks.  If the system is set to automatically recreate disks then you will see that the re-introduced disks have a status of 4 indicating that they are recreating.  If the system is not set to recreate disks automatically you will need to recreate the disks manually with recreate disk &lt;mpid&gt; for each disk with a status of 0. Note that recreates run in the background, so the recreate disk command will return immediately, but the disk recreate will continue running for some time.  The disk status will be set to 1 once the recreate completes at which point the disk re-integration is complete.

Finally, if any DSNs (e.g. odbc.ini) were previously edited to remove the faulty node's IP, ensure they are changed back.




*********** REMEMBER TO SCHEDULE A PARITY CHECK *********
  



HOWTO: Change the port used by the Kognitio Database for ODBC connectivity

Solution 00000781 by Simon Darkin at 2012-10-23T11:05:11.000+0000

Note the daemon port must be in the range 1025 - 9999 inclusive.


 


1. stop the database with wxserver stop
2. as wxroot edit the Kognitio global config file and add daemon_port=&lt;port-no&gt; to [general]
3. restart the database with wxserver start [sysimage]
4. edit the odbc.ini file and add the new port number to the appropriate DSN on any Linux client sytems that have been setup to access the database e.g.


 


[some-dsn]
ServerAddress1=&lt;IP address 1&gt;
ServerAddress2=&lt;IP address 2&gt;
ServerPort1=&lt;port-no&gt;
ServerPort2=&lt;port-no&gt;


 


5. for Windows based clients, edit the DSN entry via ODBC Data Source administrator in order to specify the new port number


 




HOWTO: write efficient queries for Kognitio

Solution 00000784 by Kognitio Support at 2012-11-21T10:31:27.000+0000



This solution contains advice for writing efficient queries. A lot of the tips
below are common to many other database products as well as Kognitio.









A) minimise memory requirements for your query

Kognitio uses buffers for each internal access. The larger the rows in an
access, the less rows fit in a buffer, and the longer the query will take in
general. Therefore, it makes sense to minimise the memory requirements for each
step of a query, particularly steps in the query which are dealing with large
numbers of rows.

1) Do not use VARCHARs for very small strings

VARCHAR fields have an 8 byte overhead per record, so it is not advisable to
use them for strings of less than 10 bytes. It is also not advisable to use
them for strings where the variation in length is very small, as the 8 byte
overhead will never be recovered.

2) Do not join or group on large strings

Minimising the number of large strings in each buffer will speed up a query. In
addition, much more computation is required to join/group on large strings
compared to performing these operations on e.g. an integer.



 



3) Avoid
carrying large strings through a query

Similarly to point (2) above, large strings slow down queries. Wherever
possible, use a small numeric type instead, then use a JOIN or CASE statement
at the end of the query to convert back to a string if required.

4) Use view images rather than table images when possible

View images have a smaller per-row overhead than table images. However, they
are a "snapshot" of the underlying data, so the images need to be
recreated to reflect changes in that data.

5) Do not specify very large VARCHAR fields if these are not required

Kognitio client software, such as the ODBC driver, determines how many rows can
be retrieved from the server by dividing its fetch buffer size by the maximum
size of a row. So specifying lots of unnecessarily large VARCHAR fields in an
object will ensure fewer rows are retrieved at a time when querying that
object, resulting in poor performance.


B) Do not make the server do unnecessary work





Often you will know a lot of information about the data that the server does not know, and you can take advantage of this to prevent the server doing unnecessary work.






1) Do not group on expressions unnecessarily

If grouping on something which is already known to be unique, do not add other
expressions for grouping, as these introduce more computational work for no
benefit.

2) Use UNION ALL rather than UNION if you know no duplicates will be
generated

UNION will attempt to remove duplicates across the data, which is an expensive
operation for large data sets with no duplicates. So use UNION ALL whenever it
is applicable to avoid this unnecessary processing.

3) Avoid unnecessary DISTINCT operations

Similar to the above, if you know a column is unique, do not add a DISTINCT as
that will be an expensive operation on an already-unique column.

4) Group by the most selective columns first (ideally non-string columns)
and keep nested grouping order consistent

If grouping by a set of columns, group by the most selective ones first. Keep
the order of grouping consistent if nesting grouping.



So 



               
GROUP BY l, c, d, t) x



               
GROUP BY l, c, d, t y



               
GROUP BY l, c, d) z



Should perform better than



               
GROUP BY t, c, l, d) x



               
GROUP BY l, c, t, d)
y              
&lt;&lt;&lt; arbitrary change in grouping column order



               
GROUP BY l, c, d) z




5) Remove duplicate values on ETL, rather than in a query

If your data has duplicates, remove these during ETL processing rather than
adding overhead for every query to remove them with e.g. window functions.










C) Skew, imaging, statistics





For best performance, you need to understand about skew which results in a small number of processes becoming the bottleneck on a query. Also, using imaging to speed up queries, and collecting statistics to give the SQL optimiser as much information as possible.






1) Watch out for skew, and then remove it

Monitor the Kognitio system for skew. Look at memory usage during a query and
see if it skews onto one or a small number of ram store processes. If this is
the case, consider how the query / memory distributions could be modified to
remove the skew. For example, partial hashing can be used to deal with a small
number of skewed values, or perhaps they are not of interest in the final
result set and could be filtered out at an early stage in processing.

2) Consider imaging intermediate objects when investigating performance
issues

This can help identify skew values. It can also be useful in the final
production system as having images allows the SQL optimiser to make better
decisions for subsequent steps of processing.

3) Collect statistics on frequently used objects

Collecting statistics on objects gives the SQL optimiser the best chance of
producing an optimal query plan.







 



 



D) disk performance advice



 

Kognitio is in in-memory system, with a persistence layer on disk. There are a number of ways to improve performance when e.g. doing transformations to disk-based tables, or bringing disk-based data into memory for analysis. 







1)
Consider truncation then re-insert rather than update

If transforming a disk-based table to change a high percentage of rows,
consider doing the transformation in RAM, truncating the original table, then
writing back the new rows from RAM (all in one transaction, of course). This is
because an update which hits a high percentage of rows can be a lot slower than
generating a new copy of the table in RAM, truncating the old table, then
inserting the new rows back to disk.

2) Use Compressed Data Maps (CDMs) for disk-based tables which are accessed
by clustered attributes

If you have a large table with no RAM image, and you frequently access it by an
attribute which the data is clustered on when it is loaded (e.g. date /
region), try adding a CDM on that attribute to speed up the data load. This is
worthwhile even if you only do this imaging as part of daily/weekly processing.

3) create VIEWs on top of objects, rather than creating new tables from them
with a large number of rows

Avoid unnecessary duplication of data by creating views (and possibly then
imaging those views in a separate statement), rather than creating new tables
which copy a lot of existing rows into the new table.




4) Ensure that disk-based tables involved in queries are defragmented

If a disk-based table is involved in a query, see if scanning that table seems
slow for the number of rows in the table (remembering that deleted rows will be
scanned, but truncated ones won't be). 

If the scan is slow, look to defragment the table with something like
"alter table t1 set slabs to all migrate defrag", but with the
appropriate target slabs specified. 

Note this should not be done if the table is very large (as a lot of disk space
will be used), and the operation will run more quickly if the table has no
memory image at the time.

If the table is small and used regularly (e.g. it is a status table updated by
an ETL process), consider always having it in RAM to prevent repeatedly
scanning the fragmented table on disk.











 



E) Best practice






There are a number of steps which are best categorised as best practice - these improve maintainability of queries, and ensure full advantage is taken of the Kognitio platform.






1) Avoid non-deterministic functions

Floating point aggregation, and window functions in which the partition and
ordering do not deterministically order the rows, can cause confusion as
multiple runs of the same query do not generate identical results.

2) Use INNER joins rather than OUTER joins where possible

If you know your data well enough, ensure you use INNER joins whenever possible
rather than OUTER join "just in case" - outer joins are more costly,
so if they are known to be unnecessary, remove them. 

3) Use a development system to work on significant new projects

Having a separate development systems isolates your changes from production.
Currently you can use Kognitio with up to 128GB RAM without having to pay for any
licence (although you will only get community support with this offering).

4) Upgrade to the latest version of Kognitio software regularly on all
systems

This simplifies administration as you can use the same tools on all systems,
and take advantage of performance enhancements, new functionality and bug
fixes.

5) Read about features in release notes, so you can take advantage of them

New feature often simplify application coding, and improve performance with no
additional investment.

6) Raise cases for all non-trivial issues

This allows Kognitio support to assist with problems, and also feeds into
future product development as common issues are identified.







HOWTO: set disk slab size

Solution 00000788 by Kognitio Support at 2013-01-23T13:36:31.000+0000

To set the logical user data size for each slab in the system, you should add the following to the [fs] section of the config file using wxviconf. In this case we are setting the logical user data size for each slab to 5GB. Note that the minimum is 1GB and the maximum (and default) is 16GB:

max_slab_gb=5

The primary reason for changing this setting from the default is to give more control over slab assignment, by having smaller slabs. Note that the setting only takes effect when the system is recommissioned - it is not possible to resize slabs on an existing system without recommissioning.

In future releases the number of slabs to use will be controllable, to ensure that the same number of slabs are available on dev/test and production systems, simplifying administration on those systems.


HOWTO: stop Kognitio from using a specific network interface for internal traffic

Solution 00000791 by Simon Darkin at 2013-03-12T16:49:36.000+0000

Should there be a requirement to stop the Kognitio database from using a particular network interface for internal traffic, the local config file can be updated on all nodes to include an entry that defines only those interfaces to be used.  Note that the list of defined interfaces must be identical on *all* nodes. 

 

1. Open the local config file on all nodes for editing with

wxviconf –L ‘{can DB}’ 

 

2. Add a default_net entry to [system] as follows:-

[system]
default_net=&lt;list of comma seperated network interfaces e.g. eth2,eth3&gt;

 

3. Run wxprobe -i and check that the correct interfaces are used, and that the correct number of MPK links are shown.  Note that in older version of Kognitio software you will need to run wxprobe -wF and cross-check the IP addresses with the output from ifconfig

 

4. Restart the database with 

wxserver start [sysimage]




HOWTO: deal with RPC port clashes

Solution 00000792 by Kognitio Support at 2013-03-14T14:17:23.000+0000


By default, the Kognitio software uses a number of privileges ports (in the range 0..1023) for internal communication between database processes. Normally the server starts from port 150 and works up, looking for a range of ports which are free. Once it finds such a range, it will then try to reserve those ports. 

It is possible for RPC services to try to grab a port the Kognitio software wants to use between the initial scan above, and the ensuing reservation of that port. In such as case, the software will fail to start, typically with a failure to connect to the server. Often this will be accompanied with a message "SOCKET DROPPED OUT!  RE-ESTABLISHING ...
" in the smd output file for one or more nodes.

In such cases, the options are:

1) move the port range used by the Kognitio software. 
The range used can be changed by making the following config file entries (the first one is the start port, the second one is the number of ports to check from there). This only makes sense if the new range is guaranteed not to be used by RPC services:

[network]

mpkports=700

mpkportct=300


2) change the port range used for RPC services, or disable services to stop them using ports

The range of ports used for RPC should be visible by looking in/proc/sys/sunrpc/min_resvport and /proc/sys/sunrpc/max_resvport. Typically this is something like 665 - 1023, which is why the Kognitio software defaults to using a lower port range. If the RPC range is problematic, it can be changed in /etc/sysctl.conf - your administrator should know how to do this.

Tools like rpcinfo can be used to see a snapshot of the RPC services that are holding ports at any given time, and "netstat -anlp" as root can show all port usage, so repeatedly running these can help identify which ports are being used by non-Kognitio software, and what services are using them, helping in identifying problems services which could be stopped or reconfigured to use different port ranges.




HOWTO: Diagnosing timestamp problems on appliances

Solution 00000794 by Deborah Martin at 2013-04-18T15:12:02.000+0000

NOTE: DO NOT DO THIS ON PRODUCTION SYSTEMS WITHOUT THE CUSTOMER'S PRIOR AGREEMENT, AND WITHOUT STOPPING THE DATABASE SOFTWARE FIRST



 


wxprobe reports any clock skew between nodes in the KAP system.Normally NTP should be in use to resolve any temporary clock skew. However, if the NTP server is down or not responding, there are some steps you can take to diagnose the problem.




Note: These steps are applicable to appliances. For non-appliances configs, some may not apply but the principle is the same and the commands tend to be the same as well. 

•Check the timestamp for all nodes in the appliance running an smd daemon to see which nodes are having problems 
wxtool -S 'date'



•If the time difference is fairly small e.g a less than an hour, you can run "wxsync -C" to resync the clocks across all the nodes in the KAP system. Then run "wxsync -c" to see whether that has resolved the clock skew.

•If the clock skew is significant e.g hours or even days, then it may be that the NTP server is down or not responding as it should so using wxsync -C may work but the skew may happen again fairly quickly 
To diagnose problems with NTP itself, check that all nodes in an appliance are running the ntp daemon with the following command:



wxtool -S '/etc/init.d/ntp status'




You would see a result similar to the one below for all KAP nodes if ntp is running:





Kognitio WX2 Administration Utility v7.02.01-rel130215 on support
(c)Copyright Kognitio Ltd 2003-2013.


Results:
For node wx2-support (ecode 0, 80 bytes):
Checking for network time protocol daemon (NTPD): ..running


 


If the ntp daemon has lost its connectivity with the NTP server, you will probably see: 


 


wx2-support:~ # /etc/init.d/ntp status
Checking for network time protocol daemon (NTPD):                     dead 


 


If NTP is not running at all you will probably see:






wx2-support:~ # /etc/init.d/ntp status
Checking for network time protocol daemon (NTPD):                     unused


 


Review the config file /etc/ntp.conf on one of the nodes in the appliance running an smd daemon and look at what ntp server a node will sync to for its ntp time. 


 


Go to the server found in ntp.conf. On appliances this is usually the "support" node which is a VM that shares the same host as the RDP and the AP nodes. On non-appliances this could be a node configured internally by the customer for which Kognitio will have no access or control over so you'd need to contact the customer's IT Team for further assistance first. 


 


Look at the ntp.conf file on the "support" node and check the ntp server configured is responding to ntp requests i.e ntpq -p 




If you see output like below, ntp is responding :- 





wx2-support:~ # ntpq -p
remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
*timex.wxasp.net 195.40.0.250     3 u  146 1024  377    4.813    2.998   1.435
rolex.wxasp.net .STEP.          16 u    - 1024    0    0.000    0.000 4000.00
LOCAL(0)        LOCAL(0)        10 l   23   64  377    0.000    0.000   0.001


 


At this point, if the "support" node appears to be getting ntp requests succesfully, check it's local time using the "date" command. If that is correct, try restarting the ntp daemon on the appliance nodes running an smd daemon. 


 


  wxtool -S '/etc/init.d/ntp stop' 
  wxtool -S '/etc/init.d/ntp start' 


 


Then run wxtool -S 'date' to see if that has corrected the problem.


 


If it doesn't report the correct time and you see an error like "Connection refused" or similar then it's likely that the ntp  server is not responding for some reason. This is likely to be causing problems with the nodes in the appliance not being able to  report the correct date and time as well. If the external ntp server is under a customer's control, you will need to report it to  them so they can investigate the problem. 






Once you have ntp working correctly from the "support" node, you can then restart the ntp daemons on the KAP nodes in the appliance. 


      


  wxtool -S '/etc/init.d/ntp stop'
  wxtool -S '/etc/init.d/ntp start'


 


For the RDP server, you would need to restart the NTP separately by whatever command on Windows should be used. It might be a windows service that you need to stop / start rather than from a command line tool. 


 




HOWTO: how do I remove a runtime parameter setting from the config file

Solution 00000797 by Kognitio Support at 2013-04-26T11:26:20.000+0000

To remove a runtime parameter setting from the config file (so the server reverts to using the default value for the parameter), edit the config file to precede the parameter with the - (minus) character.

For example, change:

ai_mergewf=0

to

-ai_mergewf=0



HOWTO: How to collect the replay log for a query

Solution 00000810 by Kognitio Support at 2018-06-25T15:13:36.000+0000




From 8.1.0 the compiler can generate a replay log to
allow analysis of problems, such as CI5500 errors. To collect the log, do the
following:



 



1. Set current_session parameter ci_replay_flags to 3



2. Perform the query



3. Drop current_session parameter ci_replay_flags



4. Find out the tno of the query (if there might be &gt;1)



5. Look in /tmp on all nodes for a file called /wx2-replay-tno&lt;tno&gt;-XXXXXX,
and pass on to development.  ** NOTE **: from 8.2, the files will be in `wxlogd dumps`, aka /var/log/wx2/dumps-&lt;systemid&gt;, rather than in /tmp.










You can also extract this information from a compiler
core file.  ci_replay_flags are 1 which
means "log replay in memory", and 2 which means "write replay
log to /tmp".  The default is 1
which means it will be embedded in a core file and can be obtained as follows;
however, if we find that this makes it run out of memory (I'm not aware of this
happening) then it can be set to 0 so that it doesn't log.



  




1.    Set
current_session parameter ci_replay_flags to 1 (the default) or 3. 



2.    Perform the
query.



3.    Dump and
fetch the compiler core file.



4.    Use the
debugger to get the address of GE.ReplayLogStart.



5.    Run this to
output the replay log from the corefile memory:



 

        
wxdhdiagnose -r  -c
&lt;corefile&gt; -b &lt;address&gt; &gt; replayfile.out



 





You then run that (or the ouput from the method Paul
described) with the standalone compiler from the same software version,
optionally forcing debugging flags, using:



 



   /nextgen/.../out/GPLinux/bin/compile
[-d] -r &lt;replayfile&gt;



 





The behaviour of the standalone with the replay should be
exactly the same because the replay log is a record of the replies from all the
calls to other parts of the system (STI, AI, PV, ...).  Differences or problems are likely to be:



 



1.  If there is a
"#ifdef CI_ALONE".  I'd regard
that as a bug, and I think we have eliminated all of these.



2.  If you are
running with a different software version. 
That can't happen by accident because it checks and will give an error,
but you can remove the check from CmpCompile() if you need to.



3.  If there is a
bug in the replay implementation.  When
writing the replay log it includes a tag that can be checked when replaying; if
it finds a mismatch then it will error.



4.  If something
"external" happens.  There
might be a nomem that happens when logging but not replaying, or
vice-versa.  The compiler is
single-threaded so that shouldn't be an issue unless another wxdb thread does a
memory overwrite.



HOWTO: reintegrate a repaired node into a system

Solution 00000816 by Kognitio Support at 2015-06-29T09:01:01.000+0000

This solution has a document attached explaining the usual steps required to reintroduce a repaired node into a system, along with the approximate time required for each step.

It is primarily intended to help customers understand the length of time required for reintegrating a node, and allowing them to pass on that information up their management chain.


Attachment: Reintegrating a repaired node.docx,

HOWTO: Perform a node failure test on KAP

Solution 00000821 by Simon Darkin at 2015-11-16T13:37:57.000+0000

To perform a node failure test on a KAP system that has been commissioned with Software RAID you will need to enable 'reliability features' to switch on the automatic recovery feature.  This ensures the system will try to restart if a node fails. It will attempt to bring the system up insisting all hardware is present 3 times (allowing a node that e.g. reset to recover and rejoin), then allow restart using software RAID to recover from missing disk resources.

CONFIG FILE SETTINGS

To enable auto-recovery you set the following in the [wxsmd] section of the configuration file:

reliability_features=yes

To ensure the restart is triggered as quickly as possible, you can prevent the initial restart attempts insisting on all hardware being present by adding the following line to the [wxsmd] section of the configuration file (I will not be doing this in my test): 

num_boots_fullsys=0

By default, the system will put all images back into memory, but you can switch to leave memory empty (faster restart, less usable machine until images are manually reloaded) by setting the following in the [wxsmd] section of the configuration file (I will not be doing this in my test):

auto_start_wxdb_command=start sysimage

MAKE A NODE FAIL IN AN UNCONTROLLED WAY

From within the HP Onboard Administrator reset one of the nodes and prevent it from coming back up by powering off after the reset.  Note that a reset must be used initially so that the node fails in an uncontrolled way. Using any other method will result in the smd shutting down cleanly preventing the automatic recovery from being triggered.  

To reset then power off a node:

Logon to the HP BladeSystem Onboard Administrator -&gt; left-click the chosen node -&gt; Virtual Devices -&gt; Reset -&gt; wait 30 secs -&gt; 'Press and Hold' (to power off)

For this test I reset node 4-1-3 in wxsupp01 which is a 4 node system running 8.1.0rel150916.  Looking in the smd directory’s monitor file on the master node (found with 

“wxserver info master”), I see the software detecting the node failure, trying a number of restarts when it insists all hardware is present, then restarting whilst tolerating missing hardware.  The output below has been annotated to make this clear:

wxadmin wxadmin@wxsupp01-wx2-rack4-enc1-4:/var/log/wx2/logs-wxsupp01/smd.T_2015-11-11_10:00:06_GMT&gt; tail -f monitor.T_2015-11-11_10\:00\:24_GMT 
T_2015-11-11_10:00:24_GMT: Monitor started.
T_2015-11-11_10:00:52_GMT: Monitor stopped.
T_2015-11-11_10:00:52_GMT: Monitor started.
T_2015-11-11_15:47:09_GMT: Dead link noted for node wxsupp01-wx2-rack4-enc1-3!  Starting check... &lt;&lt;&lt; PROBLEM DETECTED
T_2015-11-11_15:47:42_GMT: NODE wxsupp01-wx2-rack4-enc1-3 MISSING!
T_2015-11-11_15:47:43_GMT: Restarting database: db node failure
T_2015-11-11_15:48:15_GMT: Monitor stopped.
T_2015-11-11_15:48:15_GMT: Monitor started.                                                                     &lt;&lt;&lt; RESTART 1 (insisting on no missing hardware)
T_2015-11-11_15:48:20_GMT: Monitor stopped.
T_2015-11-11_15:48:21_GMT: Monitor started.                                                                     &lt;&lt;&lt; RESTART 2 (insisting on no missing hardware)
T_2015-11-11_15:48:26_GMT: Monitor stopped.
T_2015-11-11_15:48:26_GMT: Monitor started.                                                                     &lt;&lt;&lt; RESTART 3 (insisting on no missing hardware)
T_2015-11-11_15:48:31_GMT: Monitor stopped.
T_2015-11-11_15:48:32_GMT: Monitor started.                                                                     &lt;&lt;&lt; RESTART 4 (tolerating missing hardware)

If I now check in `wxlogd startup` I can see what happened from the output file:

wxadmin wxadmin@wxsupp01-wx2-rack4-enc1-4:/var/log/wx2/logs-wxsupp01/startup.T_2015-11-11_15:48:26_GMT&gt; cat output 
   --&gt;  Cleaning up unwanted files/processes.
Cleaning old processes.
   --&gt;  Clean up failed: falling back to pname method.
Erasing server working area.
   --&gt;  Examining system components.
Using target {skey BOOTING-5534135}
Detected:
WX2: Blocks 2, Nodes 4, Disks 3, Status Up.                             &lt;&lt;&lt; ONLY HAVE 4 NODES (ORIGINALLY HAD 5 INCLUDING THE AP)
Boot disks:
Boota: Disk WXD:564311D4:00000001: on_nodes 1, Status Up.
         sid wxsupp01, seq 0x56431222, Location wxsupp01-wx2-rack4-enc1-1.
         SIZE 3814Mb, nsecs 7812500, ssize 512.
           Attributes Part-Zero-DirIO-Align
Bootb: Disk WXD:564311D4:00000002: on_nodes 1, Status Up.
         sid wxsupp01, seq 0x56431222, Location wxsupp01-wx2-rack4-enc1-2.
         SIZE 3814Mb, nsecs 7812500, ssize 512.
           Attributes Part-Zero-DirIO-Align
WARNING:  System is running without a hardware map file.
Bootinfo ug flags:  current 80100, tgt 0, min 70000, phase 0.
   --&gt;  Configuring WX2 software.
Generation results:
ERROR: PMA rscore contains missing or badly ordered slices (16 before 8)   &lt;&lt;&lt; PMA COMPLAINT AS NODE MISSING, SO CANNOT REUSE OLD 
WARNING: Running with incomplete clusters.
WARNING: Memory image set not valid.  Rebuilding images instead.
WARNING: Disk cluster 1 pos 0 missing -- adding virtual disk.                              &lt;&lt;&lt; ADDING ‘VIRTUAL DISK’ TO COPE WITH MISSING DISK RESOURCE
Preparing to transfer config files.
Transferring config files.
Transfered OK.
Restarting dump server on DB nodes.
Performing boot environment checks using wxbootcheck
Setting min_free_kbytes on all nodes to &gt;= 30000
Executed OK.
Expanding /dev/shm on DB nodes
Executed OK.
Disabling ASR on nodes
Executed OK.
Switching off hugetlb.
Executed OK.
Removing old PMAs from DB nodes.
Executing commands:
Executing /opt/kognitio/wx2/ver80100rel150916/software/Linux/wxdb  -H (NOWAIT).
Executed OK.
   --&gt;  Initialising Database.
Running init command: crimage, cxt TRY_RECOVER=0, TRY_REBUILD=2048.
   --&gt;  Loading system tables, user tables and view images
   --&gt;  Starting automatic disk recreate from standbys
Completed crimage in 00:00:34.
Startup complete.  SERVER IS NOW OPERATIONAL.                                                                                                                      &lt;&lt;&lt; ALL IS WELL

You can run $f0 from wxsubmit to see how many nodes the system booted with:

Connected to localhost ODBC Version 8.01.00-rel150916 Server Version 08.01.0000
&gt;$f0
System ID   |Ver  |Patch    |Nodes|Cores|Disk GB  |RAM GB   |RAID|Comp|Int |UpTi
wxsupp01    |80100|rel150916|    3|   24|      6.7|     86.4|   2|  10|  20|  8m              &lt;&lt;&lt; SYSTEM BOOTED WITH 3 NODES
Query           1                1 row     ----   0:00.7   0:00.7   0:00.7

REINTRODUCING THE NODE

To put the node back in, I powered it back up using 'Momentary Press' from within the HP Onboard Administrator, watched it restart via a Remote Console, then ran “wxprobe –H” from a node when it was up to confirm that the system now had 4 DB nodes again:

wxadmin wxadmin@wxsupp01-wx2-rack4-enc1-4:/var/log/wx2/logs-wxsupp01/startup.T_2015-11-11_15:48:26_GMT&gt; wxprobe -H
Kognitio WX2 Hardware Discovery Tool v8.01.00-rel150916 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2015.

WX2 system has: 5 nodes in 2 groups.
Disk resources: 16.0G in 4 disks.
System has 3 unique types of node.
System has 1 unique type of disk.
System RAM 131G, 129G for data processing.
32 CPUs available for data processing.

Detected node classes:
   cp: 1 node
   full: 4 nodes           &lt;&lt;&lt; 4 DB nodes present

Detected Operating platforms:
   Linux-2.6.16.60-0.42.5-smp: 1 node
   Linux-2.6.16.60-0.89.1-smp: 4 nodes

Now doing “wxserver start” will restart the system to reincorporate the node:

wxadmin wxadmin@wxsupp01-wx2-rack4-enc1-4:/var/log/wx2/logs-wxsupp01/startup.T_2015-11-11_15:48:26_GMT&gt; wxserver start
Kognitio WX2 System Controller v8.01.00-rel150916 on wxsupp01
(c)Copyright Kognitio Ltd 2001-2015.

Logging startup to /var/log/wx2/logs-wxsupp01/startup.T_2015-11-11_16:09:42_GMT.
   --&gt;  Cleaning up unwanted files/processes.
   --&gt;  Clean up failed: falling back to pname method.
   --&gt;  Examining system components.
   --&gt;  Configuring WX2 software.
   --&gt;  Initialising Database.
   --&gt;  Recovering memory images
   --&gt;  Starting automatic disk recreate from standbys
Completed crimage in 00:00:21.
Startup complete.  SERVER IS NOW OPERATIONAL.

Now if I connect to the system with wxsubmit I can see it is recreating the disk resource on the reintroduced node:

Connected to localhost ODBC Version 8.01.00-rel150916 Server Version 08.01.0000


&gt;$f3
MPID   |CLID|Disk Status       |MPID GB  |Used GB  |Free GB  |% Used
     42|   0|good              |      1.7|      0.0|      1.7|  0.0%
     52|   0|good              |      1.7|      0.0|      1.7|  0.0%
     62|   1|good              |      1.7|      0.0|      1.7|  0.0%
     32|   1|recreating data   |      1.7|      0.0|      1.7|  0.0%
Query           1               4 rows     ----   0:00.3   0:00.3   0:00.3

WHAT IF ANYTHING UNEXPECTEDLY GOES WRONG

If anything goes wrong, capture the following information and email wx2-helpdesk@kognitio.com, or create a case and attach them:


   a) the config file
     b) the contents of /var/log/wx2
      c) the smd monitor file

Then try to restart the system with “wxserver start”. If this does not give any output, use “wxviconf” to comment out the reliability features line so it looks like this:

#reliability_features=yes

…then do “wxserver smd all restart force”, wait a minute, then do “wxserver start”.



HOWTO: Reset an enclosure switch

Solution 00000825 by Simon Darkin at 2016-05-05T13:33:53.000+0000



CAUTION: Only consider resetting an enclosure switch if the
IT services team are unavailable and you understand the switch geography and
general networking environment on KAP.  





If you discover that all similarly named interfaces
across an enclosure have become unresponsive then you will need to check and
possibly reset the enclosure switch connected to that problem
interface.   Typically you'll be made aware to this type of issue via
a Nagios alert which will identify the problematic interface by name. 
Note that you should only consider resetting the switch if it is clear that *all*
similarly named interfaces within an enclosure are unreachable.  If only
one or two show the problem then the chances are it is the interface on the
node rather than the switch that needs attention.




Before you start it is worth familiarising yourself with
switch numbering and addressing conventions used by Kognitio.



Typically the switch numbers are assigned as follows:



1 - deployment network (eth0)



2 - external interface (eth1)



3 - usually empty



4 - usually empty



5 - MPK (eth2) 



6 - MPK (eth3)



7 - MPK (eth4) 



8 - MPK (eth5)






Systems with two MPK links (which is currently the norm
where 10G links are used) won't include switches 7 and 8.






Typically IP addresses are allocated to switches as follows:



        Rack1-Enc1 
Switch1 (14.1.11.1)



14.1.    
11.              
1



        Rack2-Enc1 
Switch1 (14.1.21.1)



14.1.    
21.              
1



        Rack2-Enc1 
Switch5 (14.1.21.5)



14.1.     21.  



and so on...



Note that on some systems the last octet in the IP address
is not the switch number but rather the switch number + 100 





Steps for resetting an enclosure switch:



1. Connect to any node in the system and use ping to
double-check that the suspect interface is unreachable on all of the nodes in the
enclosure 



If for example you wanted to ping all the eth2 interfaces in
rack 4, enclosure 1 you might run:



for
i in `wxprobe -i | grep rack4-enc1 | grep eth2 | gawk '{print $3}'`; do ping -c
3 $i; done





2. Logon to the OA, connect to the appropriate enclosure and
identify which switch connects to the group of unreachable interfaces.  To
identify the correct switch - from the OA navigate to ‘Systems and Devices’
-&gt; 'Interconnect bays' -&gt; Select the appropriate numbered switch (bearing
in mind the numbering convention mentioned previously)  -&gt; Port Mapping




Cross reference the Port 1 MAC addresses listed in the
‘Device ID’ column on the OA with the MAC addresses shown when you run ifconfig
on the database nodes specifically for the problem interface.  If for
example eth2 was the unreachable interface you would list the MAC addresses
associated with that interface using wxtool -S "/sbin/ifconfig | grep eth2 | grep HW",
and then you would cross-check those with the Port 1 Device ID (MAC addresses)
on the selected switch on the OA to confirm you have made the correct
choice.  If they do not match then move to the next switch and continue
until you locate the correct one.



3. Although not strictly necessary you can try pinging the
switch itself from a command prompt on the RDP server.  The outcome won’t
necessarily affect your decision to reset the switch but you can then report
back to IT services whether or not it was completely unresponsive.   
You might be able to identify the IP address by selecting the switch in the OA
and then selecting the Information tab although this won’t provide the address for
older switches in which case you would need to deduce the IP address by
making reference to the convention mentioned earlier in the solution.



4. IMPORTANT - Be mindful that an enclosure could contain
more than one KAP instance and so make sure you are aware which systems will be
affected by resetting the switch.    Once you are
absolutely certain you have identified the correct switch and that you won’t be
interrupting another system that appears to be working fine then select the
switch from 'Systems and Devices' then select Virtual Buttons -&gt; Reset.



5. Wait a few minutes and then re-run the ping checks from
step 1 to ensure the interfaces are reachable again.  If they are not
reachable then raise an IT services case with all of the relevant details and
consider configuring out the problem interface in the KAP config files before
restarting the database.





HOWTO: How do I stop automatic restart of the KAP database

Solution 00000826 by Deborah Martin at 2016-05-17T07:00:10.000+0000

If you have reliability features switched on in your config file and you want, for example, to patch the OS on each database node, you will need to disable
reliability features first. Otherwise this feature may detect one or more database nodes are missing and that may trigger a restart of the database using software raid. Or it may error if too many nodes in a cluster are missing. 


Follow the steps below to disable the feature :- 


a) Using wxviconf and comment out the line in your config file and save the change. 
   The SMD's will be restarted automatically whenever you make    and save a change in the config file. 


	from : 
	reliability_features=yes
	to   :
	#reliability_features=yes


b) Stop the database 
	
	wxserver stop 


c) Perform the maintenance on your database nodes e.g OS patching


d) Once all the work is done, check all the database nodes are back online


	wxprobe -H 


e) Restart the database using your usual method e.g startup script or wxserver start [sysimage] 
   Note, if you have rebooted one or more of the database nodes you will not be able to use the "fast" restart method. 
   See the Kognitio documentation for the options available on database restarts. 
   . 

f) Repeat step a) using wxviconf and remove the # from "#reliability_features=yes". The SMD's will be restarted automatically when you save the
   change. There is no need to restart the database once again. 




HOWTO: commission a system on Amazon Web Services / AWS

Solution 00000829 by Michael Dams at 2018-09-13T14:56:01.000+0000

This document describes how to setup Kognitio on raw AWS instances


setup the nodes


1. Decide on the instance type and AMI to use for the database nodes. In most cases the AMI should be a SLES 11 HVM type. The choice of instance type will depend on how much RAM / CPU / performance etc is required, and on cost. Bear in mind some instance types offer 10Gbps networking. 
2. When Launching the instances, it's important to launch them in the same VPC, the same subnet and the same placement group.  This will make sure the nodes are physically as close together as possible in the AWS datacenter. Without all 3 you may not get the full benefit of 10G networking. Placement groups are only available on some instance types though. Also, make sure to specify the same private key for connecting to the nodes. If using EBS volumes for storage (recommended), it will be easier to attach them when you launch the instances rather than later. 
3. If there will be an AP, note that it doesn't need as much CPU/RAM etc as the db nodes, so it can be a cheaper instance type. See comparable existing systems to get an idea. However the same rules apply re. subnet, VPC and placement group.  Sufficient storage will need to be provided (usually by an EBS volume) for the /vol1 directory. 
4. After the instances are launched and storage assigned, make sure all instances are in the same security group, and then set the security group to allow all traffic between the nodes in the private subnet (usually 172.30.x.y)
5. clicking on each node will reveal a public IP which can be used to connect. ssh into a node with 'ssh -i &lt;private key file&gt; ec2-user@&lt;public ip&gt;. Then 'sudo su -' to become root


prepare for stage 1 install


1. Follow the steps in the solution 'FAQ: Common problems with non-appliance installs' for each node.
2. Amazon uses 64 bit images, so will need to install some 32 bit libraries on every node. For example on SLES 11 run:
zypper install glibc-32bit
zypper install libgcc-32bit
zypper install zlib-32bit
zypper install libopenssl0_9_8-32bit
zypper install perl
3. make sure you can ping one db node from another, and telnet on an arbitrary port
4. create type 60 partitions on the db nodes for KAP storage. Use fdisk -l and df to see available devices. If using EBS volumes, these may show up as /dev/xvdb or similar. To create the partition, run fdisk &lt;device path&gt;, then enter n for command, p for primary partition, 1 for partition number, enter for default first sector, enter for default last sector, t for command, 60 for hex code, w for command. Then run fdisk -l to check it's correct.


stage 1 install


1. see http://www.kognitio.com/forums/Kognitio%20Software%20Installation.pdf for a guide
2. download the .sfx kognitio installation file, untar and unzip. It's useful to have the license and system_id prepared at this stage.
3. go through the stage 1 install on each node, creating wxadmin/wxroot when prompted.
4. At this point, each node will be isolated as autodiscovery doesn't work in AWS. A setting will need to be added to the global conf file on each node:
[net eth0]
peers=10.0.5.8, 10.0.5.142               # comma separated list of each node in the system (including AP). Can be either IPs or hostnames
5. after setting the above in each node's config file and restarting SMD's, the nodes should see eachother. check this with wxprobe -H. Some other useful conf settings:
[mpk]
dgram_mtu=8970                              # was needed due to amazon VPC bugs with large MTU values
[boot options]
idle_core_cost=0                            # this stops the system making lots of ram stores to use up cores, resulting in little RAM per RS in systems with lots of cores and little RAM


stage 2 install (newsys)


1. decide on whether software RAID should be used. Note that EBS volumes have reliability / fault protection built in already to protect against single component failure. If not using RAID, consider regular snapshotting /backups.  
2. Set cluster_size, desired_slabs etc in wxviconf to prepare for stage 2 install. Also Install the license if not already done. 
3. run the newsys with sparse_file=yes the first time to make sure there are no problems. Then newsys without sparse_file


after newsys


1. on the AP, create the /vol1 directory. Find the device to be mounted there, and create a partition on it (default type, not type 60). Then create an ext3 filesystem on the partition, and mount it on /vol1.  Settings and options can be copied from comparable systems.
2. create the aimia home directory (/vol1/aimia), and set correct permissions (all this can be copied from a comparable system). create the aimia user with correct uid, set a password for it, put it in the wxadmin group, copy ~aimia/.profile from comparable system and specify the password should change on next login. Check the aimia user can run wxprobe etc. If not, you may need to restart smds
3. The security group should be restricted to particular IPs, as by default it will e.g. allow sending data to anywhere in the world.




</t>
<t tx="jonathanhudson.20201007141933.1">@language md

The status of queues can be examined using the 'sys.ipe_query_queues' and 'sys.ipe_query_queue_stats' virtual tables.

#### ipe_query_queues 

has an entry for each current query associated with a queue, indicating items such as:

* its position in the queue

* whether it is running (as opposed to queued)

* how long it was queued for

* how long it has been running to date.

#### ipe_query_queue_stats 

has an entry per queue, indicating items such as:

* how many queries are on that queue

* how many are running and how many are queued

* how many queries have been through the queue since server start time

* how many queries are runnable at one time through the queue

* the load over the last 5 minutes, 1 hour, and 5 hours, expressed in terms of the average length of the queue including running queries over that period (averages calculated in the same way as UNIX-like load averages)

* the average queue time in seconds for the last 10, 50 and 100 queries

* the average run time in seconds for the last 10, 50 and 100 queries

In addition

```
select * from sys.ipe_cursessions where command_running=4
```

will also display queued queries.</t>
<t tx="jonathanhudson.20201007142124.1">@language md

#### Using variables

If in the command shell:

```
export var1=ldate 

export var2=ltime 
```

then the following command in wxsubmit:

```
select $var1, $var2 from ipe_login 
```

would be translated to:

```
select ldate, ltime from ipe_login
```

The lack of any 'set var' for $var1 and $var2 means that wxsubmit would retrieve the values of $var1 and $var2 from the shell's environment variable list instead.

This can also be done in a single line as follows:

```
var1=ltime var2=ldate wxsubmit -s myserver myuser -p mypass myscript.sql 
```

where myscript.sql contains references to $var1 and $var2 as appropriate.

#### Insert String Variable into Table

wxsubmit will not evaluate a variable if it is in single quotes hence:

```
insert into mylog values('$mystring')
```

will actually insert '$mystring' into the table and not the current value of the $mystring variable.

To work around this, do the following:

```
set var sq ';

insert into mylog values(${sq}${mystring}${sq});
```

#### Check return status

```
select * from mytab1;

if wcserror = ok goto nextquery;

if wcserror &lt;&gt; ok quit 5;

nextquery: 

select * from mytab2;

if wcserror = ok quit 0;
if wcserror &lt;&gt; ok quit 5;
```

</t>
<t tx="jonathanhudson.20201007142238.1">@language md

The following query will work with version 7, 7.1, 7.2, and 8.1:

```
select cs.session, sum(ram_used) from sys.ipe_ram_images ri,
    sys.ipe_cursessions cs, sys.ipe_transaction t
where cs.session = t.session_id
    and t.tno = crtrans
    and t.operation = 2
    and table_id not in (select id from sys.ipe_table)
    and (table_id &lt; 10000000 or table_id &gt;= 20000000)
group by 1
order by 2 desc
at full_history
```

System-created temporary tables have IDs above 20,000,000 whereas view images are between 10,000,000 and 20,000,000 (up to and including version 8.1 of Kognitio software).</t>
<t tx="jonathanhudson.20201007142818.1">@language md

How do I stop or restart all SMDs simultaneously across all nodes?

```
wxserver smd all restart | exit
```

#### Note

It is not possible to use 

```
wxserver smd
```

to start all SMDs simultaneously, as 'wxserver' requires the SMDs to already be present in order to function. 

In these circumstances, it will be necessary to use 

```
wxsvc
```

on each node individually

#### Stop and start the SMD daemons on individual nodes


```
wxsvc stop | start | restart
```</t>
<t tx="jonathanhudson.20201007143028.1">@language md

```
select
  cast(to_char(st.usedgb+st.freegb,'999,990.9') as varchar(9)) as "Disk GB",
  cast(to_char(st.usedgb,'999,990.9') as varchar(9)) as "Used GB",
  cast(to_char(st.freegb,'999,990.9') as varchar(9)) as "Free GB",
  cast(to_char(st.availgb,'999,990.9') as varchar(9)) as "Avail GB",
  cast(to_char((st.usedgb*100)/(st.usedgb+st.freegb),'990.9') || '%' as varchar(6)) as "% Used",
  cast(to_char(((st.freegb-st.availgb)*100)/st.freegb,'990.9') || '%' as varchar(6)) as "% Unav"
from
(select
    sum(xe.data_stored*xe.cu_size)/power(1024,3) as usedgb,
    sum(xe.free_space*xe.cu_size)/power(1024,3) as freegb,
   (min(xe.free_space*xe.cu_size)*count(xe.mpid))/power(1024,3) as availgb
 from sys.ipe_xor_element xe) st(usedgb,freegb,availgb);
```
 </t>
<t tx="jonathanhudson.20201007153823.1">@language md

Run the following query from the SYS user

```
select u.name username, s.name schemaname, 
sum(f.nrows * (24 + (4 * ((mins + 3) / 4)) + (4 * ((c + 31) / 32)))) / (1024.00 * 1024 * 1024) AS minsizeGB,
sum(f.nrows * (24 + (4 * ((maxs + 3) / 4)) + (4 * ((c + 31) / 32)))) / (1024.00 * 1024 * 1024) AS maxsizeGB
from ipe_alluser u, ipe_allschema s, ipe_alltable t, ipe_ftable f,
(SELECT 
  table_id, 
  SUM(CASE WHEN datatype = 209 AND numtype = 304 
           THEN scale + 8 
           ELSE length END),
  SUM(CASE WHEN datatype = 209 AND numtype = 304 
           THEN 8 
           ELSE length END), 
  COUNT(*) 
  FROM ipe_allcolumn  GROUP BY 1) AS x(t, maxs, mins, c)
where t.owner = u.id and t.schema_id = s.id and f.table_id = t.id and t.id = x.t
group by 1,2
order by 3 desc
```
This should give you a range because WX2 does not have a quick and easy way to deal with varchar data - so the minsize shows the value if all the varchar fields have 0 bytes of data in them, and the maxsize shows what the disk usage would be if all rows had all their varchars filled to capacity.

The query takes account of deleted records, but not truncated or dropped tables.

Alternatively, the following query should give a reasonable indication of how much space is used by assuming that each 8KB disk page contains data from only one table - effectively true for non-trivial tables with the exception of system logging tables. It also does account for dropped tables:

```
select trim(s.name) || '.' || trim(t.name) as name, f.table_id, sum(block_count * 8) / (1024.00 * 1024) as GBused
from ipe_ftable f left outer join  (ipe_alltable t join ipe_allschema s on t.schema_id = s.id) on f.table_id = t.id
group by 1,2
order by 3 desc
```</t>
<t tx="jonathanhudson.20201007160629.1">@language md

TARGET FILTERS

By default wxprobe and friends will target all the nodes that are running an SMD for a given system, however commands such as wxprobe, wxtool and wxsync can be supplied with a target string allowing a subset of nodes to be targeted.

The  can either be the name of a node (usually 'hostname', but doesn't have to be), or it can be a list of target tokens enclosed in {}'s.  Target tokens can either be one or two words long depending on what the token is, these include:

can 		each node has a set of capabilities which define what it can do.  wxprobe -w lists these.  
   	     double word token targets nodes with a given attribute.  Each node has attributes defined for it's configuration file
  		single word token which targets the node with the supplied IP address
  		single word token which matches nodes with the supplied name
ukey   	targets all nodes which have the user defined key  set.

Using node capabilities as an example the 'can' token will target only those nodes which have the capability .  The capabilities of a node can be viewed in the output from wxprobe -w , which also serves as a useful reminder of how  must be specified in the target string. This formatting consideration also applies to single and double word tokens whereby the format of the node name, IP address or attribute value within the search string must match the format of that items as viewed in wxprobe output. wxprobe -wF lists all node attributes so can be used as a useful reference.

Each target string can be preceded with a '!' which negates it. For example 'can DB' would target DB nodes only, whereas '!can DB' would target non DB nodes only, useful if you want to target say APs.

Tokens can be put together. A space separator will perform a logical AND, a comma seperator will perform a logical OR.

Examples

```
wxprobe -w -a '{hardware HPBlade}'		target HPBlade hardware only
wxprobe -w -a '{can DB}' 			target DB nodes
wxprobe -w -a '{rack rack3}' 			target all nodes in rack 3
wxprobe -w -a '{enclosure enc6}' 		target all nodes in enclosure 6
wxprobe -w -a '{bay 1}'				target all nodes in bay 1 
wxprobe -w -a '{rack rack2 bay 1}'		target nodes in rack 2 bay 1 (if two nodes have both rack=2 and bay=1 they'll both be returned).
wxprobe -w -a '{rack rack2, bay 1}'		target all nodes in rack 2 and all nodes in bay 1 of other racks (i.e. return nodes with rack=2 OR bay=1)
wxprobe -w -a '{bl-f0v2r3e2b1, bl-f0v2r3e3b1}'	target the two nodes listed
wxprobe -w -a '{10.2.2.16}'			target the node listed
wxprobe -w -a '{!enclosure enc6}'		target all nodes except enclosure 6
wxprobe -a '{mpid 100}' -l  			target the node that is running mpid 100
wxtool -R -a '{updated}'			     target the nodes which need an SMD restart because config file changed. Useful to see if forgot to restart an SMD
wxtool -w -a '{aa:bb:cc:dd:ee:ff}'     target the node listed		
```

### USER DEFINED TARGET FILTERS

User defined keys can be set with the wxtool -F  command, which sets the key on all nodes targeted.  So to assign target strings to a key:

```
wxtool -a '{bl-f0v2r3e2b1}' -F key1		Add the listed node to key1
wxtool -a '{bl-f0v2r3e2b2}' -F key1		Add the listed node to key1
wxprobe -w -a '{ukey key1}'			target nodes assigned to key1

wxtool -C clears previously defined keys, so to clear a key:
```

	wxtool -a '{ukey key1}' -C key1			clear key1
    </t>
<t tx="jonathanhudson.20201007160933.1">@language md
</t>
<t tx="jonathanhudson.20201007160944.1">@language md

#### How do I permanently disable Linux/Solaris swap on WX2?

Kognitio recommend that swap is disabled on DB nodes running WX2.  Swap can be permanently disabled using the following steps:

1) Confirm that swap is in use by running 'free'.  If the total swap is non-zero then swap is enabled.  The default output is in KB so in the example below we can see swap is set to 2Gbytes.

wxadmin wxadmin@system1:/etc&gt; free
             total       used       free     shared    buffers     cached
Mem:      66012628     470260   65542368          0      46516     315340
-/+ buffers/cache:     108404   65904224
Swap:      2104504          0    2104504


2) As root, disable swap with the command:

	swapoff -a


3) edit /etc/fstab and comment out (with a #) or remove the 'swap' line or lines.  

Please note the order is important -- if you do it in the other order (edit /etc/fstab first) then swapoff -a, then on certain systems the swap partition may not be disabled as there is no longer a reference to it in /etc/fstab.


4) Finally, check that swap is set to zero by running free.

wxadmin wxadmin@system1:~&gt; free
             total       used       free     shared    buffers     cached
Mem:      66012628     170572   65842056          0       5756      74428
-/+ buffers/cache:      90388   65922240
Swap:            0          0          0</t>
<t tx="jonathanhudson.20201007161028.1">@language md

#### Test read/write disk performance

Below are some example tests that can be used to check the read/write performance of a disk on a system running WX2 on Linux.

Some example timings have been included which were taken from the following drive specification:

```
Model:		HP DG146BB976
Capacity: 		136Gb 
Drive Type:		SAS 
Rotational Speed:	10k rpm
```

The following tests assume to have access to a linux account and that you have a filesystem,

--Write out a 2Gbyte file using fsync to flush the writeback cache

--48-50Mb/s

```
for i in `seq 1 3`; do echo "Loop $i"; dd if=/dev/zero of=2gb_file.out bs=400000 count=5000 conv=fsync; echo ""; done
```

--Read a 2Gbyte file using sync to flush the read buffer

```
for i in `seq 1 3`; do echo "Loop $i"; sync ; echo 3 &gt;/proc/sys/vm/drop_caches; dd if=&lt;path&gt;/2gb_file.out of=/dev/null bs=400000 count=5000; echo ""; done
```

--Read 2Gb of data from the WX2 partition adjusting the offset on each iteration of the loop and using sync to flush buffers

--77-90Mb/s

```
for i in `seq 1 3`; do let skip=$i*5000; echo "Loop $i - skip = $skip"; sync ; echo 3 &gt;/proc/sys/vm/drop_caches; dd if=/dev/cciss/c0d0p2 of=/dev/null bs=400000 count=5000 skip=$skip ;echo ""; done
```

--Meaure how fast the drive can sustain sequential data reads under Linux, without any filesystem overhead.  Ensure the buffer cache is flushed during the processing buy including the -t switch

--82-86Mb/s

```
for i in `seq 1 3`; do echo "Loop $i"; hdparm --direct -t /dev/cciss/c0d0p2; echo ""; done
```

Note that whilst a single hdparm test may not necessarily reveal any performance issues, concurrent hdparm tests against a single logical disk could reveal poor seek performance or in the case of a hardware RAID array, a failed disk.   Case 15847 provides an example of this and explains two distinct ways in which a RAID controller deals with accesses. For example, does it try to split each large access into two and use both disks (which we've seen with some HP kit in the past), or does it do a large IO from one disk to allow any subsequent IO to a different part of the logical disk to use the other disk rather than have both drives having to do large seeks.  Cleary the effect on performance in the vent of a failed disk fail will be different depending on which approach the RAID controller uses with regard to accesses and whether or not the performance of single or concurrent accesses are being measured.


--In addition if you suspect that a disk is underperforming in WX2 you can scan a disk based table then monitor wxtop and look for diskstores that finish a significant time after others have completed.

```
select *
from &lt;large disk based table&gt;
where &lt;condition unlikely to be true&gt;;
```

If you do discover one or more diskstores that lag behind the majority then check the MPID(s) in question to see if they contain significantly more rows than the other MPIDs for the table being scanned as tha twill naturally affect scan times.


```
select mpid, sum(nrows) nrows
from ipe_ftable
where table_id = &lt;table_id being scanned&gt;
group by 1
order by 2 desc;
```</t>
<t tx="jonathanhudson.20201007161915.1">@language md

#### Reconfigure 

is an operation to add extra disk resources to KAP and spread the existing data over all the disk resources. In terms of preparation ensure that the nodes being introduced to the system have been configured in the exact same way as any pre-existing nodes including an identical global config file, software version and size of disk resource.

When running a reconfigure, a number of "create image" commands will be run (at least two), so it is best practice to clear objects out of RAM before running the reconfigure. Typically this would be done by running the "create system image" SQL command having obtained a global lock. After the reconfigure has completed, required images should be loaded back into RAM.

Reconfigure will not run if any disks in the system are not good.

Reconfigures should always be run from the wxadmin command line tool rather than using SQL, and the quickadd option should always be used rather than the deprecated fulladd option. See section 7.9 of the Configuration and Maintenance manual for an explanation of how to deskew disk data after the quickadd option

```
Performance figures

1) lmgmigros01 06/01/12
number of disk resources before deskew: 36
number of disk resources added: 12
disk resources per node: 1 of 133GB
software version: 7.1.2z111111
RAID cluster size: 4
max disk usage on old disks beforehand: 68.6%
max disk usage on new disks afterwards: 48.8%
start time for deskew: 06/01/12 18:38:39
end time for deskew: 07/01/12 05:20:32
time takes: 10:41:53
processing rate for old disks scanning data: (133 * 0.686 / 10.68) 8.5 GB / hour
processing rate for new disks adding data:(133 * 0.488 / 10.68) 6.1 GB / hour
```
</t>
<t tx="jonathanhudson.20201007162111.1">@language md

#### How do I configure the management link to run on a different network

This solution documents how a recent networking issue was investigated on lmg03, and a subsequent workaround implemented by way of switching the management link from one NIC/network to another.  Although this example is fairly specific to the LMG case the general principle could be applied on other systems where multiple network devices are installed. 

The symptoms included:

* failed backup (due to failed SMD commands)
* errors from wxprobe commands i.e. Resends then DEAD LINK message from ap1 to all other nodes even though all nodes were up and had been for weeks
* Restarting the SMDs on all nodes resulted in each AP only able to see itself, and the DB nodes able to see themselves but not the APs

These suggested a network issue relating to the link over which the SMD communicates i.e. the "MAN link", specifically on the APs. 

STAGE 1 - IDENTIFY WHICH INTERFACE AND NIC ARE AT FAULT

1. Check to see which network the management link is running on (10.4 in this case). 

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; wxprobe -wF | grep "MAN link" | more
Kognitio WX2 Hardware Discovery Tool v6.01.08-z20101109 on lmg03
(c)Copyright Kognitio Ltd 2001-2009.

MAN link  0: IP 10.4.11.1, MAC 00.26.55.7D.B5.B8, MPK Y, Status Up.
MAN link  0: IP 10.4.11.10, MAC 00.26.55.7D.C6.78, MPK Y, Status Up.
..

2. Run ifconfig to see which network interface is configured on that address. 

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; /sbin/ifconfig
..
eth0      Link encap:Ethernet  HWaddr 00:25:B3:21:A8:06  
          inet addr:10.3.29.1  Bcast:10.3.255.255  Mask:255.255.0.0
..
eth2      Link encap:Ethernet  HWaddr 00:26:55:56:21:D8  
          inet addr:10.4.29.1  Bcast:10.4.63.255  Mask:255.255.192.0
..

The 10.4 network is configured on eth2 and the 10.3 network in on eth0.

3. Try pinging a database node on the 10.4.network

4.  If the ping fails then check /var/log/messages for evidence of any issues with network devices, paying particular attention for messages relating to the interface over which 

the management link has been configured.   This was the message seen on the lmg03 AP:   

Nov  6 00:28:28 lmg03-ap1-rack2-enc9-1 kernel: nx_nic[eth2]: Device temperature 100 degrees C exceeds maximum allowed. Hardware has been shut down.

The message indicates that the device using eth2 shutdown due to overheating, which explains why SMD commands that ran over the "MAN link" had stopped working. You now need to find out what that network device is called so that it can be investigated further.

5.  Note the physical location for the interface identified in step 2.  eth2 maps to 0000:07:00.0

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; ls /sys/bus/*/devices/*/net:*/address |sort
/sys/bus/pci/devices/0000:02:00.0/net:eth0/address
/sys/bus/pci/devices/0000:02:00.1/net:eth1/address
/sys/bus/pci/devices/0000:03:00.0/net:eth2-mbd/address
/sys/bus/pci/devices/0000:03:00.1/net:eth3-mbd/address
/sys/bus/pci/devices/0000:07:00.0/net:eth2/address
/sys/bus/pci/devices/0000:07:00.1/net:eth3/address

6.  Lookup the name of the ethernet device referencing the physical location identified in step 3. 0000:07:00.0 maps to NetXen 1/10 Gigabit Server Adapter

wxadmin wxadmin@lmg03-ap1-rack2-enc9-1:~&gt; /sbin/lspci | grep Ethernet
02:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
03:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
03:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
07:00.0 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10 Gigabit Server Adapter (rev 42)
07:00.1 Ethernet controller: NetXen Incorporated NX3031 Multifunction 1/10 Gigabit Server Adapter (rev 42)

Clearly identifying an alternative interface means finding one that's not configured to use the failed NIC. We can see from the output above that eth0 is configured on the 

Broadcom device and so it's a potential alternative.  The earlier output from ifconfig showed that eth0 was on the 10.3 network so logon to a database node, identify the IP address for eth0 and try pinging that address from the AP.  If that works then switch the "MAN link" over to eth0.   


STAGE 2 - SWITCHING MANAGEMENT LINK TO A DIFFERENT NIC/NETWORK

WARNING - It's strongly advisable to perform an initial test with one or two servers to confirm the move has worked ok before making the change to all servers.

1. Connect to the rdp server, take two copies of the local_config file, one as a backup, one for editing. 

logon to the rdp server -&gt; Deployment Console -&gt; right-click LMG03 -&gt; execute -&gt;
	cp /opt/kognitio/wx2/etc/local_config /opt/kognitio/wx2/etc/local_config.old
	cp /opt/kognitio/wx2/etc/local_config /opt/kognitio/wx2/home/wxadmin/local_config.new

2. Edit local_config.new file on both APs and one 'test' DB node specifying the new interface over which the management network should run.    

[system]
management_net=eth0

3. If the local_config is identical on all DB nodes then use wxsync to synch the file edited in the previous steps across all DB nodes 

wxsync -a '{can DB}' -S /opt/kognitio/wx2/home/wxadmin/local_config.new

If the local_config file is not the same on all DB nodes then edit the files manually one by one, or better still look at why the files are different and try to get them into a consistent state where they can be synchronised.
  
4. Stop the smd on the two APS and test DB node only with "wxsvc stop" 

5. Copy the edited local_config file over the live one on the two APs and test DB nodes

cp /opt/kognitio/wx2/home/wxadmin/local_config.new /opt/kognitio/wx2/etc/local_config

6. Start the smd on the two APs and test DB nodes with "wxsvc start"

7.  Run "wxprobe -H" from one of the APs to check that it can see the two APs and test DB node 

8. If the previous step was succesful then make the same change on the remaining nodes via RDP.  

logon to the rdp server -&gt; Deployment Console -&gt; right-click WX2 -&gt; execute -&gt;
	/opt/kognitio/wx2/current/bin/wxsvc stop 
	cp /opt/kognitio/wx2/home/wxadmin/local_config.new /opt/kognitio/wx2/etc/local_config
	/opt/kognitio/wx2/current/bin/wxsvc start

9.  Run "wxprobe -H" and check that all nodes are present 

10.  Restart the server to prevent expired TCP connections timing out and causing a MISC node crash down the line.  This may not be necessary if running with a version that includes  I10537

11.  Note that some operations such as debug dumps and logfile writes may still try to use the old network if that's what the MPK link is configured on and so this may only be suitable as a temporary workaround until the faulty NIC is fixed.

In the case of lmg03 the fix was to set "Thermal Configuration -&gt; Increased Cooling" in the system bios after ensuring firmware was up-to-date to prevent the NetXen NICs from overheating.
</t>
<t tx="jonathanhudson.20201007162331.1"></t>
<t tx="jonathanhudson.20201007162604.1">@language md

#### wxbackup is a command line tool which can be used to backup your KAP data. 

There are a number of different options available :- 

#### Metadata backup 

This will backup the "infrastructure" of the database. So users, object definitions, privileges, etc. will be backed up but the actual data won't be.Useful if you want to recreate the structure of your database on another machine (e.g. a development / test system).

#### Full backup 

This will backup everything.

#### Incremental backup

You must have a full backup completed first to use this option, which will backup the changes since the last full / incremental backup. Note that you need to tie this in with disk recovery operations - the man page for wxbackup has more details.

#### Targetted backup 

You can specify specific schemas and objects in those schemas to be backed if required.

Attached is a script which caters for metadata, full and incremental backups, and could be setup in cron as a regular job if required, although be aware of generic issues with running script through cron as discussed here. 

A typical weekly backup schedule could look like the following, replacing &lt;YOURDSN&gt; with the DSN name of your KAP instance :- 

```
00 22 * * 0 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt; FULLBASE
00 22 * * 1 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL 
00 22 * * 2 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 3 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 4 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  INCREMENTAL
00 22 * * 5 /opt/kognitio/wx2/home/wxadmin/support/scripts/WX2Backup.sh &lt;YOURDSN&gt;  LASTINCREMENTAL 
```

Note: LASTINCREMENTAL has --expect-incremental removed. This is so that a reclaim/repack can recover as much disk space as possible. The next backup after the reclaim must be a full backup. 

All the options available in the attached script can be run manually from the command line and is only designed to help setup a backup strategy.

Further details on options available to wxbackup can be found in the documentation or the man page for wxbackup. 

For restoring data again, there are details in the documentation and the man page for wxrestore. 


</t>
<t tx="jonathanhudson.20201007162651.1">@language md

Preceding query with "debug" to get timings, etc
Solution 00000763 by Kognitio Support at 2012-07-17T12:57:21.000+0000

It is possible to precede a query with the "debug" keyword to collect extra information on issues like query performance. In all cases, extra information is output in `wxlogd smd` to one or more cidebug files.

The following are all useful:

```
debug time &lt;query&gt;
```

shows how long the interpreter spends in each part of the SCode.

```
debug time 1 &lt;query&gt;
```

also includes a summary of compiler time before the interpreter time. In the cidebug file you will see something like the following:

```
17-07_13:24:16_BST: TME: Syntax     0.000092 secs (0.000092)   3.00%

17-07_13:24:16_BST: TME: Resolution 0.002571 secs
(0.002663)  83.72%

17-07_13:24:16_BST: TME: Semantics  0.000062 secs (0.002725)   2.02%

17-07_13:24:16_BST: TME: Generator  0.000341 secs (0.003066)  11.10%

17-07_13:24:16_BST: TME: Final      0.000005 secs (0.003071)   0.16%
```

```
debug time 10 &lt;query&gt;
```

provides information on every step through the query transformer too - do not go above 10 as then the time spent generating the debug output becomes significant.

```
debug {time [x]} &lt;query&gt;
```

allows the query to run correctly - the items in {} tell the ODBC driver this is part of the debug statement, and hence it knows to expect results. This is only supported in 7.2.1 and later ODBC drivers.

Be aware that with streaming switched on (i.e. the default mode), some of the interpreter times can be misleading - the interpreter can issue AI requests, with the underlying activity being deferred by the AI in many cases, so steps may appear to complete very quickly because they have been deferred, and their cost will appear under a later operation.
</t>
<t tx="jonathanhudson.20201007162947.1">@language md

#### Reclaim space from slab 2 (the logging slab)

If the logging slab (slab 2) becomes full, logging of user connections, transactions and queries is prevented.
To re-enable logging, space needs to be freed up. The most effective way to do this is by copying any log data you want to keep to user tables, then truncating all the tables which reside on slab 2.

As SYS, switch off logging  and then disconnect to make the change effective for your next session:- 

set parameter llev to 0;
disconnect; 

Reconnect. 


Get a global session:- 

Switching off logging is only effective from the start of a session so if there are existing sessions already on the system, logging may still be active for them. You will need to therefore, abort all existing sessions and obtain a global lock to ensure nothing further is logged on slab 2 before truncation. 

update sys.ipe_allcursessions set abort = 2 where session &lt;&gt; current_session;
lock system;



Make a copy of the main logging tables to somewhere else. In this example, we'll create them in the LOG_ARCHIVE schema which needs to exist prior to running anything further. 



a) Find all the tables that reside on slab 2 currently. Occasionally, new tables are introduced and assigned to slab 2 and also some tables are moved from slab 2 to the user slabs. The query below will generate SQL that you can then run separately :- 

select 'CREATE TABLE LOG_ARCHIVE.'||trim(t.name)||' DISK FOR SELECT * FROM SYS.'||trim(t.name)||';'
from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2); 

Note - you may see errors such as "Table &lt;name&gt; already exists" if you've run the above before. However, the above may also catch any potentially new system tables that have been assigned to slab 2. 


b) Archive the data into the tables created in the LOG_ARCHIVE schema. The query below will generate SQL that you can then run separately :- 

select 'INSERT INTO LOG_ARCHIVE.'||trim(t.name)||' SELECT * FROM SYS.'||trim(t.name)||';'
from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2);

c) Truncate the tables that reside on slab 2. The query below will generate SQL that you can then run separately :- 

select 'TRUNCATE TABLE SYS.'||trim(t.name)||';' from ipe_alltable t where t.id in (select table_id from ipe_ftable where partition_id = 2)

The last truncate should start zeroing of all slab 2 instances which means the space in slab 2 has been recovered, although disk performance will be impacted for a while.



Re-enable logging
Still as SYS, do the following to re-enable logging

set parameter llev to 3;
disconnect;  -- You must disconnect for  the parameter change to be effective. 



Optionally, copy some of the logging rows back. If you have security classes which depend on logging rows (e.g. a check on how long a user has gone without logging in), it is best practice to copy relevant rows back into the logging tables. Note - when you copy rows back, you'll need to ensure that you are not introducing duplicates each time you do this. 



EXAMPLES:- 



insert into log_archive.ipe_command select * from sys.ipe_command where cdate &gt;= current_date - interval '30' day 
except select * from log_archive.ipe_command; 

insert into sys.ipe_command select * from log_archive.ipe_command where cdate &gt;= current_date - interval '30' day 
except select * from sys.ipe_command;
</t>
<t tx="jonathanhudson.20201007163100.1">@language md

#### How do I find header information from a row in the database?

Each database row has a header that is used internally by
the database.

It can be useful to see this information for the purposes of problem diagnosis (e.g. finding out which transaction changed a row, then looking up the query and user for that transaction).

Header information is exposed in SQL by the following functions; the
transaction number functions have been available for a long time and the other
functions were added in 7.1.2.

WX_CREATE_TNO()

Transaction number when row was inserted

WX_UPDATE_TNO()

Transaction number when row was deleted
(or replaced by a new value, because we don't modify rows in-place). Note that deleted rows persist until their space is recovered by a reclaim/repack or other disk space recovery operation.

WX_RAMPROC()

MPID of Ramstore on where this row is located

WX_RAMADDR()

Memory address in Ramstore where this row is
located

WX_DISKADDR()

Disk address where this row is located

WX_DISKMPID()

MPID of Diskstore where this row is located

WX_DISKPTN()

Slab on disk where this row is located

The RAM address functions do not apply to disk-only tables and the disk address functions do not apply to RAM-only
tables.

The location of a row in memory is uniquely described using the

tuple (WX_RAMPROC(), WX_RAMADDR()) and a row on disk is uniquely described
using (WX_DISKMPID(), WX_DISKADDR(), WX_DISKPTN()).

If a table is replicated then that row will occur once in every ramstore. If you select WX_RAMPROC() or WX_RAMADDR() from a
replicated table, the server will infer that you want to see each row from every ramstore (i.e. pretend it's random).
</t>
<t tx="jonathanhudson.20201007163326.1">@language md

#### How do I use the IPE_MPK* tables?

There are three tables: ipe_mpk_stats, ipe_mpk_link_stats and ipe_mpk_link_peer_stats.  

These provide information on the MPK at 3 levels of granularity:

ipe_mpk_stats has one row for every MPID in the system (every Kognitio heavyweight Linux process has a message passing ID called an MPID).

ipe_mpk_link_stats has one row for every link for every MPID, so a node with 3 links has 3 rows per MPID.

ipe_mpk_link_peer_stats has one row for each destination for each MPID and Link. This means that in a system with 2 links and 100 MPIDs each process will generate 200 rows for this table, 100 for each target MPID on link 0 and one for each target on link 1.

More detail on each of the tables is contained below, and finally some example queries are given to extract useful information from these tables.

### ipe_mpk_stats

Most of the columns in here are self explanatory. It contains counters for things that have happened on that MPID. Many of these are counters for events that won't make much sense unless you understand the internals of the MPK but some are useful. Interesting values to look at are:

#### frames_resent

this counts the number of times we had to resend something. This is handy for finding bottlenecks in the system.

#### noroute_frames

this counts the number of times we couldn't send a message because all links are broken. Should never happen but handy for finding problems.

#### send_rate receive_rate

These are the approximate rate of message passing in bytes/second at that point in time. You can sum this over a node to get an idea of how much 
data is going in/out but this is approximate.

#### nacks_sent and nacks_received:

these are an indication of flow control happening. When a node is getting data too quickly for it to handle it starts sending out nacks to tell other nodes to slow down. Things like hashing on a skewed value sometimes do this. By looking at the values in here you can often see where the skewed data is coming from and going too, but be aware there can be quite a bit of backscatter here; when a node is congested it often tells everyone who sends anything to back off rather than just the nodes which are flooding it.

#### deferred packets

The current length of the deferred message queue. This is an indication of memory fragmentation on the node. Messages are usually deferred because they have been received as separate frames but can't be assembled into the target address space. Usually these should be very transient, it is a problem if the same message stays deferred for long.

#### readqueue_toggles and using_readqueue

this happens in certain heavy-traffic situations on the MPK. When the MPK is having trouble delivering messages (because the recipient is busy, memory is fragmented, or whatever) we activate the readqueue which slows the MPK down a little but makes it easier to deal with nacks, defer messages, etc. Version 7.1 used the readqueue all the time.

### ipe_mpk_link_stats

Stats and counters that relate to traffic for that MPID on a particular link. Here you have the transmission control values for the link:

#### windowsize
how many unacked messages can be outstanding at any time.

#### timeout
how long (in ms) do we wait before resending a message.

#### messages_outstanding
how many unacked messages sent from this mpid on this link are outstanding.
These get adjusted dynamically by the server. They can be an indicator of problems, particularly if the timeout ends up high and the resend count is going up a lot.

The link status values are here too:

#### active 
1 or 0 depending on whether or not we think the link works.

#### score 
This is used to measure the link's health. Retransmissions reduce the score and good acks received increase it. Below 0 for a period of time causes the link to be set to inactive.

#### yoyo_count

a link 'yoyo' is a link going down and then coming back up again. When the link goes from down to up the yoyo count increases. At 5 the link cannot be made active again. It goes down by 1 every 5 hours or so until it gets to 0.
Generally speaking these are not used much by the recent versions of the MPK, which just uses the ones in the peer_stats file instead.

Then there are counters that are breakdowns of the ones in ipe_mpk_stats above to provide more granularity.  The recent_XXX values are included in the total_XXX ones of the same name and periodically reset to 0,

### ipe_mpk_link_peer_stats

This has one row for each 'route' in the MPK. A route is a directional connection between a sending and receiving MPID over a given link. This has route status values (score, active, yoyo_count) which mirror those in ipe_mpk_link_stats above but these ones are the primary ones which actually control which routes are considered 'working' and which aren't. You can get a lot of useful information from these values by aggregating with appropriate grouping to track down where problem links are.

#### peer_mpid
the MPID the route is sending to.

#### mpid
the MPID the messages come from.

#### out_frames_queued
the current number of frames queued for that destination. This value is actually shared across all links between a given pair of MPIDs because the output queue is shared.

#### messages_outstanding
this breaks down the messages_outstanding value from ipe_mpk_link_stats above. Often if there's a slow node lots of links will have messages outstanding to it and you can use this with aggregation to find things like that out.

#### frames_resent
the number of frames resent on that particular route. This is very useful for drilling down to pinpoint network faults.

#### nacks_sent and nacks_received
these are breakouts of the ones above and are useful for pinpointing the source and/or target of congestion.

Often you want to join one or more of these tables to others to perform aggregation. A useful join is between the mpid value (and/or peer_mpid) and the MPID column in ipe_process. This join gives the processor_nr value from the relevant ipe_process row, which is actually the physical node number of the node the process is on. You can use this to group values by physical node so you can, for example, get the count of resends from every physical node to every other physical node and order descending to see if there are excessive resends to a given node. You can also join processor_nr to the node_nr column in ipe_nodeinfo, which has 1 row per node and contains things like the enclosure name of the node. This allows you to aggregate MPK stats up to the enclosure level for finding switch problems, etc.

Some example queries:

-- find nodes containing outbound routes marked as bad
```
 select ni.wx2_node_name, link, count(*) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni 
where ni.node_nr = p.processor_nr 
  and p.mpid = s.mpid
  and active = 0
group by 1, 2
```

-- find nodes with bad routes going into them (more useful) 
```
select ni.wx2_node_name, link, count(*) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni 
where ni.node_nr = p.processor_nr
 and p.mpid = s.peer_mpid
 and active = 0
group by 1, 2
```

-- Sum resends by source and dest enclosure to see if there's a bottleneck

-- if you have a bad switch the top rows all have the same link and dest_enc which tells you which switch
```
 select ni.node_location as source_enc, p_ni.node_location as dest_enc, link, sum(frames_resent) 
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni, ipe_process p_p, ipe_nodeinfo p_ni 
where ni.node_nr = p.processor_nr
 and p.mpid = s.mpid
 and p_ni.node_nr = p_p.processor_nr
 and p_p.mpid = s.peer_mpid 
group by 1, 2, 3 
order by 4 desc
```

-- Check if one node is the target of excessive resends - in which case that node might be slow

-- ideally only do this after restart and shape test, as otherwise the cause might be e.g. skewed queries

select p_ni.wx2_node_name, link, sum(frames_resent)
from ipe_mpk_link_peer_stats s, ipe_process p, ipe_nodeinfo ni, ipe_process p_p, ipe_nodeinfo p_ni
where ni.node_nr = p.processor_nr
 and p.mpid = s.mpid
 and p_ni.node_nr = p_p.processor_nr
 and p_p.mpid = s.peer_mpid
group by 1, 2
order by 3 desc



-- one way to check for a slow node if previous query suggests problems is to check all nodes can do something CPU intensive

-- in roughly the same time. For example, run the following from the Linux command line on a node and compare real times for consistency:

-- wxtool -S 'time dd if=/dev/zero bs=1024 count=1000000 | gzip &gt; /dev/null'

</t>
<t tx="jonathanhudson.20201008103615.1">@language md

To resume an interrupted restore, use the following process:
     
1) Determine the manifest_id for the interrupted restore by reviewing the restore log file. You should see a line like the following (in this case the manifest_id is 1):

 -&gt; Successfully uploaded jobs to manifest 1.

2) Set all jobs that were running at the time of the "interruption", back to non-executed:

```
wxrestore -s &lt;dsn&gt; -u &lt;user&gt; -p &lt;password&gt; -m &lt;manifest_id&gt; -c resume 
```

3) Run all remaining jobs:
     
```
wxrestore -s &lt;dsn&gt; -u &lt;user&gt; -p &lt;password&gt; -m &lt;manifest_id&gt; -c run 
```     
     
Note that the above steps are not the same as rerunning failed jobs. To do that, specify "-c retry" instead after taking steps to allow those failed jobs to run.</t>
<t tx="jonathanhudson.20201008112647.1">@language md

#### Note

If the query is unioning together multiple external tables (7 or more), this may be a repeat of cases 21913, 21983

Once you have established that a session cannot be aborted you will need to go through the following steps in order to capture the correct debug information:-

- Look in the serverdbg file for the session in question and check to see if there is any record of it not being aborted due to being a solo operation. If there are entries of that nature then there is no need to go through the remaining steps. 

- Collect a history file

- Run a diagnose of the query (note this could cause a second hung session so only do this if it doesn't matter)

- Capture the entire contents of **IPE_DISK_ACCESS** and **IPE_ALLRAM_ACCESS** for the session in question and send to development straight away asking them if they can identify which ramstores should be dumped out. Usually you'll be required to dump out a ramstore and wait for it to be debugged in order to find out what else should be dumped off. This is likely to be an iterative process. Be aware that dumping out a WX2 process carries a small risk of crashing the server so this should be scheduled with the customer for a convenient time usually out-of-hours. 

If development are not available then look in **IPE_ALLRAM_ACCESS** for the session in question and try to find examples of ramstores that stand out as different, perhaps because they have a different number of accesses or they have a different state to the others. If there are no obvious candidates then try to categorise the ramstores into groups and dump out an example from each group. **EDIT**: Andy M suggests doing the following. Look in **ipe_allram_access**, and look at the **num_children column**. Usually the hung process will have 0 children, so dump off anything with 0 children. You can also look at the parent_mpid column - the hung process will often be the only one which is not the parent of any other row in **ipe_allram_access**, i.e. it's at the bottom of the tree formed by parent_mpid and should therefore be dumped off. Also each unique STATE string should be dumped off. 

- Identify the MPID of the IO node through which the session connected and dump it out.

- Retrieve net_if_name from **IPE_ALLCURSESSIONS**

```
select 
    net_if_name 
    from ipe_allcursessions
where session = &lt;unabortable-session&gt;;
```

- Take the last 4 digits from net_if_name and convert to decimal to get the MPID

- Run wxprobe -a '{mpid &lt;mpid&gt;}' -l to identify the appropriate pid and IP address then dump it out 

- Identify the MPID of the interpreter and dump it out

- Retrieve any request_id for the unabortable session

```
select 
    max(request_id) request_id
from ipe_allram_access 
    where session = &lt;unabortable-session&gt;;
```

- convert the request_id to hex then take the last four digits and convert to decimal to get the MPID

- Run 

```
wxprobe -a '{mpid &lt;mpid&gt;}' -l 
```

to identify the appropriate pid and IP address then dump it out 

- dump off the mpid column from **ipe_allcursessions**.  The mpid column, when converted to hex, will give a number like 0x02000271. Take the non-zero digits from the end and convert back to decimal. This will be the mpid to be dumped off, and it will be either an interpreter or compiler. 

For example, if the mpid column is 33555089, convert this to hex ( 0x02000291) and take the last non-zero digits (291), convert this to decimal (657) and dump that off. 

- Capture a full dump with 

```
wxdgtool -D
```

- Send all of the output from the previous steps to development

- Schedule a restart of WX2 in order to clear the session once you're satisfied that all of the required core files have been captured.  Note: sometimes a restart is not necessary, if the session is waiting for an external script to exit. These queries can be aborted by killing the relevant processes and their children. In this case you will see 'XIWait' in the state field of a row in **ipe_allram_access** for the session, which means it is waiting for an external process to exit. 

- Take the mpid and request_id for that row, and run 

```
wxtool -a '{mpid &lt;mpid&gt;}' -S 'ps -o pid,pgid,cmd | grep &lt;request_id&gt;'
```

- to see the relevant external process, along with it's PGID (process group ID). 

- Take the PGID (second column), and run 

```
wxtool -a '{mpid &lt;mpid&gt;}' -S 'ps -o pid,pgid,cmd | grep &lt;PGID&gt;
```

- to see a list of processes on that node with the given PGID. The PGID is important as external scripts can start child processes which also need to be killed (KAP will wait for the external script and all it's children to exit before the query commits). 

- From the resulting processes, kill any ones which are related to the given external script by running 

```
wxtool -a '{mpid &lt;mpid&gt;}' -S 'kill -9 &lt;pid&gt;'
```

- Then in the same way kill any other processes with corresponding XIWait rows in **ipe_allram_access**. Then the query should commit. 

</t>
<t tx="jonathanhudson.20201008114136.1">@language md

#### How do I determine if Kognitio is down?

If you are unable to connect to a Kognitio system or run queries on it, there are a number of possible causes. The most likely are:

There is a network connectivity issue
The DBA is performing an administration task and has locked out other users
The server has exhausted some resource (e.g. interpreters for running SQL, or client connections allowed)
The server is down
To determine what is causing the problem in your instance, there are a number of steps you can take:

Get a Linux command line prompt on a node in the system as the wxadmin user, or another user setup to use the Kognitio 
command line utilities. 

```
wxserver info state
```

to verify whether the server is running. If the following output appears, the server should be running - i.e. you are not in state (4) from the list above:

```
Current state: Booted.
Goal state: Booted.
Second Goal State: UNKNOWN.
Status: OK.
Current operation: .
```

If the output Status line is as follows the server is down. Follow the instructions here to collect information and restart the system:

```
Status: Fatal error entering state - ERROR: DES crash detected by monitor..
```

Note that it is possible to access the command line for a node but not connect to it via ODBC due to e.g. firewall restrictions, so being able to access the command line does not eliminate a networking problem as the cause of the original connectivity issue.

You can verify that all is well by using 

```
wxsubmit -s localhost &lt;username&gt; -p &lt;password&gt;
```

from the Linux command line on a database node. This should bring up a prompt allowing you to run SQL.

If this connection attempt, or a connection attempt from your client, returns error "AH0001" this indicates an administration task in underway (i.e. state (2) from the list above) - contact your database administrator to find out what administration is being performed, and when the system is expected to be available.

If the wxsubmit mentioned above allows queries to be run but your original client did not, then it is most likely that your original problem was caused by a network issue (state (1) from the list above). Contact your network administrator for further investigation.

Try to connect as the SYS user - some resources such as interpreters for running SQL are reserved for SYS, so if you can connect and run queries as SYS but not as another user the problem is either that the administrator has locked other users out (using login restrictions / queue pausing), or that some resource has been exhausted. 

Check with your DBA for the first case (or if you are the DBA verify that login restriction / queue pausing are not causing the problem). For the second case, you can either identify which client is hogging resources by looking in **IPE_ALLCURSESSIONS** from your SYS connection, or you can collect information to allow later analysis of the problem and restart the server to clear the issue using the information here.

If you cannot resolve the problem, the instructions here explain how to collect information and restart the system.
</t>
<t tx="jonathanhudson.20201008114802.1">@language md

This solution is for INTERNAL USE ONLY. It details how to collect information on a server crash/hang to give the development team the best chance of identifying the underlying problem. It should not be made accessible to external users, as the steps involved can be somewhat convoluted.


#### DETERMINE WHAT TYPE OF CRASH/HANG HAS OCCURRED.

In the case of a ramstore, diskstore, interpreter or compiler crash, you will be
required to dump off debug information from one of the individual crashed
nodes. Debug information for other types of crashes can be extracted with a
standard hands-off dump.

First run wxprobe -s. This will give you a quick readout of the processes that have crashed, if any. If you see the following output from wxprobe -s then there are no crashed processes at this point in time.

```
&gt; wxprobe -s

Kognitio WX2 Hardware Discovery Tool v6.01.07-r on wx2-system1

(c)Copyright Kognitio Ltd 2001-2008.

No problems found.
```

However, if the system has hung you should go to step (2.2) below to capture general information on the state of the system before restarting. If there are crashed processes from wxprobe -s, then the output will look similar to the example below. However, wxprobe -s output is not always in the same order that the processes crashed in. To see the correct order of events (assuming there were any crashed processes), you will need to view the monitor file on the master smd node. Run wxserver info master to see which node is the current master, then ssh to that node and run less `wxlogd smd`/monitor*. After browsing to the correct time period you should see similar output to wxprobe -s. 

The important crash to look for is the first non-watchdog process that crashed. In the example below, this is process 28286. 

```
T_2015-07-07_10:34:57_BST: Detected a crash on node madison01-wx2-rack1-enc1-1: 
T_2015-07-07_10:34:57_BST: Process 28261: Name WXDB(WATCHDOG), nthreads 1, state T, size 15044608. 
T_2015-07-07_10:34:57_BST: : ppid 8609, pgid 28261, tracerpid 0, time 20(10,10)+0(0,0). 
T_2015-07-07_10:34:57_BST: : status Crashed(ERROR: Child pid 28286 stopped (probably crashed)). 
T_2015-07-07_10:34:57_BST: : mpid -1, type WATCHDOG 
T_2015-07-07_10:34:57_BST: SERVER CRASH! 
T_2015-07-07_10:34:57_BST: Detected a crash on node madison01-wx2-rack1-enc1-1: 
T_2015-07-07_10:34:57_BST: Process 28286: Name WXDB(0): Ramstore, nthreads 54, state T, size 3994587136. 
T_2015-07-07_10:34:57_BST: : ppid 28261, pgid 28261, tracerpid 0, time 464(203,261)+17769796(5453491,12316305). 
T_2015-07-07_10:34:57_BST: : status Crashed(ERROR: Signal SEGV at EIP 0x04DFE2FE data address 0x4dfe2fe: address not mapped to object). 
T_2015-07-07_10:34:57_BST: : mpid 0, type Ramstore
```

Look for the "type"
in the last line of the output to identify what type of process has crashed
e.g. ramstore, diskstore, io node etc.

#### RETRIEVING THE CORE FILE INFORMATION

Having established in step 1 that a crash has occurred
you now need to extract the debug files and compress them ready to be sent to
Kognitio. Follow the steps in 2.1 and 2.2 for ramstore, diskstore, compiler or interpreter crashes,
for any other type of crash go straight to step 2.2 .

If the monitor file shows multiple crashed processes, it's normally sufficient to only dump off the first instance of each type. The exception to this is if the status message points to another process. This is shown in the example below:

```
T_2015-06-20_18:33:52_BST: Monitor started. 
T_2015-06-21_15:47:54_BST: Detected a crash on node bbb01-wx2-rack1-enc1-13: 
T_2015-06-21_15:47:54_BST: Process 15430: Name WXDB(WATCHDOG), nthreads 1, state T, size 16502784. 
T_2015-06-21_15:47:54_BST: : ppid 1, pgid 15430, tracerpid 0, time 31(13,18)+0(0,0). 
T_2015-06-21_15:47:54_BST: : status Crashed(ERROR: Child pid 15522 stopped (probably crashed)). 
T_2015-06-21_15:47:54_BST: : mpid -1, type WATCHDOG 
T_2015-06-21_15:47:54_BST: SERVER CRASH! 
T_2015-06-21_15:47:54_BST: Detected a crash on node bbb01-wx2-rack1-enc1-13: 
T_2015-06-21_15:47:54_BST: Process 15522: Name WXDB(353): Interpreter, nthreads 33, state T, size 94523392. 
T_2015-06-21_15:47:54_BST: : ppid 15430, pgid 15430, tracerpid 0, time 6(3,3)+78785(37290,41495). 
T_2015-06-21_15:47:54_BST: : status Crashed(ERROR: Too many resends to 286. ). 
T_2015-06-21_15:47:54_BST: : mpid 353, type Interpreter 
```

In this example, the first non-watchdog process (pid 15522, mpid 353) crashed with "too many resends to 286", meaning that it tried repeatedly to send data to another process (mpid 286) and was unable to do so. In this case, both mpid 353 and mpid 286 would need to be dumped off, since mpid 286 may have been unresponsive and hence needs to be investigated. To find the Linux pid corresponding to mpid 286, you would run: wxprobe -l | grep -B 3 "mpid 286,". for example:

```
&gt; wxprobe -l | grep -B 3 "mpid 286,"

Kognitio WX2 Hardware Discovery Tool v8.01.00-rel141124 on wxsupp01

(c)Copyright Kognitio Ltd 2001-2014.

  Process 10532: Name WXDB(286): Ramstore, nthreads 56, state S, size 3935567872.

              : ppid 10502, pgid 10502, tracerpid 0, time 238(158,80)+72625(18973,53652).

              : status Running.

              : mpid 286, type Ramstore

To find the node that process was running on, run: wxprobe -a '{mpid 57}' -l | head -n1
```

#### CAPTURING SPECIFIC INFORMATION FOR LARGE PROCESSES (Ramstore / diskstore crashes)

If any node in the system running an SMD contains a linux
partition that has more than 10GB disk free, then you can run the dump command
from there. This is referred to as a "local dump" . If none of the
nodes running an SMD have at least 10GB disk free in a linux partition then you
must dump the information to a remote node, referred to as a "remote
dump". Decide whether you are going to dump from a local or remote node
and chose either option i) or ii) below as appropriate.

i) WHEN THERE IS A NODE WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (LOCAL DUMP)

The syntax to target a crashed process from a node within
the system is: 

```
wxdgdump -t &lt;target-host-ip&gt; -p &lt;crashed-process-id&gt; -C &lt;core-filename&gt;
```

The target-host-ip for any node can be found by running: wxprobe -i and looking at eth2 interface for that node.

It's helpful to specify the type of crash and mpid of the
crash in the output filename so using the monitor file output from step 1 as an
example you would run the following to dump off the crashed process:-

```
&gt; cd &lt;volume with at least 10Gb free disk space&gt;
&gt; wxdgdump -t 10.1.2.3 -p 28286 -C rs0.core
&gt; gzip rs0.core
```

Repeat the above steps for any other processes that need to be dumped, then go to step 2.2 to gather general debug information.

ii) WHEN THERE ARE NO NODES WITH AT LEAST 10GB FREE DISK SPACE AVAILABLE (REMOTE DUMP)


If no nodes in the system have sufficient disk space free
to store a core file (typically these can be several Gbytes) then you should
identify a node outside of the system which does have sufficient space and ssh
from there.

The syntax to target a crashed process from a remote node
is 

```
ssh wxadmin@&lt;externally-accessible-node&gt; 
wxdgtool -o - -c\'"target {&lt;target-host-ip&gt;},pid &lt;crashed-process-id&gt;"\'-d &gt; &lt;core-filename&gt;
```

It's helpful to specify the type of crash and mpid of the
crash in the output filename so using the monitor file output from step 1 as an
example you would run the following:-

```
&gt; cd &lt;volume with at least 10Gb free disk space&gt;
&gt; ssh wxadmin@10.1.2.3 
wxdgtool -o - -c \'"target {10.1.2.3},pid 28286"\' -d &gt; rs0.core
&gt; gzip rs0.core
```

Repeat the above for any other processes that need to be dumped, then go to step 2.2 to gather general debug information.

#### CAPTURING GENERAL DEBUG INFORMATION

First take a copy of the logging directory. Connect to
any node in the system, navigate to /var/log and tar the wx2 directory as
follows:

```
&gt; ssh wxadmin@any-node-in-system
&gt; cd /var/log
&gt; tar czvf /tmp/logfiles.tgz wx2
```

You will now need to use "wxdgtool -D" to
generate info, history and wxd files, with the wxd file containing the relevant
core files.

If any node in the system running an SMD contains a linux
partition that has more than 10GB disk free, then you can run the dump command
from there. This is referred to as a "local dump" . If none of the
nodes running an SMD have at least 10GB disk free in a linux partition then you
must dump the information to a remote node, referred to as a "remote
dump". Decide whether you are going to dump from a local or remote node
and chose either option i) or ii) below as appropriate.

i) WHEN THERE IS A NODE WITH AT LEAST 10GB FREE DISK SPACE AVAILABLE (LOCAL DUMP)

```
wxdgtool -D
```
command will create a timestamped core, info and history file. These should be extracted, tar'd and
compressed as follows:-

```
&gt; cd &lt;volume with at least 10Gb free disk space&gt;
&gt; wxdgtool -D -O . 
&gt; tar czvf wxdgtool.tgz &lt;core-file history-fileinfo-file&gt;
```

ii) WHEN THERE ARE NO NODES WITH AT LEAST 10GB FREE DISK
SPACE AVAILABLE (REMOTE DUMP)

If no nodes in the system have sufficient disk space you
should identify a node outside of the system which has sufficient space and ssh
from there, e.g. here I'm starting an ssh connection to 10.1.2.3 from a remote
node and running wxdgtool:

```
remote-node&gt; cd &lt;volume with at least 10Gb freedisk space&gt;

remote-node&gt; ssh wxadmin@10.1.2.3
wxdgtool -D -o - &gt; wxdgtool.wxd

remote-node&gt; gzip wxdgtool.wx
```

#### RESTARTING THE SERVER

Having extracted the debug information in step 2, the
Kognitio system can be restarted. However different systems need to be started in different ways. Some can be simply restarted with "wxserver start" which will
restart the database and run a "create image" restoring the system to
it's original state prior to the crash. In version 8 and above, "wxserver start" will attempt to start with image recovery, which is much faster than other methods, as it reuses data that are already in memory. However not all systems require this. At the time of writing, startup instructions are held in a 'password file' which is on a shared network drive. Ask a colleague if you don't know where this is. 

#### VIEWING THE HISTORY FILE

Often the proximate cause of a crash can be determined by looking at the last few queries run on the system leading up to the crash. These queries can be viewed in two places: system tables (ipe_command, ipe_transaction etc) and the history file (which was generated by wxdgtool -D). It's important to investigate this, as otherwise a customer might run the same query again after a restart, causing a second crash. Of course not all crashes are triggered by a query. The first thing to note is the exact time when the first crashed process showed up in the monitor file. In the example in step 1, this was 10:34:57. Obviously if a query caused this crash, it must have occurred before this. Note that the problem query will not always be the most recent one before 10:34:57, sometimes it will be a query started at say 10:19:00 which took some time to reach the stage that caused the crash. There's no easy way to be sure which query crashed the system, but the following techniques can help to make an educated guess, and any such guesses can then be relayed to the customer, so they can avoid running them:

```
wxsqlhist -A -f &lt;history file&gt;              
```

this will send a listing of recent queries to stdout. with transaction numbers, session id's etc. These tno's can be looked up  in ipe_transaction and ipe_command to see when they occurred. 

```
wxsqlhist -f history.T_2015-07-08_22:02:02_BST -qer | grep "mp 0" | tail             
```

this will show you which sessions and tno's were running for mpid 0. This is useful if mpid 0 was the first non-watchdog process that crashed. Often the last line of output will be the tno/session which caused the crash

```
wxsqlhist -f history.T_2015-07-08_22:02:02_BST -t 11289820 -qer                     
```
this shows detailed information for tno 11289820

```
select * from ipe_alltransaction where tno = 11289820                                
```

when run in the database, this will show you the time that the tno occurred. 

The nature of the query can also help, for example a "select * from ipe_allschema" is unlikely to have caused a crash, whereas a more complicated query, or a query using newly introduced features, or little-tested features is more likely to be the culprit.

#### MOVING FILES

After the above steps are complete, the dump files need to be moved to a location where the devs can access them for analysis. At time of writing, this is the support box (193.35.206.116) in /debug/helpdesk. So run ssh -A 193.35.206.116, then cd /debug/helpdesk. In this directory there are some directories of the form &lt;company name&gt; and some of the form &lt;system name&gt;. cd to the affected system name if it's there. Otherwise cd to the company name, and then cd to the system name, which should exist as a subdirectory beneath the &lt;company name&gt; directory. In either case, you will then want to create a new directory for that date. For example, mkdir 20150723; cd 20150723.  This will be the destination directory for the dump files. Before retrieving the files, make sure they are compressed. To retrieve the files, run rsync -P &lt;ip of affected system&gt;:/&lt;path to file&gt; ./ for each file. rsync -P will ensure partial files are kept, so if large file transfers get interrupted, you can resume from where you left off by running the same command again. When the files have been copied, run chmod +r ./* to make them readable by the devs. 







</t>
<t tx="jonathanhudson.20201008115752.1"></t>
<t tx="jonathanhudson.20201008115754.1">@language md

remove a node from a system for repair/diagnostics
Solution 00000779 by Kognitio Support at 2016-06-14T11:18:08.000+0000
If a node in a system has suspected hardware issues, it can be removed from the system to allow diagnostics to be run in isolation, and/or the node to be repaired.

To do this, use the following checklist:
ensure that the system has software RAID on - check in the `wxlogd startup`/clustermap file to ensure RAID clusters have &gt; 1 member each. If software RAID is not already on, you cannot remove a node with one or more disk resources on it.

use to stop the database processes on all nodes. This prevents processes on the problem node from interfering with the processes on the remaining nodes in future.

```
wxserver halt
```

as root on the problem node, do 

```
/opt/kognitio/wx2/current/bin/wxsvc stop
```
to stop the smd.

as root on the problem node, edit the config file with 

```
vi /opt/kognitio/wx2/etc/config
```
and set a different system_id under the [general] heading. e.g. "system_id=foo". 

This prevents the node from rejoining the rest if e.g. it is rebooted. THE SYSTEM_ID MUST BE 11 CHARACTERS OR LESS.
change the daemon_port setting under [general], e.g. "daemon_port=1634". This is to prevent users connecting through to any single node test/soak instance that you create. You need to do this even if the node does not have an external interface, as must ensure that e.g. the AP does not manage to connect to it when running ETL/backup.

if possible, edit ODBC DSNs everywhere to ensure the removed node is not included in the set of IP addresses for the system, but the list of ServerAddress&lt;x&gt; entries is still contiguous - e.g. if the missing node was ServerAddress5 out of 10, comment it out, and rename ServerAddress10 as ServerAddress5. Ensure this is done for DSNs used by backup, JDBC bridge, AP connections, ... Notify users to do the same 

if you do not have access to all DSN settings. The only downside if this is not done is that connection attempts may periodically be slow, as they wait for the connection to the missing node to time out. 

**IMPORTANT** these DSNs need to be changed back to their original settings when the node is reintroduced after repair. So make a note or set yourself a reminder for that.  

use 

```
wxviconf
```

to edit the config file on the remaining nodes. Add **virtual_diskstores=yes** to the **[boot options]** section, to allow restarting 
with the node missing.

restart the database software on the remaining nodes, which will make the system usable again. Note that disk performance will be noticably worse as the database will be using software RAID to reconstruct contents of any missing disks. Performance from RAM will also be impacted by the loss of RAM and CPU.  

if the system is running with standby disks then you can replace the failed disk with the standby at this step to quickly restore disk performance.     

See the solution "HOWTO: How do I add or introduce a standby disk to an existing Kognitio instance?" for details.

When the node has been repaired, follow the steps in solution 780 (HOWTO: Replace a faulty node in a Kognitio database) to reintroduce it into the system.
</t>
<t tx="jonathanhudson.20201008120118.1">@language md

#### Replace a faulty node in a Kognitio database

Prerequisites:-

The database is configured to use software RAID -  check in the `wxlogd startup`/clustermap file to ensure RAID clusters have &gt; 1 member each. 

If software RAID is not already on, you cannot remove a node with one or more disk resources on it.
the database is able to run with virtual diskstores either by having virtual_diskstores=1 in [boot options] and/or by having reliability_features=yes in [wxsmd] in the config file.

#### Remove the faulty node

This stage requires a database restart and so will require downtime to be scheduled if the database is up and running. 

Follow the steps in the solution entitled "HOWTO: remove a node from a system for repair/diagnostics" to ensure the node is safely removed and the database restarted.

#### Re-introduce the replacement node 

NOTE: If the replacement node was rebuilt e.g OS and KAP s/w reinstallation rather than having just h/w replaced, check that the KAP s/w version running on the remaining database nodes matches. If not, install it and change the current_pointer to that version on the replacement node otherwise there is a risk an older version may be used instead which could cause compatibility issues.

This stage requires a database restart and may therefore require some scheduled downtime. 

Note that B41278 which went into 8.1.0rel150709, increases the size of the write buffer to speed up disk zeroing.  With this change KAP can become unresponsive when zeroing disks on nodes that are running DB processes, and so it is recommended that disks be zeroed before RAM and CPU resources from those nodes has been introduced into the system.   

- 2.1 If the replacement node has identical hardware, ensure that drivers/firmware on the replacement node are identical to the other nodes. For example, use 'ethtool -i eth2' to check drivers/firmware on eth2, and similarly for other interfaces. 
- 2.2 ensure that the database software and the smd on the replacement node are stopped
- 2.3 copy the global config file from an existing node to the replacement node ensuring file ownership and permissions are retained
- 2.4 ensure that the current pointer on the replacement node is pointing at the same version of software as an existing node 
- 2.5 run wxsvc restart on the replacement node then wait a few seconds and run wxprobe -H to check that the full complement of nodes are present

#### RESTARTING WITH NEW DISKS 

Only execute steps 2.5 to 2.8 if the replacement node has been fitted with new disks or if the UID on the original disks has changed i.e. the node was commissioned as a single node system for testing

- 2.5 zero the disk on the replacement node with wxtool -Z.   You can monitor the progress of the disk zeroing by viewing the output file in the current startup directory on the replacement node
- 2.6 once zeroing has completed run wxprobe -wD to confirm the status has changed to disk_is_zeroed 1 for the disk on the replacement node and note the new UID
- 2.7 view the clustermap for the current boot on the main system and note down the UID for the failed drive i.e. the drive marked as &lt;virtual ds&gt;
- 2.8 having noted the UID of the replacement disk(s) in step 2.6 (new UID) and the UID of the virtual diskstore(s) in step 2.7 (original UID), run wxserver start [sysimage] without recovery replace uid &lt;original UID&gt; with &lt;new UID&gt;

Typically you would include the sysimage option if the client has an imaging script or if the normal restart instruction for that system is a system image rather than a full image (which is usually the case if their create image time is prohibitively long ).

If you need to restart with multiple disk replacements then use wxserver start [sysimage] without recovery replace uid &lt;old-uid-1&gt; with &lt;new-uid-1&gt; uid &lt;old-uid-2&gt; with &lt;new-uid-2&gt; ...

#### RESTARTING WITH ORIGINAL DISKS 

Only execute step 2.9 if the replacement node is using the original disks with unaltered Kognitio partitions

- 2.9 run wxserver start [sysimage] without recovery to restart the database 

#### Final checks

Connect to the database and run select mpid, status from ipe_xor_element order by 1; to check the status of the disks.  If the system is set to automatically recreate disks then you will see that the re-introduced disks have a status of 4 indicating that they are recreating.  If the system is not set to recreate disks automatically you will need to recreate the disks manually with recreate disk &lt;mpid&gt; for each disk with a status of 0. Note that recreates run in the background, so the recreate disk command will return immediately, but the disk recreate will continue running for some time.  The disk status will be set to 1 once the recreate completes at which point the disk re-integration is complete.

Finally, if any DSNs (e.g. odbc.ini) were previously edited to remove the faulty node's IP, ensure they are changed back.

********** REMEMBER TO SCHEDULE A PARITY CHECK *********
  </t>
<t tx="jonathanhudson.20201008120544.1">@language md

#### Write efficient queries for Kognitio

This solution contains advice for writing efficient queries. A lot of the tips
below are common to many other database products as well as Kognitio.

A) minimise memory requirements for your query

Kognitio uses buffers for each internal access. The larger the rows in an
access, the less rows fit in a buffer, and the longer the query will take in
general. Therefore, it makes sense to minimise the memory requirements for each
step of a query, particularly steps in the query which are dealing with large
numbers of rows.

1) Do not use VARCHARs for very small strings

VARCHAR fields have an 8 byte overhead per record, so it is not advisable to
use them for strings of less than 10 bytes. It is also not advisable to use
them for strings where the variation in length is very small, as the 8 byte
overhead will never be recovered.

2) Do not join or group on large strings

Minimising the number of large strings in each buffer will speed up a query. In
addition, much more computation is required to join/group on large strings
compared to performing these operations on e.g. an integer.

3) Avoid carrying large strings through a query

Similarly to point (2) above, large strings slow down queries. Wherever
possible, use a small numeric type instead, then use a JOIN or CASE statement
at the end of the query to convert back to a string if required.

4) Use view images rather than table images when possible

View images have a smaller per-row overhead than table images. However, they
are a "snapshot" of the underlying data, so the images need to be
recreated to reflect changes in that data.

5) Do not specify very large VARCHAR fields if these are not required

Kognitio client software, such as the ODBC driver, determines how many rows can
be retrieved from the server by dividing its fetch buffer size by the maximum
size of a row. So specifying lots of unnecessarily large VARCHAR fields in an
object will ensure fewer rows are retrieved at a time when querying that
object, resulting in poor performance.

B) Do not make the server do unnecessary work

Often you will know a lot of information about the data that the server does not know, and you can take advantage of this to prevent the server doing unnecessary work.

1) Do not group on expressions unnecessarily

If grouping on something which is already known to be unique, do not add other
expressions for grouping, as these introduce more computational work for no
benefit.

2) Use UNION ALL rather than UNION if you know no duplicates will be
generated

UNION will attempt to remove duplicates across the data, which is an expensive
operation for large data sets with no duplicates. So use UNION ALL whenever it
is applicable to avoid this unnecessary processing.

3) Avoid unnecessary DISTINCT operations

Similar to the above, if you know a column is unique, do not add a DISTINCT as
that will be an expensive operation on an already-unique column.

4) Group by the most selective columns first (ideally non-string columns)
and keep nested grouping order consistent

If grouping by a set of columns, group by the most selective ones first. Keep
the order of grouping consistent if nesting grouping.

So 
               
GROUP BY l, c, d, t) x
GROUP BY l, c, d, t y
GROUP BY l, c, d) z

Should perform better than

GROUP BY t, c, l, d) x
GROUP BY l, c, t, d) y               -- arbitrary change in grouping column order
GROUP BY l, c, d) z

5) Remove duplicate values on ETL, rather than in a query

If your data has duplicates, remove these during ETL processing rather than
adding overhead for every query to remove them with e.g. window functions.

C) Skew, imaging, statistics

For best performance, you need to understand about skew which results in a small number of processes becoming the bottleneck on a query. Also, using imaging to speed up queries, and collecting statistics to give the SQL optimiser as much information as possible.

1) Watch out for skew, and then remove it

Monitor the Kognitio system for skew. Look at memory usage during a query and
see if it skews onto one or a small number of ram store processes. If this is
the case, consider how the query / memory distributions could be modified to
remove the skew. For example, partial hashing can be used to deal with a small
number of skewed values, or perhaps they are not of interest in the final
result set and could be filtered out at an early stage in processing.

2) Consider imaging intermediate objects when investigating performance
issues

This can help identify skew values. It can also be useful in the final
production system as having images allows the SQL optimiser to make better
decisions for subsequent steps of processing.

3) Collect statistics on frequently used objects

Collecting statistics on objects gives the SQL optimiser the best chance of
producing an optimal query plan.

D) disk performance advice

Kognitio is in in-memory system, with a persistence layer on disk. There are a number of ways to improve performance when e.g. doing transformations to disk-based tables, or bringing disk-based data into memory for analysis. 

1) Consider truncation then re-insert rather than update

If transforming a disk-based table to change a high percentage of rows,
consider doing the transformation in RAM, truncating the original table, then
writing back the new rows from RAM (all in one transaction, of course). This is
because an update which hits a high percentage of rows can be a lot slower than
generating a new copy of the table in RAM, truncating the old table, then
inserting the new rows back to disk.

2) Use Compressed Data Maps (CDMs) for disk-based tables which are accessed
by clustered attributes

If you have a large table with no RAM image, and you frequently access it by an
attribute which the data is clustered on when it is loaded (e.g. date /
region), try adding a CDM on that attribute to speed up the data load. This is
worthwhile even if you only do this imaging as part of daily/weekly processing.

3) create VIEWs on top of objects, rather than creating new tables from them
with a large number of rows

Avoid unnecessary duplication of data by creating views (and possibly then
imaging those views in a separate statement), rather than creating new tables
which copy a lot of existing rows into the new table.

4) Ensure that disk-based tables involved in queries are defragmented

If a disk-based table is involved in a query, see if scanning that table seems
slow for the number of rows in the table (remembering that deleted rows will be
scanned, but truncated ones won't be). 

If the scan is slow, look to defragment the table with something like
"alter table t1 set slabs to all migrate defrag", but with the
appropriate target slabs specified. 

Note this should not be done if the table is very large (as a lot of disk space
will be used), and the operation will run more quickly if the table has no
memory image at the time.

If the table is small and used regularly (e.g. it is a status table updated by
an ETL process), consider always having it in RAM to prevent repeatedly
scanning the fragmented table on disk.

E) Best practice

There are a number of steps which are best categorised as best practice - these improve maintainability of queries, and ensure full advantage is taken of the Kognitio platform.

1) Avoid non-deterministic functions

Floating point aggregation, and window functions in which the partition and
ordering do not deterministically order the rows, can cause confusion as
multiple runs of the same query do not generate identical results.

2) Use INNER joins rather than OUTER joins where possible

If you know your data well enough, ensure you use INNER joins whenever possible
rather than OUTER join "just in case" - outer joins are more costly,
so if they are known to be unnecessary, remove them. 

3) Use a development system to work on significant new projects

Having a separate development systems isolates your changes from production.
Currently you can use Kognitio with up to 128GB RAM without having to pay for any
licence (although you will only get community support with this offering).

4) Upgrade to the latest version of Kognitio software regularly on all
systems

This simplifies administration as you can use the same tools on all systems,
and take advantage of performance enhancements, new functionality and bug
fixes.

5) Read about features in release notes, so you can take advantage of them

New feature often simplify application coding, and improve performance with no
additional investment.

6) Raise cases for all non-trivial issues

This allows Kognitio support to assist with problems, and also feeds into
future product development as common issues are identified.


</t>
<t tx="jonathanhudson.20201008121853.1">@language md

deal with RPC port clashes
Solution 00000792 by Kognitio Support at 2013-03-14T14:17:23.000+0000

By default, the Kognitio software uses a number of privileges ports (in the range 0..1023) for internal communication between database processes. Normally the server starts from port 150 and works up, looking for a range of ports which are free. Once it finds such a range, it will then try to reserve those ports. 

It is possible for RPC services to try to grab a port the Kognitio software wants to use between the initial scan above, and the ensuing reservation of that port. In such as case, the software will fail to start, typically with a failure to connect to the server. Often this will be accompanied with a message "SOCKET DROPPED OUT!  RE-ESTABLISHING ...
" in the smd output file for one or more nodes.

In such cases, the options are:

1) move the port range used by the Kognitio software. 
The range used can be changed by making the following config file entries (the first one is the start port, the second one is the number of ports to check from there). This only makes sense if the new range is guaranteed not to be used by RPC services:

```
[network]

mpkports=700

mpkportct=300
```

2) change the port range used for RPC services, or disable services to stop them using ports

The range of ports used for RPC should be visible by looking in `/proc/sys/sunrpc/min_resvport` and `/proc/sys/sunrpc/max_resvport`

Typically this is something like 665 - 1023, which is why the Kognitio software defaults to using a lower port range. If the RPC range is problematic, it can be changed in `/etc/sysctl.conf` - your administrator should know how to do this.

Tools like `rpcinfo` can be used to see a snapshot of the RPC services that are holding ports at any given time, and `netstat -anlp` as root can show all port usage, so repeatedly running these can help identify which ports are being used by non-Kognitio software, and what services are using them, helping in identifying problems services which could be stopped or reconfigured to use different port ranges.
</t>
<t tx="jonathanhudson.20201008122547.1">@language md

How to collect the replay log for a query

From 8.1.0 the compiler can generate a replay log to allow analysis of problems, such as CI5500 errors. To collect the log, do the
following:

1. Set current_session parameter ci_replay_flags to 3
2. Perform the query
3. Drop current_session parameter ci_replay_flags
4. Find out the tno of the query (if there might be &gt;1)
5. Look in /tmp on all nodes for a file called /wx2-replay-tno&lt;tno&gt;-XXXXXX,and pass on to development.  

```
wxlogd dumps`, aka /var/log/wx2/dumps-&lt;systemid&gt;
```

You can also extract this information from a compiler core file.  

ci_replay_flags are 1 

which means "log replay in memory", and 2 which means "write replay log to /tmp".  

The default is 1

which means it will be embedded in a core file and can be obtained as follows;
however, if we find that this makes it run out of memory (I'm not aware of this
happening) then it can be set to 0 so that it doesn't log.


1. Set

```
current_session parameter ci_replay_flags to 1 (the default) or 3. 
```

2. Perform the query.

3. Dump and fetch the compiler core file.

4. Use the debugger to get the address of GE.ReplayLogStart.

5. Run this to output the replay log from the corefile memory:
    
```
wxdhdiagnose -r  -c &lt;corefile&gt; -b &lt;address&gt; &gt; replayfile.out
```

You then run that (or the ouput from the method Paul described) with the standalone compiler from the same software version,

optionally forcing debugging flags, using:

```
   /nextgen/.../out/GPLinux/bin/compile
[-d] -r &lt;replayfile&gt;
```

The behaviour of the standalone with the replay should be
exactly the same because the replay log is a record of the replies from all the
calls to other parts of the system (STI, AI, PV, ...).  

Differences or problems are likely to be:

1. If there is a "#ifdef CI_ALONE". I'd regard that as a bug, and I think we have eliminated all of these.

2. If you are running with a different software version.  That can't happen by accident because it checks and will give an error, but you can remove the check from CmpCompile() if you need to.

3. If there is a bug in the replay implementation.  When writing the replay log it includes a tag that can be checked when replaying; if
it finds a mismatch then it will error.

4. If something "external" happens.  There might be a nomem that happens when logging but not replaying, or vice-versa.  The compiler is single-threaded so that shouldn't be an issue unless another wxdb thread does a memory overwrite.

</t>
<t tx="jonathanhudson.20201008124905.1">@language md

### Predefined submit queries/shortcuts

Overview

A number of useful SQL queries have been predefined in wxsubmit to give a quick overview of various aspects of a Kognitio instance. These queries can be accessed through the '$fn' shortcut capabilities of wxsubmit, and are also pre-loaded as function key bindings in Kognitio Console by default.

As these queries access certain ipe_* system tables, the Kognitio user running these queries will need to be a member of the pre-defined groups 'grp_monitor' and 'grp_diskusage' or be the SYS user.

NOTE: The output of these queries is subject to change as the underlying capabilities of the Kognitioserver are modified or enhanced. 

#### $f0 - High-Level Overview Of A WX2 Instance

 
System ID   |Platform|Version|Patch Level|Nodes|Disk GB  |RAM GB   |RAID CS

xxxxxxxxxxx |Linux   |70000  |c          |   12|  1,168.2|    167.3|      4


where 'Disk GB' is the total available disk

      'RAM GB'  is the total available RAM

      'RAID CS' is the RAID cluster size, or 'none' if RAID is not being used



#### $f1 - Overall Disk Utilisation


Disk GB  |Used GB  |Free GB  |Avail GB |% Used|% Unav

  1,168.2|    724.5|    443.7|    443.3| 62.0%|  0.1%


where 'Free GB'  is the total free space

      'Avail GB' is the total free space that is available for use

      '% Unav'   is the percentage of free space that is not available
for use normally due to the presence of skewing



#### $f2 - Overall RAM Utilisation


RAM GB   |Used GB  |Free GB  |Avail GB |% Used|% Unav|RS GB    |RS No

    167.3|     56.7|    110.6|    110.5| 33.9%|  0.1%|      2.0|   84


where 'Free GB'  is the total free space

      'Avail GB' is the total free space that is available for use

      '% Unav'   is the percentage of free space that is not available for use (normally due to the presence of skewing)
      'RS GB'    is the size of each RAM Store

      'RS No'    is the number of RAM Stores



#### $f3 - Disk Status By MPID


MPID   |CLID|Disk Status       |MPID GB  |Used GB  |Free GB  |% Used

     11|   0|good              |     97.3|     60.4|     36.9| 62.1%

     23|   0|good              |     97.3|     60.4|     37.0| 62.0%

     35|   0|good              |     97.3|     60.4|     36.9| 62.0%

     47|   0|good              |     97.3|     60.4|     37.0| 62.0%

     59|   1|good              |     97.3|     60.4|     37.0| 62.0%

     71|   1|good              |     97.3|     60.4|     37.0| 62.0%

     83|   1|good              |     97.3|     60.4|     37.0| 62.0%

     95|   1|good              |     97.3|     60.3|     37.0| 62.0%

    107|   2|good              |     97.3|     60.4|     37.0| 62.0%

    119|   2|good              |     97.3|     60.4|     37.0| 62.0%

    131|   2|good              |     97.3|     60.4|     36.9| 62.0%

    143|   2|good              |     97.3|     60.4|     37.0| 62.0%


where 'CLID' is the RAID Cluster ID that the MPID belongs to

    


#### $f4 - Disk Status By Slab


Slab ID|Slab GB  |Used GB  |Free GB  |% Used

      9|     61.5|     46.0|     15.5| 74.9%

      8|    182.7|     58.0|    124.7| 31.8%

      7|    182.7|    111.2|     71.5| 60.9%

      6|    182.7|    123.8|     58.8| 67.8%

      5|    182.7|    130.2|     52.4| 71.3%

      4|    182.7|    127.2|     55.5| 69.6%

      3|    182.7|    127.8|     54.9| 69.9%

      2|      5.3|      0.2|      5.1|  3.4%

      1|      5.3|      0.0|      5.3|  0.2%

Total  |  1,168.2|    724.5|    443.7| 62.0%



#### $f5 - Current Sessions Summary


Session|User        |Client      |Status    |Queue       |Dur  |Work GB|Cmd

 400643| DEMO       | L041       | idle     | -          | -   | -     |

========================================================================

SELECT name, disc_frag, type, id FROM sys.ipe_table t, sy

s.ipe_base b WHERE t.id = b.table_id and t.type &amp;lt;&amp;gt; 'V'and

 schema_id = 3 UNION All SELECT name, cast(case when frag

ment_id is null then 0 else 2 end as int ) as RamStatus, 

type,id FROM sys.ipe_table t, sys.ipe_view_im

________________________________________________________________________

 400923| DEMO       | poc3-ap2   | running* | 1          |  5s | -     |

========================================================================

update t_ret_sale set storeno=-1 where storeno=-1

________________________________________________________________________

 400927| DEMO       | poc3-ap2   | running  | 1          | 22s |   3.1 |

========================================================================

select count(*) from t_ret_sale

________________________________________________________________________


where 'Status'  is the current status of the query. Note that any 
status with a '*' after it indicates that the
query is currently blocked waiting for a lock, e.g. in
the example above, session 400923 is actually blocked
by a lock held by another session, in this case 400927.
'Queue'   is the ID of the queue on which the query is
currently running

      'Dur'     is the current query run-time measured in
whole seconds, minutes or hours

      'Work GB' is the amount of temporary working RAM currently
being used by the query

      


#### $f6 - Disk Space Used By Tables In Current Schema


Table Name                      |Schema  |Num Rows       |Size GB

...

T_RET_PROD2                     |DEMO    |         16,739|     0.0

T_RET_PRODUCT                   |DEMO    |         16,739|     0.0

T_RET_PROD_DEPT                 |DEMO    |              7|     0.0

T_RET_PROD_GROUP                |DEMO    |             70|     0.0

T_RET_PROD_SECTION              |DEMO    |            477|     0.0

T_RET_SALE                      |DEMO    |    526,576,615|    25.5

T_RET_SALE_1999Q1               |DEMO    |     84,453,317|     4.1

T_RET_SALE_1999Q2               |DEMO    |    262,104,312|    12.7

T_RET_SALE_1999Q3               |DEMO    |    180,018,986|     8.7

T_RET_STORE                     |DEMO    |          1,099|     0.0

T_RET_STORE_TEST                |DEMO    |             99|     0.0

T_TELCO_ACCOUNTS                |DEMO    |        725,036|     0.2

T_TELCO_CALL_TYPE_LOOKUP        |DEMO    |              5|     0.0

T_TELCO_CDRS                    |DEMO    |  1,429,798,920|   106.5

T_TELCO_SCHEME_LOOKUP           |DEMO    |              9|     0.0

T_TELCO_STATE_LOOKUP            |DEMO    |            339|     0.0

total ALLOCATED space           |        |               |   287.1

total DROPPED space             |        |               |     0.4

total _OVERALL space            |        |               |   287.6


[ NOTE: when run as the SYS user the figures are given for the tables
in all schemas not just the current one
]

where 'total DROPPED space' is space that can be reclaimed. This
figure does not include space removed
through table truncation or row deletes

      


#### $f7 - RAM Spaced Used By Images In Current Schema


Image Name                       |Num Rows       |Size GB |Details

T_RET_STORE                      |         84,000|     0.0|tble repl

V_RET_SALE                       |  3,159,459,690|    82.7|view rand

total TABLE IMAGE space          |               |     0.0|

total VIEW IMAGE space           |               |    82.7|

total _OVERALL space             |               |    82.7|


[ NOTE: when run as the SYS user the figures are given for the images
in all schemas not just the current one
]
      

The $f7 command can also be used to monitor images as they are
created, in which case the image is marked as 'in progress' along
with its ID:


Image Name                       |Num Rows       |Size GB |Details

in progress (10000020)           |  1,143,656,888|    30.5|

total VIEW IMAGE space           |               |    30.5|

total _OVERALL space             |               |    30.5|



#### $f8 - Slab Allocations For Tables In Current Schema


Table Name                      |Schema  |Used Slabs          |Available Slabs

IPE_ALLAUTHORISED_KEYS          |SYS     |1                   |def

IPE_ALLBASE                     |SYS     |1                   |def

...

IPE_REM_SERVER                  |SYS     |1                   |def

IPE_SEC_CLASS                   |SYS     |1                   |def

IPE_ALLLOGIN                    |SYS     |2                   |def

IPE_ALLTRANSACTION              |SYS     |2                   |def

IPE_COMMAND                     |SYS     |2                   |def

IPE_ERRORLOG                    |SYS     |2                   |def

IPE_NODE                        |SYS     |2                   |def

IPE_FILE_FORMAT                 |DEMO    |3                   |def

IPE_IMPORT_ERRORS               |DEMO    |3                   |def

T_FIN_ACCOUNT                   |DEMO    |3,5*                |3,4

T_FIN_ACC_ACTIONS               |DEMO    |3                   |3,4

T_FIN_MERCHANT                  |DEMO    |3                   |3,4

T_FIN_MERCH_DEPT                |DEMO    |3                   |3,4

IPE_ALLPRIVS_DOMAINTEXT         |SYS     |9                   |def

IPE_ALLPRIVS_TEXT               |SYS     |9                   |def

IPE_BC_PRIVNOS                  |SYS     |9                   |def

...

IPE_TYPE_TEXT                   |SYS     |9                   |def

IPE_VI_NULLS_TEXT               |SYS     |9                   |def


[ NOTE: when run as the SYS user the allocations are given for the tables
in all schemas not just the current one ]

where 'Used Slabs'       are the slabs where data is currently
stored for the table

      'Available Slabs'  are the slabs where new data for the
table will be stored

      '*'                typically indicates that a table has been
truncated but its space cannot be
automatically reclaimed due to the
presence of other non-empty tables on

                         the slab
</t>
<t tx="jonathanhudson.20201008125129.1">@language md

The following solution aims to help troubleshoot certain errors and collect all necessary information before reporting to development. This should allow us to address problems in a more timely manner, without as many iterations between support and development. In time, we will modify the solution to be applicable for end-users, and then advise them to attach all information collected to the relevant case.

Each error / set of errors is followed by a list of actions for troubleshooting:

#### CG0106: Internal code generator error

1) Prefix the query that gives the error with "diagnose". For example:

diagnose select * from sys.ipe_system 

Capture this output to a file.

2) Prefix the query with "debug 12":

debug 12 select * from sys.ipe_system

This will output debug info into a file called cidebug.&lt;timestamp&gt; with the `wxlogd smd` directory on each of the WX2 nodes in the system. Take a copy of any cidebug files within that directory that have been written to since the debug command ran, and pass them to support along with the other information capture above.

3) Finally, from one of the WXDB nodes, execute the following command and pass the output to support:

grep 0106 `wxlogd smd`/server*

#### CG065: Invalid Date Value 

Check to see if any have any references to something like "current_date - interval '6' month"- if so, verify this is a valid date. For example if the current date was 2011-03-31 then - 6 months would make the date 2011-09-31 and that date does not exist.

A workaround in this case would be to use add_months(current_date,-6) instead. 

#### CIxxxx e.g. "CI5500 Optimiser generator internal error" 

1) Obtain the full sql that caused this error, and a note of the exact error. 

2) Collect "debug 12 ..." output for the query. "debug 12 diagnose .." will run a diagnose but not run the actual query. This is useful if you would normally have to drop images for example and that may not be possible  on a production system. 

3) Run the tool "wxreprotab" on the object or objects involved in the query, as this should allow the problem to be reproduced. Running wxreprotab by itself will give all the options available. This tool has to run as the SYS user. 

Note that it may be useful turning the SELECT statement into a CREATE VIEW for the purposes of running wxreprotab, but this can still give a CI error in certain cases in which case you will need to specify all of the referenced objects on the wxreprotab command line. 
</t>
<t tx="jonathanhudson.20201008125511.1">@language md

Typical symptom for the problem below is that processes crash failing to talk to disk store mpid(s). To confirm this is caused by the problem below, someone from development will need to look at a core file for a disk store in the system which could not be communicated with.

There is a situation a Disk Store can get into where it attempts to do a parity write to another DS in its cluster, yet that Diskstore doesn't have a large enough contguous free chunk of memory to receive that message. Because the sending DS doesn't wait for a reply it is unaware that the message hasn't been received (and likely will never be). However, this means the lock on that disk region will never be released, causing any subsequent writes to that area to block indefinitely. Eventually, the system will crash as something will timeout trying to get a lock.

This can be mitigated by

1. lower the mulw parameter. This governs the maximum number of cache units (8192 bytes) that the Disk Store will attempt to write in a single parity write, with any larger writes being split across multiple messages. This thus requires the receiving disk store to have smaller contiguous chunks available to hold the incoming message. 

The default for this is 150, however basic testing has shown reducing to as low as 40 has no detriment on write performance, with lower values progressively worse.

From version 8.0 this parameter can be set dynamically without resorting to a restart of the system. Previous versions can only have mulw set as a runtime parameter in the config file and thus requires a restart of the system to change the value being used.

2. As the disk store only have a problem when low on memory, the issue can be mitigated by configuring the selective loaders to have a higher remaining ram requirement before accepting new loads.

The default amount of disk store RAM that the selective loaders leave free is 5%. This can be altered with the ds_sl_min_ram parameter.</t>
<t tx="jonathanhudson.20201009081854.1">@language md

#### Notes on using gdb to debug WX2

*  If the source directory has been moved then you can tell GDB using e.g.:
     set substitute-path /nextgen/sccs /kognitio/dev/releases

*  Specify the binary file as the unstripped version /nextgen/..., not
   the stripped version in /opt/wcsserver/ver.../... otherwise you won't see
   any names in the backtrace.
   Or build with UNSTRIP and possibly STATIC.

*  Make sure that you are using the correct version of the binary 
   corresponding to the running program.

*  If you are not using the correct binary or threading model (etc.) then you
   may corrupt the process if you set breakpoints.

*  Things will go wrong if you attach to a process running under Valgrind
   (which runs a simulation of the program).

*  Export LD_LIBRARY_PATH as:
      &lt;path to buildtree&gt;/GPLinux/bin:/opt/wcsserver/current/lib/Linux
   There may be binaries in some of the parent mirror directories, in which 
   case to see their symbols you need to include those directories in reverse
   order after the build tree.

*  If you attach to a stopped process, earlier versions of gdb will hang and
   you will need to do "kill -CONT &lt;inferior pid&gt;".  This is fixed by gdb 7.1.

*  DDD has a bug where the "info thread" command doesn't give function names.

*  Useful:  gdb command "thread apply all backtrace", etc.

*  errno is typically a define, so might not work with threading.  You need
   to use *(int*)__errno_location (), although unfortunately this only works
   for a running process...

*  If your debugging session kills the process, then wxprobe won't be able to
   see it any more and you can no longer do wxdgdump.

*  If you debug at a breakpoint you will stop the process.  This will show up
   in wxprobe even though there's no problem.  Your wxsubmit session may get
   aborted unless you have set "stopd_crash=no" in [wxsmd] in wxviconf.

*  If you have set a breakpoint and the process didn't stop when you expected
   it to, maybe you haven't done so on the right processes (e.g. there are
   several compilers).

*  If DDD crashes with weird X errors it could be that too long a line is
   stored in its command line history:  removing the long lines from
   ~/.ddd/history (or just deleting it) may fix it.

*  These DDD shortcuts are useful for QDB in the compiler:
```
     graph delete display // Delete All
     graph display *(tindex[()].table_addr) // Tab ()
     graph display *(eindex[()].expr_addr) // Exp ()
     graph display *(pindex[()].pred_addr) // Prd ()
     graph display (*(char*)(().data)) @ (().cur_size * 4) // Name ()
     graph display (*(().data)) @ (().cur_size) // Vec ()
```
*  While gdb is attached to a process, dgdump will not be able to dump it.
   Restarting the database will fail with some old versions of WX2 (but not
   any more for some reason).

*  You can create a coredump for the process using the command 
   generate-core-dump.

*  Some of the commands need to have environment variables set, e.g. wxsync.  
   You can do this by copying the `which wxsync` Perl script and changing the
   last line to run bash instead, then running gdb from that shell.

*  You can use solib-absolute-prefix to specify a prefix for libraries to be
   loaded, and solib-search-path to specify which libraries to use instead of
   the default search path.  This is useful if you want to debug a core dump:
```
     set solib-absolute-prefix /path/to/chroot/dir
     file /opt/wcsserver/current/software/Linux/wxsync
     core /nextgen/ben/core     
```
   You can also use it to debug a live process running stripped libraries:

```
     set solib-absolute-prefix /dev/null
     set solib-search-path /lib/tls:/lib:/nextgen/ben/sys50408f-gp/GPLinux/bin:\
        /nextgen/sccs/sys50408-gp/GPLinux/bin:/opt/wcsserver/current/lib/Linux
     exec-file /opt/wcsserver/current/software/Linux/wxsync
     symbol-file /nextgen/ben/sys50408f-gp/GPLinux/bin/wxsync.dynamic
     break cm_set_clocks
```
*  Note that declaring solib-absolute-prefix means that gdb will only look in
   solib-search-path, so if you need to load symbols from libraries in other
   directories gdb won't see them.  (solib-absolute-prefix and sysroot are
   the same.)

*  The above can solve the problem where gdb breaks when a new thread starts.

*  You can (currently) extract stub libraries from a core file that has been
   generated from wxdgdump using wxcoretool.  These contain minimal information
   to let gdb unwind the stack (.eh_frame section in ELF which contains CFI).
   Use "wxcoretool -f &lt;sysroot-dir&gt; &lt;core-file&gt;" and it will create the stub
   libraries in &lt;sysroot-dir&gt;/lib.  Gdb (7 or later) will then debug them
   using the command given by wxcoretool.

*  Gdb has added Python scripting support in 7.2.

*  To dereference a linked list use the following (where "$" in gdb refers to 
   the last variable printed).
```
     p *head
     p *$.next
     &lt;Enter&gt;
     ...
```
*  If you want to debug into libraries such as gputils, it may help to build 
   static versions of the programs, for example:
     `make -r out/GPLinux/bin/wxprobe`
   or: 
     `UNSTRIP=1 STATIC=1 make -r packages`

*  If you get "UNABLE TO LOAD SYSTEM CONFIGURATION" ... "No config file found"
   then set `WCS_BASE_DIR=/opt/wcsserver` or `/opt/kognitio/wx2`

*  Using the new systabgen generated system table definitions from, you can 
   cast a raw system/logging/virtual table record in memory to a more readable
   type, for example:
```
     print ((ipe_alltable_row *)0x12345678)-&gt;id
```
*  You can inspect the records held in a table in memory in a ramstore using
   something like this.  Note that RootChains is an array of 1000 (that is,
   number_of_rootentry_hash_chains) linked lists of tables with id mod 1000.
   
   NB this doesn't work for versions &gt;= 80000.
```  
     # Print out the address and ID for all IPE_ALLTABLE records where the 
     # TYPE is 'T'
     set $file = 1
     set $r = RootChains[$file % 1000]
     while ((struct rootentry *)$r)-&gt;file_id != $file
         set $r = ((struct rootentry *)$r)-&gt;next
     end
     set $x = ((struct rootentry *)$r)-&gt;start_ptr[0]
     set $n = ((struct rootentry *)$r)-&gt;no_recs
     while $n &gt; 0
        # The 6 ints at the start of each record are:
        # 0: Pointer to next record
        # 1: Disk MPID (MSB) and partition (LSB)
        # 2: Disk address
        # 3: Row length
        # 4: Create TNo
        # 5: Update TNo
        set var $row_ptnext = ((int*)$x)[0]
        set var $row_diskid = (((int*)$x)[1] &gt;&gt; 16) &amp; 0xFFFF
        set var $row_diskpt = ((int*)$x)[1] &amp; 0xFFFF
        set var $row_diskad = ((int*)$x)[2]
        set var $row_length = ((int*)$x)[3]
        set var $row_crttno = ((int*)$x)[4]
        set var $row_updtno = ((int*)$x)[5]
        set var $row_ptdata = &amp;((int*)$x)[6]

        if ((ipe_alltable_row*)$row_ptdata)-&gt;type[0] == 'T'
           printf "%x %d\n", $x, ((ipe_alltable_row*)$row_ptdata)-&gt;id
        end

        # Go to next record
        set $x = $row_ptnext
        set $n = $n - 1
     end
```
*  You can print all the values in the stack and the lines they correspond to
   using this.  This can be useful to investigate what happened before a
   longjmp() call.
```
   set $top = topofstack
   set $bot = topofstack - 0x2000000 / 4
   set $pos = $top
   while $pos &gt; $bot
      set $pos = $pos - 4
      set $val = *(int*)$pos
      printf "*(int *)%p = %p: ", $pos, $val
      info line * $val
   end
```
*  You can step through machine code, such as generated code, using "si" or 
   "ni".  Gdb doesn't automatically display the next instruction but you can
   get it to do this using:
```
     define hook-stop
         x/i $eip
     end
```
   You can use the INT3 instruction in assembler/generated code to hard-code
   a breakpoint which a debugger will catch (but without a debugger it will 
   crash).  On i386, you can simply continue from there in the debugger.
   To do this using the code generator use "{I_INT3, NONE, NONE}," inside a
   CODE_BLOCK.

*  To read a record off disk given a disk address, mpid and partition:
   identify the file or device for the "disk" for that mpid.  Now to find the
   offset into the file, look at that mpid's process/core file and in DSMain()
   find dgi-&gt;pl, the partitionlist.  It's a linked list and look for the
   partition you want and get physicalstartsector.
   Then the row should start (with header) at offset
      physicalstartsector * 512 + disk_address * 4

*  If you want to attach to a process but immediately continue to avoid
   affecting it because of the wait, you can use this:
```   
     set $hook_first = 0
     define hook-stop
         if $hook_first == 0
             continue
         end
         set $hook_first = 1
     end
```
*  If things seem plausible but inconsistent, for example the same memory 
   location being different in different stack frames, it is possible you 
   might have a badly built binary: you can try doing "make clean" and
   rebuilding from scratch.

*  You can detect some memory overruns using ElectricFence.  If you are running
   the database then use this in the config file:
```
     [SETENV int]
     LD_PRELOAD=libefence.so.0.0
     EF_ALLOW_MALLOC_0=1
     EF_DISABLE_BANNER=1
```
   If running a process in gdb use this:
```
     set environment LD_PRELOAD libefence.so.0.0
     set environment EF_ALLOW_MALLOC_0 1
     set environment EF_DISABLE_BANNER 1
```
   Don't set EF_ALIGNMENT=0 because unfortunately some of the code assumes
   allocations are aligned.  EF_PROTECT_BELOW=1 detects underruns, but overruns
   (the default) seem to be more common.
   Note that ElectricFence tends to run out of memory and crash.

*  Issues 12014/33 make thread-local variables describing the thread:
     _thisprocess      Pointer to Process structure for ProcAlloc() threads
     _thisname         Name (set by ProcAlloc() or ProcSetName())
     _thismpid         Message passing MPID
     _thisfunc         Entry function (set by ProcAlloc() or ProcSetFunc())
   Thus in gdb you can find a thread more easily using "thread apply all", for
   example:
```
     t a a p _thisname
```
   A crashed thread will have "*** CRASHED ***" prepended to its name.

*  If gdb can't show thread-local variables see the section on libthread_db
   below: if you are cross-debugging - including 64-bit gdb and a 32-bit
   process - then you need to give gdb an appropriate libthread_db.

*  If you want to insert a watchpoint programmatically, rather than attaching
   with gdb or another debugger, you can do so using fork() and ptrace().  See
   /home/ben/misc/watchpoint which came from 
   http://blogs.sun.com/nike/entry/memory_debugger_for_linux.
   Note that this isn't useable yet because it needs to be set for all threads,
   and wxdb is multi-threaded.
   The function to set watchpoints should allow you to choose 1, 2 or 4 byte
   watchpoints.
   It could also do with a function to remove watchpoints.

*  From Issue 10759, the least recently used compiler or interpreter is chosen
   to run queries.  This is a pain when attaching with gdb because it's harder
   to guess which process will be used, but you can disable it by setting the
   parameter sm_alloc_lru to 0.  Alternatively you can attach to multiple
   processes in gdb.

*  Prior to Issue 13961, for certain crashes in core files created by wxdgdump,
   the stack trace (backtrace) stops and the debugger can't unwind it further,
   typically the function that caused a signal to be caught.  This is because
   wxdgdump's core files didn't include AUXV, which gives information useful
   for the debugger.
   The AUXV can be obtained from /proc/&lt;pid&gt;/auxv on the relevant system and 
   GDB can be hacked to obtain that data from say a file instead.

*  You can use the expect program multixterm to control multiple xterms to 
   debug multiple processes at the same time, potentially even on different 
   nodes; this can be useful if you want to break on a function but don't know 
   what interpreter (say) is going to be used.

*  Gdb version 7 can handle multiple processes, called inferiors.  This means
   that you can attach to and set breakpoints on say all the compilers using
   a single gdb invocation.  See /nextgen/ben/bin/multigdb or the following 
   example.
```
     file /opt/wcsserver/current/software/Linux/wxdb
     set pagination off
     set schedule-multiple on
     break GULError_
     attach pid1
     clone-inferior
     inferior 2
     attach pid2
     clone-inferior
     inferior 3
     attach pid3
     ...
     clone-inferior
     inferior n
     attach pidn
     continue
```
   You need to run "set schedule-multiple on" to allow the processes to
   continue at the same time (which isn't the default).
   Note that if you are attaching to a binary with symbols stripped then you
   will need to tell gdb the file for every inferior that you attach to
   otherwise symbols will only be found for the first one.
   (You would use add-inferior if you wanted to debug processes running
   several different binaries.  Here we want clone-inferior so that if we are
   debugging wxdb.direct loaded by wx2-linker we can tell it to use the
   wxdb.direct symbol file once at that start; with add-inferior we'd have to
   do that each time.)

*  If you have a memory leak with memory allocated using malloc() then you
   can follow the allocations programatically.  This worked for one particular
   version of libc but will not in general because other versions will have 
   different memory representations.  "mpd" points to an array allocated 
   fairly early on but you can use another location.  You will want to use
   "info files" to identify memory regions to tell where to stop.
```
   set pagination on
   set $p = (((int*)mpd) - 1)
   set $stopat = 0x14fe6000
   while $p &lt; $stopat
       set $v = ((int*)$p)[0] / 4
       set $t = ((int*)$p)[0] % 4
       printf "\nAllocation 0x%x words type %d at 0x%x:\n", $v, $t, $p
       print/x *(((int*)$p) + 1)@($v - 1)
       set $p = $p + $v
   end
```
   This program that does some allocations and frees might help interpret the 
   results:
```
   #include &lt;stdlib.h&gt;
   #include &lt;string.h&gt;
   #include &lt;unistd.h&gt;
   int main()
   {
     int *x[1000];
     int i;
     for (i = 0; i &lt; 1000; i ++)
     {
       int size = sizeof(int) * ((i * i) % 60);
       x[i] = malloc(size);
       memset(x[i], i, size);
     }
     for (i = 0; i &lt; 1000; i ++)
       if (i % 2)
         free(x[i]);
     while(1)
       sleep(10);
   }
```
*  The following script will walk malloc allocations and show free chunks.  It
   might be useful to determine whether there is a memory leak or memory
   fragmentation.
```
     # NB 1. Check that these values are correct for your libc version!
     #    2. This assumes a valid main_arena symbol.
     set $first_fastbin_offset = 2
     set $num_fastbins = 10
     set $first_bin_offset = 14
     set $num_bins = 128 - 1
     set $next_offset = 272
     set $system_mem_offset = 275
     set $start = &amp;main_arena
     set $arena = $start
     set $sz = 0
     set $unall = 0
     while $sz == 0 || $arena != $start
         set $arenaunall = 0
         set $bin = 0
         while $bin &lt; $num_fastbins
             set $chunk = ((int*)$arena)[$first_fastbin_offset]
             while $chunk != 0
                 set $csz = (((int*)$chunk)[1] &amp; (~0x3))
                 printf "   fastbin chunk unallocated 0x%x size %d\n", $chunk, $csz
                 set $arenaunall = $arenaunall + $csz
                 set $chunk = ((int*)$chunk)[2]
             end
             set $bin = $bin + 1
         end
         set $bin = 0
         while $bin &lt; $num_bins
             set $lastchunk = ((int*)$arena)[$first_bin_offset + 2 * $bin + 1]
             set $chunk = ((int*)$arena)[$first_bin_offset + 2 * $bin]
             while $chunk != 0 &amp;&amp; $chunk != $lastchunk
                 set $csz = (((int*)$chunk)[1] &amp; (~0x3))
                 printf "   bin chunk unallocated 0x%x size %d\n", $chunk, $csz
                 set $arenaunall = $arenaunall + $csz
                 set $chunk = ((int*)$chunk)[2]
                 if $chunk == $lastchunk
                     set $chunk = 0
                 end
             end
             set $bin = $bin + 1
         end
         set $arenasz = ((int*)$arena)[$system_mem_offset]
         set $sz = $sz + $arenasz
         printf "Arena 0x%x: system_mem = %d, unallocated = %d\n", $arena, $arenasz, $arenaunall
         set $unall = $unall + $arenaunall
         set $arena = ((int*)$arena)[$next_offset]
     end
     printf "TOTAL: %d, %d unallocated\n", $sz, $unall
```
*  Memory leaks can be found using the MM allocator, for which the ramsize must
   be positive.  The ramsize is set in the [boot options] section of the config
   file.  (Set se_ramsize or tm_ramsize for the compiler and interpreter.)
   Dump a process using wxdgdump, and then run:
     wxdhdiagnose -c &lt;corefile&gt; -A leak -b &lt;MM_Mem address&gt; -l &lt;MM_MemDim0 len&gt;
   The MM values can be obtained using:
     gdb /opt/kognitio/wx2/current/software/Linux/wxdb &lt;corefile&gt; -ex \
                'printf "-b 0x%x -l %d\n", MM_Mem, MM_MemDim0' -ex quit

*  If you have a core file but didn't obtain /proc/&lt;pid&gt;/maps then you can use
   the command "maintenance info sections".

*  If you have a process that exits on a signal that the handlers won't catch
   then you can get Linux to dump it.  This works regardless of the core
   rlimit settings.
```
      printf '#!/bin/sh\ncat &gt; /tmp/$2.$1.core\n' &gt; /tmp/makecore
      chmod a+x /tmp/makecore

      # Dump using /tmp/makecore script
      echo "|/tmp/makecore %p %e" &gt; /proc/sys/kernel/core_pattern

      # Dump all sections
      echo 127 &gt; /proc/self/coredump_filter
```
*  To turn off the MM allocator for the ramstore use "max_rs_ramsize=-1"
   under [boot options].

*  You can hook any library function call using the --wrap option to the 
   linker.  An example is malloc, realloc, calloc and free in sysinit.c.

*  You can modify values in a core file using wxdhdiagnose's -M option.  Give
   it a colon-separated sequence of rewrites of the form "MEM:&lt;offset&gt;:&lt;value&gt;"
   or "REG:&lt;thread id&gt;:&lt;reg name&gt;:&lt;value&gt;".

   We can use this to fix the stack that linux_signalhandler() munges, so that
   gdb can get a backtrace, using:
```
      wxdhdiagnose -c &lt;corefile&gt; -M "MEM:&lt;off0&gt;:&lt;val0&gt;:MEM:&lt;off1&gt;:&lt;val1&gt;"
```
   where the values:
```
      off0 := &amp;stkframe[0]
      off1 := &amp;stkframe[1]
      val0 := stkframe_preserve[0]
      val1 := stkframe_preserve[1]
```
   can be obtained from gdb (although it is often not necessary to rewrite 
   stkframe[0]) from the relevant thread and linux_signalhandler() frame using:
```
      printf "MEM:0x%x:0x%x:MEM:0x%x:0x%x\n", &amp;stkframe[0], stkframe_preserve[0], &amp;stkframe[1], stkframe_preserve[1]
```
*  For a nasty crash, typically a signal caught in generated code, you can
   rewrite the registers to point to a different frame:
```
   wxdhdiagnose -c &lt;core file&gt; -M REG:&lt;thread pid&gt;:EIP:&lt;eip val&gt;:REG:&lt;thread pid&gt;:EBP:&lt;ebp val&gt;:REG:&lt;thread pid&gt;:ESP:&lt;esp val&gt;
```
   The values for eip, ebp, and esp can be found in the registers listed in
   ucont-&gt;mcontext-&gt;gregs (EIP is 14, EBP is 6 and ESP is 7), or in registers
   for later versions of wxdb.
   Gdb is a bit more fussy when unwinding and you might need to go up another
   frame or two, particularly in generated code.  E.g.
```
   printf "REG:threadpid:EIP:0x%x:REG:threadpid:EBP:0x%x:REG:threadpid:ESP:0x%x\n", ucont-&gt;uc_mcontext-&gt;gregs[14], ucont-&gt;uc_mcontext-&gt;gregs[6], ucont-&gt;uc_mcontext-&gt;gregs[7]
   printf "REG:threadpid:EIP:0x%x:REG:threadpid:EBP:0x%x:REG:threadpid:ESP:0x%x\n", ((int*)*(int*) ucont-&gt;uc_mcontext-&gt;gregs[6])[1], ((int*)*(int*) ucont-&gt;uc_mcontext-&gt;gregs[6])[0], ((int*) ucont-&gt;uc_mcontext-&gt;gregs[6])[0]+8
```
   Or from v8:
```
   printf "REG:threadpid:EIP:0x%x:REG:threadpid:EBP:0x%x:REG:threadpid:ESP:0x%x\n", registers-&gt;eip, registers-&gt;ebp, registers-&gt;esp
   printf "REG:threadpid:EIP:0x%x:REG:threadpid:EBP:0x%x:REG:threadpid:ESP:0x%x\n", ((int*)*(int*)registers-&gt;ebp)[1], ((int*)*(int*)registers-&gt;ebp)[0], ((int*)registers-&gt;ebp)[0]+8
```
*  You can also modify values in a core file using GDB.  Either use the
   command-line option "-write", or the command "set write on".  Note that 
   this applies to the executable as well as the core file, and only to files
   loaded after the command is issued.  So to prevent modifying the binary do:
     file /path/to/exec-file
     set write on
     core /path/to/core-file

*  Some memory leaks are hard to track down because the memory chunk is sent
   to another thread which is supposed to free it.  You might be able to use
   something like the following:
```
      break mpktcp.c:2841   &lt;-- line after allocation
      commands
         printf "Allocated %d bytes at 0x%x by:\n", mfh-&gt;messlen, m-&gt;data
         thread all apply backtrace
         continue
      end
      set logging file gdb.log
      set logging redirect on
      attach 13630
      set logging on
      continue
```
   Then use "wxdhdiagnose -A chunklist -S file:line" to find the allocations
   made and search for the addresses in the gdb output.  You might be able to 
   identify the thread waiting to be sent that chunk.

*  If gdb can't do a backtrace, you can unwind using EBP with:
```
     set $ad = $ebp
     while $ad != 0
         set $insptr = ((int*)$ad)[1]
         info line *$insptr
         set $ad = ((int*)$ad)[0]
     end
```
*  If you want to define a datatype that gdb doesn't know (because of stripped
   symbols or something just not in the binary at all): create a file defining
   a variable of the desired struct, compile with debugging info, and load
   into gdb using "add-symbol-file a.out 0".
   https://stackoverflow.com/questions/7272558/can-we-define-a-new-data-type-in-a-gdb-session

*  To be able to run against 32-bit binaries you need a 32-bit-capable
   valgrind.  Use "yum install -y valgrind.x86_64 valgrind.i686" on AWS Linux.
   Similarly use "yum install -y ElectricFence.i686 ElectricFence.x86_64" for
   ElectricFence.

*  If you are debugging memory leaks, rather than memory access bugs, it can
   be better to set VALGRINDING to 0.  (Setting VALGRINDING to 1 annotates
   MM and GUL allocations but this confuses the Valgrind leak checker, or
   possibly just me.)


#### Issues with parsers

* If you need to debug parsing of a query, either in the Compiler or systabgen
  (which has rubbish error handling), this might help:
````
     break SysTabLexInit
     run
     watch LexCharsRead
     commands
     continue
     end
     display LexCharsRead
     display yytext
     break yyerror
     break exit
     break _exit
     continue
```

#### Issues with processes launched via linker, aka wxdb.direct

*  By default as of snapshots s180315 and 8.2.1 the database will be
   invoked as wxdb.direct, launched via a linker (viz.
   software/Linux/runtime32/wx2-linker).  This is to remove the
   requirement for customers to install 32-bit libraries.  It has the
   advantage that core files from other systems will be debuggable.

*  If you have problems with wxdb.direct you can revert to wxdb.dynamic which
   links to native libraries normally by using "[binaries] wxdb=wxdb.dynamic"
   in the config file.  This requires the 32-bit libraries to be present.
   Note that bugs seen on wxdb.direct might not reproduce on wxdb.dynamic,
   especially if they involve library behaviour.  This does not help if you
   want to debug a corefile or attach to an existing process for wxdb.direct.

*  If you have the debugger launch the process via the linker, e.g. with a
   command like "gdb --args /lib/ld-linux.so.2 myprogram" then you will have
   to load the symbols for myprogram manually, as described at
   https://stackoverflow.com/q/28463987/2319122
   In brief, you need to do "add-symbol-file myprogram &lt;addr&gt;" where &lt;addr&gt;
   is the first address given by "readelf -WS myprogram | grep -F '.text'".
   If you attach to a core file or running process using "gdb myprogram ..."
   then this appears to work fine.

*  When attaching to a process started by the linker, compiled with -rpath,
   gdb is unable to find the appropriate libthread_db and won't show threads.
   Fix as follows; see the section below on libthread_db for more info.
```
      # From shell
      gdb /path/to/wxdb,direct

      # In gdb
      add-auto-load-safe-path /kognitio/tools/compenv/extras/debuglibs/V6p3/GPLinux_64
      set libthread-db-search-path /kognitio/tools/compenv/extras/debuglibs/V6p3/GPLinux_64
      attach &lt;pid&gt;
```
*  Normally wxdb.dynamic runs using the native libc so everything is okay,
   but wxdb.direct uses the libc we supplied (from CentOS 7 on ct6base.dev).

*  Test case:
```
   # Setup
   cat &gt; thread.c &lt;&lt;-EOF
        #include &lt;unistd.h&gt;
        #include &lt;pthread.h&gt;
        #include &lt;sys/types.h&gt;
        #include &lt;signal.h&gt;
        
        
        void *child(void *I_dont_really_need_this)
        {
          while (1)
            sleep (5);
        }
        
        
        int main()
        {
          pthread_t foo;
          pthread_t bar;
          pthread_create (&amp;foo, NULL, child, NULL);
          pthread_create (&amp;bar, NULL, child, NULL);
          while (1)
            sleep (1);
          return 0;
        }
	EOF
   /kognitio/tools/compenv/V6p3/GPLinux/bin/i686-kognitio-linux-gcc -Wl,-rpath='/kognitio/tools/compenv/V6p3/GPLinux/platform/lib' -o thread ~ben/debug/20180320-threads/thread.c -pthread -Wall -ggdb
   /kognitio/tools/compenv/V6p3/GPLinux/platform/lib/ld-linux.so.2 ./thread &amp;

  # Expected: 3 threads shown
  # Observed: "Unable to find libthread_db matching inferior's thread library, thread debugging will not be available" and 1 thread shown
  gdb ./thread -ex "add-auto-load-safe-path /kognitio/tools/compenv/V6p3/GPLinux/i686-kognitio-linux/lib" -ex "set libthread-db-search-path /kognitio/tools/compenv/V6p3/GPLinux/platform/lib" -ex "attach ps h -C ld-linux.so.2 -o pid" -ex "info threads"
```

#### Debugging generated code

*  You can display generated code with annotations by setting the parameter
   cg_log_gencode=1 and then doing "wxdhdiagnose -C -t &lt;tno&gt; -s &lt;stmt&gt;".

*  You can also extract generated code from a core file using
   "wxdhdiagnose -c &lt;core&gt; -C -b &lt;codeblock&gt; -l &lt;cblen&gt;", where codeblock
   and cblen are fields of the relevant rsaccess, given by:
      ((rscontext *)(((int *)&lt;workspace&gt;)[99]))-&gt;acc[0]

*  If multiple functions are present in the generated code then the
   function being run will be noted as accno in the rscontext:
      ((rscontext*)_thisprocess[0].cg_workspace[99])-&gt;accno

*  The workspace is available in kernel_run(), but if that stack frame is
   corrupt it is also given by the $EBP register in generated code.

*  The workspace is also available in _thisprocess-&gt;cg_workspace as of 8.2.4.

*  Addresses in the dump of the generated code are relative to its start.

*  To find an address (e.g. EIP) in the dump use:
      &lt;address&gt; - (intptr_t)((rscontext*)(((int*)$ebp)[99]))-&gt;acode + &lt;doffset&gt;
   where doffset is the offset of the start of the main function - in fact the
   second element of the line before the exclamation marks, probably 0x500.

*  For live debugging you might want to use "set disassemble-next-line on"
   which will disassemble the next instruction to be run (as well as the
   source code line where that's available - but it won't for generated code!)

*  You might also want to use "disassemble /r &lt;start-addr&gt;, &lt;end-addr&gt;" to
   disassemble between specific addresses.  The "/r" option shows the raw
   hex bytes as well.

*  You can switch from the default AT&amp;T disassembly to the Intel notation
   using "set disassembly-flavor intel".

#### Issues with threads and libthread_db

*  When debugging a multi-threaded process, debuggers such as gdb use the
   library libthread_db to identify and debug threads.  It needs to be the
   appropriate version for the threading library that the process is running.
   Otherwise gdb will say "Unable to find libthread_db matching inferior's
   thread library, thread debugging will not be available."

*  To be completely clear: libthread_db is loaded by gdb, not by the process
   being debugged.

*  For more details see http://timetobleed.com/notes-about-an-odd-esoteric-yet-incredibly-useful-library-libthread_db/

*  When the debugger and program being debugged run on the same platform,
   this is fine.  But for cross debugging gdb needs to be able to dlopen()
   libthread_db so it must be available for the host (debugging) platform.
   For example, if you are running gdb on i686 and the process being debugged
   is on ARM then gdb needs an i686 version of libthread_db.

*  The above also applies to a x86_64 version of gdb debugging an i686 process:
   the 64-bit gdb process can't link against a 32-bit library so a 64-bit
   version of libthread_db is needed.

*  For compenv V6p3 we have put a 64-bit version of libthread_db in
   /kognitio/tools/compenv/extras/debuglibs/V6p3/GPLinux_64 so the following
   will tell gdb to look there for it:
      add-auto-load-safe-path /kognitio/tools/compenv/extras/debuglibs/V6p3/GPLinux_64
      set libthread-db-search-path $sdir:$pdir:/kognitio/tools/compenv/extras/debuglibs/V6p3/GPLinux_64
   (We add $sdir and $pdir so that gdb will continue to work with processes
   using the native libthread_db.)

*  For this to work the wxcoretool stub version of libpthread isn't enough.
   You need a full copy of libpthread.  (Probably not but I don't know what is
   missing.)  The rest of the libraries can be stubs.

*  The libpthread and libthread_db on some CentOS, e.g. bob.core at the time
   of writing, do not appear to match - so thread debugging is broken for both
   32 and 64 bit processes!  (glibc-2.17-196.el7_4.2.x86_64 and i686)

*  StackOverflow question on getting thread-local variables from a core dump
   manually.
   https://stackoverflow.com/questions/10841219/thread-specific-data-from-linux-core-dump


#### Issues with (very) old Linux, gdb or WX2 versions

*  You need to export LD_ASSUME_KERNEL to the same as that used by the WX2 
   processes.  This is typically 2.4.1 (for SuSE) or 2.2.5.  You can check by 
   typing:
      ps eww -p &lt;pid&gt; | grep -o LD_ASSUME_KERNEL=[0-9.]*

*  Some versions of libc and gdb in some distributions have proved buggy.  In 
   particular: 
     1.  There is a bug in SuSE 9.3 where gdb doesn't see threads.  Novell 
         has a fix.

*  It appears that gdb can't understand the newer glibc system calls on SuSE
   without debugging information in glibc, and if that is not available it
   will give a stack like this.  Red Hat seems okay.
```   
     (gdb) bt
     #0 0xffffe410 in ?? ()
     #1 0xbffff068 in ?? ()
     #2 0x40132bd0 in __elf_set__libc_thread_... () from /lib/tls/libc.so.6
     ...
     #4 0x400a8983 in sleep () from /lib/tls/libc.so.6
```
   This seems to be because it can't backtrace through far calls without the
   debug info.  Running WX with a debug libc in /usr/lib/debug, and setting 
   LD_LIBRARY_PATH appropriately (shouldn't be necessary) before starting up
   should give a more readable backtrace.

*  If all the threads still look horrible, they might be in a kernel call 
   function with odd return stack behaviour; doing ni (next instruction) a few
   times might make the backtrace readable.  
   Note that wxdgdump should fix this to make things readable but it might fail
   if it doesn't recognise the calling convention.

*  Added config option [general] assume_kernel=...

*  GDB 6.3 (at least) has an annoying bug where it crashes your program when 
   it creates a new thread.  6.4 fixes this.

*  For some reason the gwcspbd might die when you attach to wcsdb - you can see
   this with "wxdes info daemons" and restart with "wxdes daemon all restart".
   (This does not apply to versions 5.4/6.0.)

</t>
<t tx="jonathanhudson.20201218084806.1">@language md

#### Record Types

#### Realtime

case JUT:			Baccara Total
case JUT:			Betting Summary
case JUT:			Bingo Total
case JUD:			Game Register Details
case JUT:			Game Register Total
case JUT:			Poker Cash Game
case JUT:			Poker Tournament
case JUT:			Session Total
case JUT:			Virtual Betting Summary

#### Periodics

case SIPLAFT:		AML Transaction Details
case ORT:			Network Operator Account Total
case OPT:			Operator Account Total
case PEND:		Pending Bet Details
case CJD:			Player Account Details
case LIQ:			Player Account Settlement
case RUD:			Player Registration Details
case RUT:			Player Registration Total
case LEX:			Self Exclusion Details
</t>
<t tx="jonathanhudson.20210115085519.1">@language md

#### Notes

Check the number of links to a file usually there will be 1 which is the directory node

```
find /data/bricks/dk-brick/.glusterfs -links 1 -type f

getfattr -d -e hex -m . &lt;filepath-on-brick&gt;
```

Stating files can sometimes help jog a heal

```
find /mnt/SAFE_DATA_XX -exec stat {} \;
```</t>
<t tx="jonathanhudson.20210204082559.1">@language md

#### Commands

Show control characters

```
:set list
```

Hide control character

```
:set nolist
```
</t>
<t tx="jonathanhudson.20210301114224.1">@language md


To update, extract the ansible-2.9.3.tar.gz archive to a suitable location, cd into the folder and then:

For windows ( in Cygwin ):
•	python –m setup build
•	python –m setup install
•	cp build/scripts-2.7/ansible* /bin/

For centos ( as root ):
•	python –m install

Verify the install with ansible --version , it should return something like:

 

The windows version install doesn’t copy the binaries properly, which is why we need to copy them manually. The install works just fine by itself on centos.

I’ve copied ansible-2.9.3.tar.gz to \\aragorn\Teams\Regulatory Platform Services\Junior Regulatory Platform Services Developer\Tools\Ansible 

</t>
<t tx="jonathanhudson.20210301145653.1">@language md

```
git config --global diff.tool bc3
git config --global difftool.bc3.trustExitCode true
```</t>
<t tx="jonathanhudson.20210301153511.1">@language md

```
git config --global user.name "Jonathan Hudson"
git config --global user.email jonathan.hudson@bet365.com

git config --global credential.helper "cache --timeout=3600"
git config --global core.autocrlf input
```</t>
<t tx="jonathanhudson.20210302081726.1">@language md

#### Shrink the title bars

Place in file called

~/.config/gtk-3.0/gtk.css

```
/* shrink headerbars (don't forget semicolons after each property) */
headerbar {
    min-height: 0px;
    padding-left: 2px; /* same as childrens vertical margins for nicer proportions */
    padding-right: 2px;
    background-color: #2d2d2d;
}

headerbar entry,
headerbar spinbutton,
headerbar button,
headerbar separator {
    margin: 0px; /* same as headerbar side padding for nicer proportions */
}

/* shrink ssd titlebars */
.default-decoration {
    min-height: 0; /* let the entry and button drive the titlebar size */
    padding: 0px;
    background-color: #2d2d2d;
}

.default-decoration .titlebutton {
    min-height: 0px; /* tweak these two props to reduce button size */
    min-width: 0px;
}

window.ssd headerbar.titlebar {
    padding-top: 3px;
    padding-bottom: 3px;
    padding-right: 6px;
    padding-left: 6px;
    min-height: 0;
}
```</t>
<t tx="jonathanhudson.20210303153926.1">@language md

#### Notes

Issue running smoke test had to set remote host bind address in `setenv.sh`

Error
```
BUILD FAILED
/data/home/d3s-dk/d3s/lib/admin/flush.xml:67: java.rmi.ConnectException: Connection refused to host: 10.48.94.28; nested exception is:
```
Fixed by
```
-Dcom.sun.management.jmxremote.host=0.0.0.0
```
</t>
<t tx="jonathanhudson.20210305132703.1">@language md
# Install mkdocs

Installation for Python on CentOS 7

## Python

Check python version

    python3 --version
    Python 3.6.8

## mkdocs

Install **mkdocs** and dependencies.

    pip3 install Click-7.0-py2.py3-none-any.whl
    pip3 install six-1.12.0-py2.py3-none-any.whl
    pip3 install tornado-6.0.3.tar.gz
    pip3 install livereload-2.6.1-py2.py3-none-any.whl
    pip3 install PyYAML-5.1.1.tar.gz
    pip3 install setuptools-41.0.1-py2.py3-none-any.whl
    pip3 install Markdown-3.1.1-py2.py3-none-any.whl
    pip3 install MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl
    pip3 install Jinja2-2.10.1-py2.py3-none-any.whl
    pip3 install mkdocs-1.0.4-py2.py3-none-any.whl

## Verify installation

    mkdocs --help
    Usage: mkdocs [OPTIONS] COMMAND [ARGS]...
    
      MkDocs - Project documentation with Markdown.
    
    Options:
      -V, --version  Show the version and exit.
      -q, --quiet    Silence warnings
      -v, --verbose  Enable verbose output
      -h, --help     Show this message and exit.
    
    Commands:
      build      Build the MkDocs documentation
      gh-deploy  Deploy your documentation to GitHub Pages
      new        Create a new MkDocs project
      serve      Run the builtin development server</t>
<t tx="jonathanhudson.20210308150442.1">@language md

LCL : ksl6rxCgf39z</t>
<t tx="jonathanhudson.20210309121030.1"></t>
<t tx="jonathanhudson.20210309121037.1">@language md

# NL Signature Validator

This tool can be used to validate the signature and timestamp of batches produced by the Dutch SAFE and also to extract the encrypted content.

## Configuration

* Edit or make a copy of the `BatchValidator.properties` configuration file.
* Update the configuration parameters as necessary:

### Configuration Parameters

safe.tenant.root=/mnt/SAFE_DATA/d3s-nl/storage/Dictao1/DICTAO1
extraction.root=./extracted

encryption.keystore.type=PKCS12
encryption.keystore.filename=keystores/dev-d3s-client-test-1.p12
encryption.keystore.password=wew2zBpi

signer.truststore.type=JKS
signer.truststore.filename=truststores/dev-client-truststore.jks
signer.truststore.password=password


## Execution

The tool takes two parameters:

* The name of the configuration file.
* The name of the batch file to validate.  This should include the path under the `safe.tenant.root` directory.

e.g.

    java -jar nl-signature-validator-&lt;version&gt;.jar \
      -config BatchValidator.properties \
      -batchFilename /WOK/Dictao1/1/2021/03/08/Dictao1-1-0000000001-20210308152500.zip

The batch contents will be extracted to a UUID directory under the `extraction.root` directory and checks performed on the signature and timestamp.
</t>
<t tx="jonathanhudson.20210310150844.1">@language md

#### Notes

To enable rsyslog to listen on port 514 you need to edit the `/etc/rsyslog.conf` file and uncomment the following lines

```
# Provides UDP syslog reception
$ModLoad imudp
$UDPServerRun 514

# Provides TCP syslog reception
$ModLoad imtcp
$InputTCPServerRun 514
```

then perform 

```
service rsyslog restart
```

then you can see the port

```
netstat -nap | grep 514
```
</t>
<t tx="jonathanhudson.20210315095504.1">@language md

```
find . -type f -name "*.txt" -print0 | xargs -0 sed -i '' -e 's/foo/bar/g'

find . -type f -name "*.pom" -print0 | xargs -0 sed -i '' -e 's/safe-nl/jono-nl/g'
```

with `gnu` find

```
find -name 'pom.xml' -exec sed -i 's/safe-nl/jono-nl/g' {} +
```
</t>
<t tx="jonathanhudson.20210323152114.1">@language md

In order for recon tests to work the ntp time must be synced with UATY

```
vi /etc/ntp.conf
```
set server to **10.250.0.123** which is used by UAT
```
service ntpd stop
ntpdate 10.250.0.123
service ntpd start
```</t>
<t tx="jonathanhudson.20210401090111.1">@language md

### Kognitio PMA overview

#### PMAs

The PMA (which stands for Persistent Memory Area) is a persistent data store which Kognitio's ramstores use to store database data, metadata and transaction information.  Persistent in this case refers to persistence of the data when the Kognitio processes are stopped and subsequently restarted.  Since the data is only stored in RAM it is not truly persistent, restarting a node, for example, will erase the PMA data on that node.
PMA data is stored using Linux's Posix shared memory feature.  PMA data is stored in shared memory 'files' which reside in /dev/shm.  /dev/shm is a mounted tmpfs filesystem, which means that the files created in that area reside only in memory (Linux implements this by making them cached files without a backing store to write the cache back to).  Kognitio can use up to 85% of system memory for PMAs, which requires a special option to be given in /etc/fstab for the /dev/shm mount to allow it to use that much memory.  For normal 'standalone' Kognitio installs running as root Kognitio changes this automatically.  For other types of system, users might have to configure things like this:

https://kognitio.com/documentation/latest/install/resources/install-devshm.html

#### PMA creation and reuse

Kognitio uses one PMA file for each ramstore which is in use.  When booting for the first time, a ramstore will create the PMA it uses.  On subsequent restarts the system configuration code (in the desconf library) will detect the existing PMAs and attempt to reuse them if it finds a valid set.  If not, any existing PMAs will be deleted and a new set will be created when the ramstores run 'create image'.

Kognitio's startup command has syntax to allow different image re-use behaviours:

```
wxserver start    -- allow re-use of memory images.  Rebuild if not able to re-use.
wxserver start without recovery -- force a rebuild of memory images
wxserver start with recovery -- force re-use of existing images.  Fail to start if a valid set not found
wxserver start recover or sysimage -- try to re-use existing images.  
```

If this fails, don't rebuild any images but run a 'create system image' instead.  This mode is often used in automated startup scripts which want to re-image in a specific order if images can't be re-used.

Kognitio supports multiple named sets of ramstores.  By default there is a sys ramstore set and a core ramstore set, but other sets can be configured and used.  Each ramstore set creates its own group of PMAs.  In order to validly re-use images, *all* sets must be considered valid.  SO, for example, we cannot re-use the sys ramstore set if a PMA member for the core ramstore set is missing.  This is something we were planning to improve over time.

#### PMA structure

The PMA is presented to the ramstore as a fixed sized array of fixed sized blocks which can be allocated for different purposes and chained together to store larger quantities of data.  In version 8 PMA blocks are 32K by default and the PMA has a maximum size of about 3.5G.  In version 9 PMA blocks are 64K and the size of a PMA is more or less unlimted providing the server is running in 64 bit mode.
Each block is allocated to a given subsystem, which is referred to as the 'user' of the block.  Subsystems can include PMA-internal usages like structure list storage (which holds copies of critical ramstore data structures, see below) and the 'free block' subsystem (which hands out unused blocks to new users) or it could be one of the ramstore specific subsystems like table storage.  See 'PMA users' below.

Each PMA file consists of the following structures:

- A header.  This is at the start of the file and contains control data for the PMA which is updated during operation.  Data includes the PMA block size, number of blocks, a signature to identify which system and ramstore set the PMA is part of and 'freelist' data to identify unused blocks.
- A blockmap.  This comes after the header and consists of an array of blockmap structures, one for each PMA data block.  The blockmap structure contains the count of readers and writers which have the block open (see 'accessing blocks'), the address the block is mapped into, the 'user' of the block and a subsystem specific tag (could be a table ID, transaction number, etc).
- The PMA blocks themselves.  These are stored one after another in the file, but they do not need to be mapped into memory next to one another.  

#### PMA library functions
You can find the source code definitions for the PMA data structures in include/pma.h .  Manipulation of the PMA is done using the libpma library, the source code for which can be found in src/libs/pma.  
PMA functions are defined in pma.h and work on multiple levels -- the pma_ functions operate against a PMA block (pma_open, pma_create, etc).  The pmb_ functions operate on PMA blocks once a pma has been opened, so pmb_open, pmb_close, etc to open and close blocks.
The PMA library also contains a third level of functions to manipulate PMA structure lists.  See 'Strucure lists' below.  The code for this subsystem is in the pmasl.c file and function prototypes are also in the pma.h header.

#### Accessing PMA data

To access a PMA inside a ramstore, the ramstore first has to open it.  pma_open is called to return a pma* value which is held in rsg-&gt;core_pma.  There is also code which talks about a 'replicated PMA' but this was never implemented -- the ramstore only has a core PMA.
Once a PMA is open, the ramstore is able to allocate blocks and open them using the pmb_open call.  The pmb_close call is used to indicate that the ramstore is finished with the block.  When opening the block, the caller needs to indicate if the block is being opened for writing or just read-only, indicating the same when closing the block.  There is currently no distinction between read-only and write opens beyond counting, the distinction was intended for use in the future, but the ramstore should correctly set these flags for table block opens.  Future iterations were planned which would have used read-only memory protection for read-only ops to prevent accidental memory overwrites, for example, and could use the write flag to indicate a need to write back to backing storage on close.
A block has a particular address it is mapped into within the ramstore process, but the ramstore data structures are designed in such a way that it should not matter where in memory a particular block is mapped, or even that it is always mapped into the same location.  Currently all blocks are mapped into the ramstore on startup, but the location of a block will vary between startups, so everything has been tested with potentially moving blocks.
There is, however, no need for un-opened blocks to be in memory at all and it was intended that future iterations of the software would not map all blocks into memory all of the time.  All that is required is that once a block is opened it gets mapped into an address and stays at that address until it gets closed again.  Once closed the block could be unmapped.  Currently only some usage subsystems are able to work like this and we only ever ran working prototype systems which unmapped table data blocks.

####  PMA atomic operations and usage flags

The PMA has various header fields which are used to determine if a set of PMA files is valid.  These include:
- the number of PMAs in the set
- the logical number of this PMA
- the PMA structure version (used for compatibility when upgrading, etc).
- the PMA *data* version -- this is different from the structure version.  The structure version talks about the overall PMA structures (blockmap, etc) while the data version is for the data inside those blocks (so, for example, the structure of a transaction list within a block allocated to the transaction subsystem.
- the PMA 'atomic' flag -- this is set when an atomic operation is being performed within the ramstore and cleared when it finishes.  Atomic operations are things like re-linking lists, creating new tables which require multiple data structures to be created, etc.  If a restart detects the atomic flag is set then it cannot re-use the PMA.  This is an area we would have worked on in the future to gradually remove the uses of this flag, replacing them with journalled operations or something similar, so that we reduce the risk of a failure causing an invalid PMA.
- the 'valid' flag.  This is set when a valid set of data has been built inside the PMA.  If this is not set that indicates a restart after a failure while recovering images or running a create image.  This stops the PMA from being used.  Generally when beginning a recovery or create image the flag will be cleared and a specific s-code instruction in the query plan for that operation will be used at the end to tell all ramstores to set the flag.

#### PMA structure lists

The final capability of the PMA is structure lists.  Structure lists are used to provide a sort of lightweight table storage within the PMA which does not rely on the standard database table storage.  Structure lists are lists of a specific single struct with a particular purpose.  Structures are stored in PMA blocks allocated to the structlist subsystem and each structure in a list has a unique identifier, which is the block number and offset within that block of the structure.
PMA structure lists are used to store away critical information which the recovery process can use after a restart to re-attach to the PMA.  The PMA provides the infrastructure for these, but the ramstore defines which lists are in use and what goes in them.  Structure lists in use by the ramstore include:

- the table list -- one struct per table.  Contains critical table data like create tno, update tno, owner, number of records, etc.
- the partition list -- one struct per partition in a table.  Used to store the first block of the partition's data block chain and various other useful things
- The transaction list -- one per currently open transaction

Structure lists are generally not the operational data sets for the information they store.  Typically the ramstore keeps operational information somewhere else and updates the structlist structure when that information changes.

#### PMA user subsystems

There are a number of different subsystems which can use the PMA.  These include:

- the structure list index -- contains a list of the structure lists in use and the first block in the structure block chain for each list
- the structure list subsystem -- contains structures for one structure list
- the free block subsystem -- contains unused blocks in a chain.  Top and bottom of the chain are in the header block.
- ramstore table data subsystem -- contains ramstore table data -- a  rs_tablechunk structure followed by actual data.
- ramstore record list subsystem -- contains lists of records for a given ramstore record list.  Record lists are used to implement things like sampling and scans on replicated data
- transaction subsystem -- stores lists of transaction log entries for transactions.  Used in commit and rollback.
- ramstore tempblock subsystem -- because the PMA uses around 85% of the ramstore's memory, it is nescessary to be able to 'steal' some of this memory and use it for temporary data structures 

some of the time.  The ramstore configures an 'alternative allocator' in the MM to allow certain MM_GetMem (and all related) calls to be intercepted and possibly transformed into tempblock allocations inside the PMA.
There are other subsystems as well, see pma.h for a full list of these.

#### Future intentions

In future iterations of the product, it was intended that the PMA would slowly evolve into a fully tiered storage architecture where data is stored on a network storage subsystem (perhaps S3, HDFS or similar or perhaps a custom one derived from Kognitio's diskstore) with a local SSD disk cache and memory on top of that (this is how hive LLAP, for example, works).  This would eliminate the need to have a diskstore at all as data could always be fetched back into memory when needed.  Tables would have flags to indicate whether their blocks could be unmapped from memory, etc.

The ramstore would be able to map different physical storage blocks into the pma memory block 'slots' and then map these into memory as required.  Since the PMA is aware of block chaining it would be able to do things like keeping the first few blocks of a chain in memory all the time and then doing readahead to get the other blocks in the chain when the first ones are accessed (this is how ParAccel/RedShift works).  
Another benefit of this architecture would be to get backing store based partitioning of data for free -- something we found hard to implement in the existing diskstore.

Finally we were planning to evolve tables to have 'vertical' as well as 'horizontal' partitioning which would allow the option of having columnar tables.  These would scan in parallel the columns a query required without scanning those it didn't.  For RAM-backed tables this makes little difference, and even slows down scanning, but for tables which can be flushed out of RAM and demand loaded it would speed up table scanning considerably in some cases.</t>
<t tx="jonathanhudson.20210401090719.1">@language md

### Adjust limits of shared memory

Adjust the limits on the /dev/shm filesystem

You need to do this because Kognitio is an in-memory system.

The /dev/shm filesystem (symlinked to /run/shm on some Linux distributions) is a tmpfs filesystem used to hold shared memory objects. Kognitio uses these objects to hold memory images. Typically this filesystem is mounted with a limit of 50% of system memory, which is not enough to run a Kognitio cluster in a container that uses most of a node’s memory. You will typically need to remount /dev/shm with the option ‘size=90%’ to allow up to 90% of system memory to be used for shared memory objects. For a running node you can do this with:

```
mount /dev/shm -o remount,size=90%
```

This will take effect immediately but will not persist after a reboot. To persist the change after a reboot, make this change in /etc/fstab, for example:

```
tmpfs                   /dev/shm                tmpfs   size=90%        0 0
```


</t>
<t tx="jonathanhudson.20210420100717.1">@language md

List of software we use

Software preinstalled by Service Desk
•	Google Chrome
•	Microsoft Office
•	Notepad++
•	7zip (7z1805-x64)
•	SoapUI-5.4.0 for Windows
•	Adobe Reader
•	Beyond Compare
•	Filezilla client
•	Python
•	TortoiseGit
•	Putty
•	Visio
•	Skype for Business
•	PrivateArk
•	XMLSpy Pro (Pro licence)
•	MarkdownPad Pro (the licence has to be inserted as the relevant user)


Software installed by us
•	Notepad++ (+ Xml Tools 2.4.9.2 x86 Unicode plugin)
•	Git
•	IntelliJ (Community)
•	OpenJDK
•	mRemoteNG (mRemoteNG-Installer-1.75.7000.19194)
•	Cygwin

Optional
•	gpg4win-3.0.0
•	postgresql-9.6.1-1-windows-x64-binaries

</t>
<t tx="jonathanhudson.20210422110307.1">@language md

#### Notes

Add jiras cert as trusted

```
openssl s_client -connect jira:443
```
Extract the server certificate

```
-----BEGIN CERTIFICATE-----
MIIFUTCCAzmgAwIBAgIIDQqcSsmanCMwDQYJKoZIhvcNAQELBQAwgcQxCzAJBgNV
BAYTAkdCMRYwFAYDVQQIEw1TdGFmZm9yZHNoaXJlMRcwFQYDVQQHEw5TdG9rZS1v
bi1UcmVudDEfMB0GA1UEChMWSGlsbHNpZGUgTmV3IE1lZGlhIEx0ZDEWMBQGA1UE
CxMNSVQgT3BlcmF0aW9uczEXMBUGA1UEAxMOSW5mcmFzdHJ1Y3R1cmUxMjAwBgkq
hkiG9w0BCQEWI2luZnJhc3RydWN0dXJlQGhpbGxzaWRlbmV3bWVkaWEuY29tMB4X
DTE3MDQyNDE0MDkwN1oXDTQwMDkyMTE0MDkwN1owdDELMAkGA1UEBhMCR0IxFjAU
BgNVBAgTDVN0YWZmb3Jkc2hpcmUxFzAVBgNVBAcTDlN0b2tlLU9uLVRyZW50MSUw
IwYDVQQKExxIaWxsc2lkZSAoTmV3IE1lZGlhKSBMaW1pdGVkMQ0wCwYDVQQDEwRq
aXJhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwOf7sb/WVv5k/D5d
UFhD9mNUNlcBkv4Xy/rFep7q+708FZe3tH2NDzS4GAl9rScOfY2rHoSxaYr8knor
mppvx1xvwJE4Q1nAF+RPtcVkfAuWVJVLCVcLOFv7by/h2dsmBxBrdraaDeCf2LvX
1YkL8uKhW3cCvyoqexDD1amGlrPlBVzVl0TLeUpe0XftXixWMPyi4hdyaC7YwjAA
w7hq9IVZgCnnDxRhlgWU9iiUdaHw+VFgcCneMLnAY3+Hv5WDUP8oUqJM8gSkzyDX
No/s2eF82FJzGQZwtEFiNWn2g+uUGrdSf8szwe5DCQ81H9ctZ2B+W2JqAcpEbsfS
dCCcPwIDAQABo4GVMIGSMB0GA1UdDgQWBBSgpctjlLlTgZJZmbeQ0f6QuR4mQzAf
BgNVHSMEGDAWgBTW/sfDME6wOTHHBBISdg5bvgRFdTAJBgNVHRMEAjAAMAsGA1Ud
DwQEAwIE8DAnBgNVHSUEIDAeBggrBgEFBQcDAQYIKwYBBQUHAwIGCCsGAQUFBwMD
MA8GA1UdEQQIMAaCBGppcmEwDQYJKoZIhvcNAQELBQADggIBAK2z4AlUYjy5Op/+
4E70ZhHXkkPDC0HH1b3AE0PDkrzjyD2875Y6wQ6Q5tdA1Ks+MKjAog14u2vjo4uE
TPJd1Qd2rZPOOpfBqUYZfEEqJ7BS4pVx3v4WEGTcwNHmrGXiQfjwAju8hHP8/1y/
GXaONQVcAT3zMUojBwuv1ZUk9Qg69IYP5OMBhLn+XeBEp8LNCUKzQsFf2ON17VTC
uXUwhLd0p8P3wsCUn7T4MM5tFA7FgcwrVdX/HCEIP7JUD49+iP60HKiOFA/3M7/E
CXOTGSUyQM+U42xZSV0vLemQV0Zw7Tb0/t1CO1KgpnQzI8byr98VDairr/1PmJTa
3UCFphPhgyuEoBiyXCMgJdv+G0CmNxoaA6ymGMCTFBFhm0anmuY0qWm5SEIR7j15
B465BfZSts6UGaOKbUTqUw5eX8Hxje5LjNWUKtwW5HdP6H4hn9AgbjTYXz0EXCLH
atKN20haHl+CfqCjryeqy2j3GZC5hKd+xLWLupDekMT3CTTNPqKz0TLG0s1JDMTd
Ukti4R5CoPChSTwfhoTgpTDSfcY3qH2QaaLBaNVSQDq/9we7PYXHkhJgSDzTDbno
00e0xHjcSJgQVcUrQyanceD1J19xx10YKGe/6X7OHajOJeWRWVJN3WoXKpFpomJu
oeDH8J7oqR0vvR1Q6OV6EWCdWBVX
-----END CERTIFICATE-----
```
Place in file 
```
jira.crt
```

From root
```
/usr/lib/jvm/jdk8u275-b01/jre/lib/security

/usr/lib/jvm/jdk8u275-b01/bin/keytool -importcert -keystore cacerts -file jira.crt -alias jira
```

</t>
<t tx="jonathanhudson.20210422134336.1">@language md

Truncate all jar files to 1 byte

```
find . -name "*.jar" -exec truncate -s1 {} \;
```
</t>
<t tx="jonathanhudson.20210426141656.1">@language md

#### Notes

To detach a repo from it's origin you can

```
git remote -v
```

Check it has an origin and then

```
git remote remove origin

git remote -v
```

</t>
<t tx="jonathanhudson.20210518103750.1">safe.tenant.root=/mnt/SAFE_DATA/d3s-nl/storage/Dictao1
extraction.root=./extracted

encryption.keystore.type=PKCS12
encryption.keystore.filename=keystores/dev-d3s-client-test-1.p12
encryption.keystore.password=wew2zBpi

signer.truststore.type=JKS
signer.truststore.filename=truststores/dev-client-truststore.jks
signer.truststore.password=password
</t>
<t tx="jonathanhudson.20210609084921.1">@language md

#### Notes

To disable the wsdl and status page use in web.xml

```
&lt;context-param&gt;
    &lt;param-name&gt;com.sun.xml.ws.server.http.publishStatusPage&lt;/param-name&gt;
    &lt;param-value&gt;false&lt;/param-value&gt;
&lt;/context-param&gt; 
```
</t>
<t tx="jonathanhudson.20210611122815.1">@language md

#### Notes

```
git checkout -b &lt;release-branch-check&gt; origin/&lt;release-branch&gt;
git merge --no-commit --no-ff &lt;feature-branch&gt;
git diff --cached
git merge --abort
```
</t>
<t tx="jonathanhudson.20210621085608.1">@language md

#### Notes

List tags by date

```
git log --tags --simplify-by-decoration --pretty="format:%ai %d" | grep '2021-02-24'
```

List tags

```
git log --decorate --all --oneline --graph

* 456c340 (HEAD, origin/tk/SAFALL-2100/feature-branch-1, origin/rl/TSTRPSB/aardvark, rl/TSTRPSB/aardvark) SAFALL-2100 Updated to use reverse proxy nexus
* 6591803 SAFALL-2100 Updated safe-app-config-version
* 125326c (tag: 1.0.1.0, origin/rl/TSTRPSB/widget, origin/rl/TSTRPSB/baboon, origin/master, origin/HEAD, master) SAFALL-2100 Updated to RELEASE
* 3e21784 SAFALL-2100 Updated to next SNAPSHOT
* 934a1d6 (tag: 1.0.0.0) SAFALL-2100 Updated to RELEASE
* 0234dee SAFALL-2100 Updated plugins
* 6bfe351 SAFALL-2100 Changed component prefix to test
* afe2e7f SAFALL-2100 Initial import
* c39957c Initial commit
```

Show commit for tag 1.0.1.0

```
git rev-list -n 1 1.0.1.0
125326c3955eac158ab42914ecb4abec73b4a500
```

Show commit for tag 1.0.0.0

```
git rev-list -n 1 1.0.0.0
934a1d6c57da84d869a83f7b048cf5e6f87b30d0
```
</t>
<t tx="jonathanhudson.20210621121255.1">@language md

#### Notes

Using dssc in dev

#### default.properties

In dss/dss/conf/default.properties

```
javax.net.ssl.keyStore=/data/home/dss/dss/certificates/tls/dev-dss-client.p12
javax.net.ssl.keyStorePassword=d6XI:?kURp
javax.net.ssl.keyStoreType=PKCS12

javax.net.ssl.trustStore=/data/home/dss/dss/certificates/tls/dev-client-truststore.jks
javax.net.ssl.trustStoreType=JKS
javax.net.ssl.trustStorePassword=password

dssc.cmd.global.url=https://localhost:38443/dss-signature/secure/v0/SignatureWebServiceImpl
```

#### Execute dssc

```
~/${symlink}/bin/dssc sign --tid "demo-policy-xades-basic" --reqid "requestid" --tag "tag" --dataToSign "${HOME}/${symlink}/test/sign-in.xml" --out "${HOME}/${symlink}/test/sign-out.xml" --insecure  --signType ENVELOPED -sysprop default
```

Result should be : 0

</t>
<t tx="jonathanhudson.20210712110051.1">@language md

Example searches safe data looking for zips that match the pattern then grepping them for OK

```
find /mnt/SAFE_DATA -name "*storage*.zip" -print0 | xargs -0 zgrep 'OK'
```

Look for hash and remove duplicates

```
find .  -name "*submission*.log" -print0 | xargs -0 egrep -io '\-1,.*==' | sort -u | wc -l
```

count-logs.sh

```
echo "Submission Log Count"
find .  -name "*submission*.log"
find .  -name "*submission*.log" -print0 | xargs -0 zgrep 'OK' | wc -l

echo "Storage Log Count"
find .  -name "safe-storage-log.2021-07-12.log.zip"
find .  -name "safe-storage-log.2021-07-12.log.zip" -print0 | xargs -0 zgrep 'OK' | wc -l
```

or counting hashes

```
echo "Submission Log Count"
echo "Files Found"
find .  -name "*submission*.log"
echo "Counting"
find .  -name "*submission*.log" -print0 | xargs -0 egrep -io '\-1,.*==' | sort -u | wc -l

echo "Storage Log Count"
echo "Files Found"
find .  -name "safe-storage-log.2021-07-12.log.zip"
echo "Counting"
find .  -name "safe-storage-log.2021-07-12.log.zip" -print0 | xargs -0 unzip -p | wc -l
```


</t>
<t tx="jonathanhudson.20210811090906.1"></t>
<t tx="jonathanhudson.20210811090910.1">@language md

#### Notes

### Prerequisites:

    open-vm-tools version is at 10.0.0 or greater
    OS supports fuse
    Kernel version &gt;= 3.10 (if open-vm-tools &lt; 10.3.0. kernel version must be &gt;= 4.0)
    Supports systemd

### Procedure:

    Remove any extra files from the /mnt/hgfs/ directory (if no other shared folders, the command rm -rf /mnt/hgfs/* works and worked for me)
    Create the file /etc/systemd/system/mnt-hgfs.mount with this content:

```
[Unit]
Description=VMware mount for hgfs
DefaultDependencies=no
Before=umount.target
ConditionVirtualization=vmware
After=sys-fs-fuse-connections.mount

[Mount]
What=vmhgfs-fuse
Where=/mnt/hgfs
Type=fuse
Options=default_permissions,allow_other

[Install]
WantedBy=multi-user.target
```

Add to or create the file `/etc/modules-load.d/open-vm-tools.conf` with this content:

`fuse`


Enable the systemd service with the command: 

```
sudo systemctl enable mnt-hgfs.mount
```

This will make sure the hgfs fdirectory will be mounted after a reboot.

Make sure the 'fuse' module is loaded: 

```
sudo modprobe -v fuse
```

In Workstation or Fusion, enable "Shared Folders" in "Virtual Machine Settings" &gt; "Options", and set the folders to be shared.
The shared folders should appear in the directory /mnt/hgfs. 

If that is not the case, start the service with: sudo systemctl start mnt-hgfs.mount or reboot.

The details/source information is here: https://kb.vmware.com/s/article/74650
</t>
<t tx="jonathanhudson.20210812104814.1">@language md

#### Sorting the storage log

```
sort -k1 -n -t, storage-logs-d3s-es-2021-08-12.log &gt; storage-logs-d3s-es-2021-08-12-sorted.log
```
</t>
<t tx="jonathanhudson.20210812104855.1">@language md

#### Sorting

Sorting a csv file by the first column

Column 1, numeric with ',' seperator

```
sort -k1 -n -t, storage-logs-d3s-es-2021-08-12.log &gt; storage-logs-d3s-es-2021-08-12-sorted.log
```

</t>
<t tx="jonathanhudson.20210812105245.1">@language md

#### Local in safe-scheduler

```
---
- hosts: linux
  vars:
    sourceFolder: '{{ sourceDirectory }}/safe-tasks/ancillary/safe-storage-log'
    configFolder: '{{ sourceDirectory }}/safe-tasks/ancillary/safe-storage-log/configs'
    tomcatFolder: '/usr/catalina/apache-tomcat-{{ tomcatVersion }}'
    webappsFolder: '~/safe-scheduler/catalina/webapps'
    safeTaskVersion: '1.0.1.0-SNAPSHOT'
    
    scheduledTasks:

      - user: 'safe-scheduler'
        artifactName: 'safe-storage-log-task'
        version: '{{ safeTaskVersion }}'
        sourceJarPath: '{{ sourceDrive }}{{ sourceDirectory }}/safe-tasks/ancillary/safe-storage-log/safe-storage-log-task/target'
        taskXmlFile: '{{ sourceDrive }}{{ configFolder }}/LCL/SafeStorageLog-task.xml'
        log4jXmlFile: '{{ sourceDrive }}{{ configFolder }}/LCL/SafeStorageLog-log4j.xml'
        target: '{{ myhost }}'

  roles:
    - deploy-safe-scheduler-task

  tasks:
  - file: path=/var/log/ansible state=touch
    become: yes
    become_user: root
    become_method: dzdo

  - name: log execution
    lineinfile: dest=/var/log/ansible line='{{ ansible_date_time.iso8601 }} deploy safe-log-tidy {{safeTaskVersion}}' insertafter='EOF'
    become: yes
    become_user: root
    become_method: dzdo
```</t>
<t tx="jonathanhudson.20210812105341.1"></t>
<t tx="jonathanhudson.20210812105356.1"></t>
<t tx="jonathanhudson.20210812105405.1">@language md

#### ES Local using safe-scheduler

```
---
- hosts: linux
  vars:
    webappsFolder: '~/safe-scheduler/catalina/webapps'
    safeTaskVersion: '1.1.0.2-RELEASE'

    scheduledTasks:

      - user: 'safe-scheduler'
        artifactName: 'anteriores-task'
        version: '{{ safeTaskVersion }}'
        sourceJarPath: '{{ safeTasksFolder }}/regulatory/anteriores/anteriores-task/target'
        taskXmlFile: '{{ configFolder }}/safe-scheduler/Anteriores-task.xml'
        log4jXmlFile: '{{ configFolder }}/safe-scheduler/Anteriores-log4j.xml'
        target: '{{ myhost }}'

  roles:
    - deploy-safe-scheduler-task

  tasks:
    - file: path=/var/log/ansible state=touch
      become: yes
      become_user: root
      become_method: dzdo

    - name: log execution
      lineinfile: dest=/var/log/ansible line='{{ ansible_date_time.iso8601 }} deploy anteriores task' insertafter='EOF'
      become: yes
      become_user: root
      become_method: dzdo
```
</t>
<t tx="jonathanhudson.20210816151808.1"></t>
<t tx="jonathanhudson.20210816152317.1">@language md

#### Commands

```
h[elp]      			Get help on gdb commands  
h[elp] &lt;cmd&gt;			Get help on a specific gdb command  
r[un]       			Run to next breakpoint or to end  
s[tep]      			Single-step, descending into functions  
n[ext]      			Single-step without descending into functions
fin[ish]	  			Finish current function, loop, etc. (useful!)
c[ontinue]  			Continue to next breakpoint or end
up          			Go up one context level on stack (to caller)
do[wn]      			Go down one level (only possible after up)
l[ist]          		Show lines of code surrounding the current point
p[rint] &lt;name&gt;  		Print value of variable called &lt;name&gt;
p *&lt;name&gt;				Print what is pointed to by &lt;name&gt;
p/x &lt;name&gt;			    Print value of &lt;name&gt; in hex format
p &lt;name&gt; @ &lt;n&gt;			print &lt;n&gt; values starting at &lt;name&gt;
p &lt;chars&gt; &lt;tab&gt;		    List all variables starting with &lt;chars&gt;
b[reak] &lt;name&gt;          Set a breakpoint at function &lt;name&gt;  
b &lt;class&gt;::&lt;name&gt;       Set a breakpoint at &lt;name&gt; in &lt;class&gt;  
b &lt;class&gt;::&lt;tab&gt;        List all members in &lt;class&gt;  
h[elp] b	            Documentation for setting breakpoints
i[nfo] b	            List breakpoints
i                       List all info commands
dis[able] 1             Disable breakpoint 1
en[able] 1              Enable breakpoint 1
d[elete] 1              Delete breakpoint 1
d 1 2                   Delete breakpoints 1 and 2
d	                    Delete all breakpoints
cond[ition] 1 &lt;expr&gt;    Stop at breakpoint 1 only if &lt;expr&gt; is true
cond 1                  Make breakpoint 1 unconditional
comm[ands] 1            Add a list of gdb commands to execute each time breakpoint 1 is hit (usually just print &lt;var&gt;)
```

</t>
<t tx="jonathanhudson.20210817154805.1">@language md

#### Notes

Stop the service as wxadmin first
```
wxserver stop
```

#### Compiling
Run build as kognitio user:
From: /data/home/kognitio/wx2/

```
make clean
make -r makefiles
UNSTRIP=1 STATIC=1 make –rj12 wx2
make –rj12 wx2
```

this builds the server package (wx2-90000rel200917.wxpkg) only. There is no need to rebuild makefiles since no new targes are added

then I deploy as wxadmin

From /opt/kognitio/wx2/current/bin

```
wxserver install /data/home/kognitio/wx2/out/packages/wx2-90000rel200917.wxpkg force
wxserver smd all restart force
wxserver start
```

to help debug the correct interpreter process

to limit the number of interpreters and compilers use wxviconf
```
[boot options]
external_tables=yes
external_scripts=yes
num_comp_nodes=2
num_int_nodes=2
topup_intnodes=0

[logs]
smd_log_level=100
```
then look for the Interpreter process
```
wxprobe -l | grep Interpret
```

#### Logging
(This assumes that you are not updating the build version in version.h, I don’t see a point since it’s just testing otherwise you’d need to do wxserver set current_version &lt;version&gt;  before restarting smd and server)

add debug code
```
cg_debug(cgws, CG_DBG_ERROR, "In cg_embed_string: %s\n", str);

char *dataline = make_string("In if, arg1type: [%d], arg2type: [%d]", arg1type, arg2type);
cg_debug(cgws, CG_DBG_ERROR, "%s\n", dataline);
xfree(dataline);
```
build with symbols
```
make clean
UNSTRIP=1 STATIC=1 make –rj12 wx2
```  

#### GDB Debug
```
lsof -i -P -n | grep *:6550
gdb -p 199440
set detach-on-fork off
set log on
break cgcodeop.c:154
break semexpr.c:4564
c
```
commands
```
info breakpoints ( i b )
break &lt;file&gt;:&lt;line&gt; &lt;condition&gt;
enable breakpoint 1
disable breakpoints
delete
fin
s
c
```
multiple breakpoints
```
gdb -p 202090
set schedule-multiple on
break intentry.c:703
clone-inferior
inferior 2
attach 191007
continue
```

#### Testing
```
wxsubmit -s 10.15.10.227 -u sys -p rliteG3863 -x "debug 12 select floor(32767+4.0) from CNTL.DW_DATA_SOURCE;"
```

#### SQL
```
select floor(32767+4.0) from CNTL.DW_DATA_SOURCE;
diagnose select floor(32767+4.0) from CNTL.DW_DATA_SOURCE;
debug 12 select floor(32767+4.0) from CNTL.DW_DATA_SOURCE; -&gt; cidebug log file 
```

#### Stepping through machine code
Possible ways to step through the machine code  

You can step through machine code, such as generated code, using "si" or "ni"  

Gdb doesn't automatically display the next instruction but you can

You can use the INT3 instruction in assembler/generated code to hard-code a breakpoint which a debugger will catch (but without a debugger it will 
crash).  
On i386, you can simply continue from there in the debugger. To do this using the code generator use "{I_INT3, NONE, NONE}," inside a CODE_BLOCK.

get it to do this using:

```
 define hook-stop
     x/i $eip
 end
```
</t>
<t tx="jonathanhudson.20210818130028.1"></t>
<t tx="jonathanhudson.20210818130036.1">@language md

#### Notes

Compile

```
make cialone
unstrip=1 make -r cialone

gdb --args compile -s 10.15.10.227 -u sys -p rliteG3863 -t "diagnose select floor(32768+4.0) from CNTL.DW_DATA_SOURCE" -d

./compile -s 10.15.10.227 -u sys -p rliteG3863 -t "diagnose select floor(32768+4.0) from CNTL.DW_DATA_SOURCE" -d
```


```
wxsubmit -s 10.15.10.227 -u sys -p rliteG3863 -x "debug 12 select floor(32767+4.0) from CNTL.DW_DATA_SOURCE;"
```
</t>
<t tx="jonathanhudson.20210819153018.1">@language md

#### Analysis

```
SCODE ANALYSIS

4 byte length - 4 byte cost - 4 byte instruction

757
91  0   0   0   0   0   0   0   144 0   	SC_DEP( Generate Diagnose )
0   0   0   0   0   0   0   0   0   0   
1   0 	0   0   255 255 255 255 154 2   
0   0   1   0   0   0   219 7   0   0   	Table ( 2011 ) = 219 7
0   0   0   0   0   0   0   0   0   0   
0   0   0   0   240 63  253 3   0   0   
19  0   0   67  78  84  76  46  68  87  	67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69	
95  68  65  84  65  95  83  79  85  82  	C  N  T  L  .  D  W  _  D  A  T  A  _  S  O  U  R  C  E
67  69  0   0   0   0   0   0   0   0   

56  0   0   0   0   0   0   0   2   0   	SC_DIAGNOSTIC ( Set the interpreter diagnostic flag )
0   0   0   0   0   0   0   0   0   0 
252 255 255 127 0   0   0   0   0   0 
0   0   0   0   0   0   0   0   0   0 
0   0   0   0   0   0   0   0   0   0 
0   0   0   0   0   0 


72  0   0   0   0   0   0   0   8   0 		SC_SQLTEXT ( Record the SQL text for this SCode ) 
0   0   0   0   0   0   0   0   0   0 
48  0   0   0   115 101 108 101 99  116     115 101 108 101 99 116 32 102 108 111 111 114 40 51 50 55 54 56 43 52 46 48 41 32 102 114 111 109 32 67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69 
32  102 108 111 111 114 40  51  50  55      s   e   l   e   c  t      f   l   o   o   r   (  3  2  7  6  8  +  4  .  0  )      f  r   o   m      C  N  T  L  .  D  W  _  D  A  T  A  _  S  O  U  R  C  E	
54  56  43  52  46  48  41  32  102 114 
111 109 32  67  78  84  76  46  68  87 
95  68  65  84  65  95  83  79  85  82 
67  69 

20  0   0   0   0   0   0   0   18  0 		SC_INITLOCKLIST ( Initialise (clear) the lock definition list ) 
0   0   45  67  28  235 226 54  26  63 

32  0   0   0   0   0   0   0   20  0 		SC_DEFINETABLELOCKS ( Add some table locks to the list (defered) ) 
0   0   241 104 227 136 181 248 4   63 
1   0   0   0   219 7   0   0   1   0 		Table ( 2011 ) = 219 7
0   0 

20  0   0   0   0   0   0   0   19  0 		SC_APPLYLOCKLIST ( Apply locks in the lock definition list ) 
0   0   97  50  85  48  42  169 51  63 

251 0   0   0   0   0   0   0   68  0 		SC_GENERICSTARTACCESS ( Start a single-table generic access )
0   0   109 116 244 22  20  224 133 63 		
1   0   0   219 7   0   0   0   0   0 		Table ( 2011 ) = 219 7
0   2   0   0   0   1   0   0   0   0 
0   0   0   0   0   0   0   0   0   0 
0   0   0   0   0   0   5   0   0   0 
72  0   0   0   0   0   0   0   0   0 		72 Record Length
0   0   0   1   0   0   0   0   0   0 
0   0   0   0   0   0   0   240 63  0 
0   0   0   255 255 255 255 1   0   0
0   0   0   0   0   255 255 255 255 255 
255 255 255 0   0   0   0   0   0   0 
0   255 255 255 255 0   0   253 3   0 
0   0   0   0   0   0   0   0   0   0 
0   0   0   0   0   0   0   0   0   0 		Reverse Polish ( 5 bytes )
0   5   0   0   0   230 0   0   0   8 		230 0   0   0   8   0   0   0   4   128 0   0   0   0   0   0   28
0   0   0   4   128 0   0   0   0   0 		230 8 32772 0 28	
0   28  0   0   0   255 255 255 255 1 		
0   0   0   3   0   1   0   0   0   0  
0   8   0   0   0   255 255 255 255 11		8 : SrcLen 		11 : Precn
0   0   0   0   0   0   0   4   0   0       4 : TgtStart
0   255 255 255 255 0   0   0   0   0       0 : Scale 
0   0   0   0   0   0   0   0   0   0 
0   0   0   0   0   0   0   0   0   0
0   0   0   0   0   0   0   0   0   0 
0 

57  0   0   0   15  0   0   0   65  100 	SC_SEARCHEDDEL ( Prequalified delete from RAM and then Disk )
100 32  99  111 108 117 109 110 32  115 
112 101 99  38  0   0   0   0   0   0  
0   0   0   0   0   0   0   1   0   8 
0   11  0   0   0   255 255 255 255 4 
0   0   0   8   0   0   0 

30  0   0   0   0   0   0   0   76  0 		SC_FETCHCHECKONEROW ( Fetch and check for a single row )
0   0   59  223 79 141  151 110 114 63 
0   0   0   0   0   0   82  128 67  73 

22  0   0   0   0   0   0   0   50  0 		SC_RETURNDATABUF ( Return a data buffer row to the client )
0   0   252 169 241 210 77  98  80  63 
0   0 

22  0   0   0   0   0   0   0   78  0 		SC_DISCARDACCESS ( Discard an access ) 
0   0   174 216 95  118 79  30  102 63 
0   0 

64  0   0   0   0   0   0   0   146 0 		SC_LOGPRIVILEGES ( Log the privileges used )
0   0   0   0   0   0   0   0   0   0 
10  0   0   0   4   0   0   0   8   0 
0   0   0   0   0   0   255 255 255 255 
0   0   0   0   219 7   0   0   249 255
255 255 6   0   0   0   255 255 255 255 
255 255 255 255 

20  0   0   0   0   0   0   0   3   0 		SC_END ( Indicate the end of a sequence of SCode )
0   0   0   0   0   0   0   0   0   0 

```</t>
<t tx="jonathanhudson.20210819153406.1">@languag md

#### Notes

```
757
91 0 0 0 0 0 0 0 144 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 255 255 255 255 154 2 0 0 1 0 0 0 219 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 240 63 253 3 0 0 19 0 0 0 67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69 0 0 0 0 0 0 0 0 56 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 252 255 255 127 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 72 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 48 0 0 0 115 101 108 101 99 116 32 102 108 111 111 114 40 51 50 55 54 56 43 52 46 48 41 32 102 114 111 109 32 67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69 20 0 0 0 0 0 0 0 18 0 0 0 45 67 28 235 226 54 26 63 32 0 0 0 0 0 0 0 20 0 0 0 241 104 227 136 181 248 4 63 1 0 0 0 219 7 0 0 1 0 0 0 20 0 0 0 0 0 0 0 19 0 0 0 97 50 85 48 42 169 51 63 251 0 0 0 0 0 0 0 68 0 0 0 109 116 244 22 20 224 133 63 1 0 0 219 7 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 72 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 240 63 0 0 0 0 255 255 255 255 1 0 0 0 0 0 0 0 255 255 255 255 255 255 255 255 0 0 0 0 0 0 0 0 255 255 255 255 0 0 253 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 230 0 0 0 8 0 0 0 4 128 0 0 0 0 0 0 28 0 0 0 255 255 255 255 1 0 0 0 3 0 1 0 0 0 0 0 8 0 0 0 255 255 255 255 11 0 0 0 0 0 0 0 4 0 0 0 255 255 255 255 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 57 0 0 0 15 0 0 0 65 100 100 32 99 111 108 117 109 110 32 115 112 101 99 38 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 8 0 11 0 0 0 255 255 255 255 4 0 0 0 8 0 0 0 30 0 0 0 0 0 0 0 76 0 0 0 59 223 79 141 151 110 114 63 0 0 0 0 0 0 82 128 67 73 22 0 0 0 0 0 0 0 50 0 0 0 252 169 241 210 77 98 80 63 0 0 22 0 0 0 0 0 0 0 78 0 0 0 174 216 95 118 79 30 102 63 0 0 64 0 0 0 0 0 0 0 146 0 0 0 0 0 0 0 0 0 0 0 10 0 0 0 4 0 0 0 8 0 0 0 0 0 0 0 255 255 255 255 0 0 0 0 219 7 0 0 249 255 255 255 6 0 0 0 255 255 255 255 255 255 255 255 20 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0

backtrace

#0  SemCheckNSFunction (pSem=0xffff8890, pFunc=0x85fbf30, pbNeedCast=0xffff84cf, bDoDecCast=false) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:13506
#1  0x0812bf8b in SemCheckExpr (pSem=0xffff8890, pExpr=0x85fbf24, pbNeedCast=0xffff84cf, bDoDecCast=false) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:15294
#2  0x0812c51f in SemCheckExprDecCast (pSem=0xffff8890, pExpr=0x85fbf24) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:15430
#3  0x081acdeb in SemCheckSelectList (pSem=0xffff8890, pSelectList=0x85fc01c, TabIdx=4, pppNames=0x85fc2b4, pnCols=0x85fc2b0) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:2219
#4  0x081b3584 in SemCheckQuerySpec (pSem=0xffff8890, pQuerySpec=0x85fc268, pppNames=0x85fc2b4, pnCols=0x85fc2b0, bIgnoreDistinct=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:5414
#5  0x081b5a2b in SemCheckQueryTerm (pSem=0xffff8890, pQueryTerm=0x85fc264, pppNames=0x85fc2b4, pnCols=0x85fc2b0, bIgnoreDistinct=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:6239
#6  0x081b8bae in SemCheckQueryExpr (pSem=0xffff8890, pQueryExpr=0x85fc25c, pppNames=0x85fc2b4, pnCols=0x85fc2b0, fDistinctAtHigherLevel=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:7562
#7  0x081b8ea6 in SemCheckOrdQuery (pSem=0xffff8890, pOrdQuery=0x85fc25c) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:7648
#8  0x080e9070 in SemSelect (pSem=0xffff8890, pSelect=0x85fc25c) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:6970
#9  0x080ec4ef in SemCheckRequest (pSem=0xffff8890, pPTree=0x8571ce4 &lt;gPTree&gt;) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:8667
#10 0x080ed005 in Semantics (pPTree=0x8571ce4 &lt;gPTree&gt;, pSymTab=0x8571cc8 &lt;gSymTab&gt;, pCompEnv=0xffffd0f8, SemType=SEMTYPE_NORMAL, OuterDepth=0) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:9040
#11 0x08052bc2 in CmpCompile (pSql=0xffffd818 "select floor(32768+4.0) from CNTL.DW_DATA_SOURCE", nSqlSize=48, pCompEnv=0xffffd0f8, DebugMode=true, ReplayFile=0x0, szSchema=0x0,ErrorContexts=0xffff9028, nErrorContexts=5) at /data/home/kognitio/wx2/src/wxdb/ci/cmpentry.c:1184
#12 0x08050047 in main (argc=10, argv=0xffffd674) at /data/home/kognitio/wx2/src/apps/compile/compile.c:535

semexpr.c

	Line 13506 - Start of the Floor / Ceiling function analysis
	
	
	
Reverse Polish

    =====================================================================================================================================
	
	Precision = 6 ( 999,999 )
	
	230 4 327710 387 134 0 1 229 0 -1 2 2 425 256 -999999 999999 28

	230 		Literal
	8			Size
	327710		327710.0
	387			NSFunc
	134			CG_FLOOR_DEC32
	0			Conv = False
	1			Case = True
	229			RP_UNARYCONV
	0			Cast = False
	-1			Conv = ( Diff from Src and Tgt Precision ) if not 0 then goes into SemAddUnaryConvRP (&amp;RP, Cast2, Conv2, Source, Target);
	2			Source Type ( 2 = CG_INT32, 3 = CG_INT64 )
	2			Target Type ( 2 = CG_INT32, 3 = CG_INT64 )
	426			RP_RANGE_INT32 ( adds a range check with min and max values )
	256			Error Code
	-999999		Min value byte 1
	999999   	Max value byte 1
	28			RP_END
	
	=====================================================================================================================================
	
	Precision = 11 ( 99,999,999,999 )
	
	230 8 327720 0 387 132 0 1 229 0 -1 3 3 426 256 -1215752191 -24 1215752191 23 28
	
	230 		Literal
	8			Size
	327720		32772.0
	387			NSFunc
	132			CG_FLOOR_DEC64
	0			Conv = False
	1			Case = True
	229			RP_UNARYCONV
	0			Cast = False
	-1			Conv = ( Diff from Src and Tgt Precision ) if not 0 then goes into SemAddUnaryConvRP (&amp;RP, Cast2, Conv2, Source, Target);
	3			Source Type ( 2 = CG_INT32, 3 = CG_INT64 )
	3			Target Type ( 2 = CG_INT32, 3 = CG_INT64 )
	426			RP_RANGE_INT64 ( adds a range check with min and max values )
	256			Error Code
	-1215752191	Min value byte 1
	-24			Min value byte 2
	1215752191	Max value byte 1
	23			Max value byte 2
	28			RP_END
```</t>
<t tx="jonathanhudson.20210823103451.1">@language md

#### vip up

#! /bin/sh
/sbin/ip addr add 10.15.11.91/24 dev eth0

#### vip down

#! /bin/sh
/sbin/ip addr del 10.15.11.91/24 dev eth0</t>
<t tx="jonathanhudson.20210824152921.1">@language md

```
error

Log files at /var/log/wx2/logs-rps_dev/smd.T_2021-08-13_14:04:10_BST

T_2021-08-16_16:11:18_BST: CG 0106: Argument size mismatch for instruction at /data/home/kognitio/wx2/src/wxdb/cg/cgcodeop.c:170.
T_2021-08-16_16:11:18_BST: CG 0106: Constant optimisation error. at /data/home/kognitio/wx2/src/wxdb/cg/cgrp.c:2735.

scode

757
91 0 0 0 0 0 0 0 144 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 255 255 255 255 154 2 0 0 1 0 0 0 219 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 240 63 253 3 0 0 19 0 0 0 67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69 0 0 0 0 0 0 0 0 56 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 252 255 255 127 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 72 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 48 0 0 0 115 101 108 101 99 116 32 102 108 111 111 114 40 51 50 55 54 56 43 52 46 48 41 32 102 114 111 109 32 67 78 84 76 46 68 87 95 68 65 84 65 95 83 79 85 82 67 69 20 0 0 0 0 0 0 0 18 0 0 0 45 67 28 235 226 54 26 63 32 0 0 0 0 0 0 0 20 0 0 0 241 104 227 136 181 248 4 63 1 0 0 0 219 7 0 0 1 0 0 0 20 0 0 0 0 0 0 0 19 0 0 0 97 50 85 48 42 169 51 63 251 0 0 0 0 0 0 0 68 0 0 0 109 116 244 22 20 224 133 63 1 0 0 219 7 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 72 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 240 63 0 0 0 0 255 255 255 255 1 0 0 0 0 0 0 0 255 255 255 255 255 255 255 255 0 0 0 0 0 0 0 0 255 255 255 255 0 0 253 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 230 0 0 0 8 0 0 0 4 128 0 0 0 0 0 0 28 0 0 0 255 255 255 255 1 0 0 0 3 0 1 0 0 0 0 0 8 0 0 0 255 255 255 255 11 0 0 0 0 0 0 0 4 0 0 0 255 255 255 255 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 57 0 0 0 15 0 0 0 65 100 100 32 99 111 108 117 109 110 32 115 112 101 99 38 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 8 0 11 0 0 0 255 255 255 255 4 0 0 0 8 0 0 0 30 0 0 0 0 0 0 0 76 0 0 0 59 223 79 141 151 110 114 63 0 0 0 0 0 0 82 128 67 73 22 0 0 0 0 0 0 0 50 0 0 0 252 169 241 210 77 98 80 63 0 0 22 0 0 0 0 0 0 0 78 0 0 0 174 216 95 118 79 30 102 63 0 0 64 0 0 0 0 0 0 0 146 0 0 0 0 0 0 0 0 0 0 0 10 0 0 0 4 0 0 0 8 0 0 0 0 0 0 0 255 255 255 255 0 0 0 0 219 7 0 0 249 255 255 255 6 0 0 0 255 255 255 255 255 255 255 255 20 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0

backtrace

#0  SemCheckNSFunction (pSem=0xffff8890, pFunc=0x85fbf30, pbNeedCast=0xffff84cf, bDoDecCast=false) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:13506
#1  0x0812bf8b in SemCheckExpr (pSem=0xffff8890, pExpr=0x85fbf24, pbNeedCast=0xffff84cf, bDoDecCast=false) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:15294
#2  0x0812c51f in SemCheckExprDecCast (pSem=0xffff8890, pExpr=0x85fbf24) at /data/home/kognitio/wx2/src/wxdb/ci/semexpr.c:15430
#3  0x081acdeb in SemCheckSelectList (pSem=0xffff8890, pSelectList=0x85fc01c, TabIdx=4, pppNames=0x85fc2b4, pnCols=0x85fc2b0) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:2219
#4  0x081b3584 in SemCheckQuerySpec (pSem=0xffff8890, pQuerySpec=0x85fc268, pppNames=0x85fc2b4, pnCols=0x85fc2b0, bIgnoreDistinct=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:5414
#5  0x081b5a2b in SemCheckQueryTerm (pSem=0xffff8890, pQueryTerm=0x85fc264, pppNames=0x85fc2b4, pnCols=0x85fc2b0, bIgnoreDistinct=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:6239
#6  0x081b8bae in SemCheckQueryExpr (pSem=0xffff8890, pQueryExpr=0x85fc25c, pppNames=0x85fc2b4, pnCols=0x85fc2b0, fDistinctAtHigherLevel=false) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:7562
#7  0x081b8ea6 in SemCheckOrdQuery (pSem=0xffff8890, pOrdQuery=0x85fc25c) at /data/home/kognitio/wx2/src/wxdb/ci/semsel.c:7648
#8  0x080e9070 in SemSelect (pSem=0xffff8890, pSelect=0x85fc25c) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:6970
#9  0x080ec4ef in SemCheckRequest (pSem=0xffff8890, pPTree=0x8571ce4 &lt;gPTree&gt;) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:8667
#10 0x080ed005 in Semantics (pPTree=0x8571ce4 &lt;gPTree&gt;, pSymTab=0x8571cc8 &lt;gSymTab&gt;, pCompEnv=0xffffd0f8, SemType=SEMTYPE_NORMAL, OuterDepth=0) at /data/home/kognitio/wx2/src/wxdb/ci/semmain.c:9040
#11 0x08052bc2 in CmpCompile (pSql=0xffffd818 "select floor(32768+4.0) from CNTL.DW_DATA_SOURCE", nSqlSize=48, pCompEnv=0xffffd0f8, DebugMode=true, ReplayFile=0x0, szSchema=0x0,ErrorContexts=0xffff9028, nErrorContexts=5) at /data/home/kognitio/wx2/src/wxdb/ci/cmpentry.c:1184
#12 0x08050047 in main (argc=10, argv=0xffffd674) at /data/home/kognitio/wx2/src/apps/compile/compile.c:535

semexpr.c

	Line 13506 - Start of the Floor / Ceiling function analysis
	
	
	
Reverse Polish

=====================================================================================================================================

Precision = 6 ( 999,999 )

230 4 327710 387 134 0 1 229 0 -1 2 2 425 256 -999999 999999 28

230 		Literal
4			Size
327710		327710.0
387			NSFunc
134			CG_FLOOR_DEC32
0			Cast = False
1			Conv = True
229			RP_UNARYCONV
0			Cast = False
-1			Conv = ( Diff from Src and Tgt Precision ) if not 0 then goes into SemAddUnaryConvRP (&amp;RP, Cast2, Conv2, Source, Target);
2			Source Type ( 2 = CG_INT32, 3 = CG_INT64 )
2			Target Type ( 2 = CG_INT32, 3 = CG_INT64 )
425			RP_RANGE_INT32 ( adds a range check with min and max values )
256			Error Code
-999999		Min value byte 1
999999   	Max value byte 1
28			RP_END

=====================================================================================================================================

Precision = 11 ( 99,999,999,999 )

230 8 327720 0 387 132 0 1 229 0 -1 3 3 426 256 -1215752191 -24 1215752191 23 28

230 		Literal
8			Size
327720		32772.0
387			NSFunc
132			CG_FLOOR_DEC64
0			Conv = False
1			Cast = True
229			RP_UNARYCONV
0			Cast = False
-1			Conv = ( Diff from Src and Tgt Precision ) if not 0 then goes into SemAddUnaryConvRP (&amp;RP, Cast2, Conv2, Source, Target);
3			Source Type ( 2 = CG_INT32, 3 = CG_INT64 )
3			Target Type ( 2 = CG_INT32, 3 = CG_INT64 )
426			RP_RANGE_INT64 ( adds a range check with min and max values )
256			Error Code
-1215752191	Min value byte 1
-24			Min value byte 2
1215752191	Max value byte 1
23			Max value byte 2
28			RP_END
```</t>
<t tx="jonathanhudson.20210824152956.1">@language md

</t>
<t tx="jonathanhudson.20210824153105.1">@language md

Adding debug to the code will apear in the server debug log

```
char *valline = make_string("arg1size: [%d], arg2size: [%d], sizeof1: [%lu], sizeof2: [%lu], arg1type: [%d], arg2type: [%d], arg1val: [%zd], arg2val: [%zd]", arg1size, arg2size, sizeof(arg1val), sizeof(arg2val), (arg1type &amp; T_32), (arg2type &amp; T_64), arg1val, arg2val);
void *btbuffer[10];
unsigned int btframes = backtrace(btbuffer, 10);
cg_assert(cgws, btframes &lt;= 10);
char *srcline = make_string("CG BUG 45645: Argument size mismatch for instruction: addr2line -f -e /proc/%d/exe ", getpid());
for (int i = 0; i &lt; btframes; i ++)
{
	srcline = join_strings( srcline, make_string( " %p", btbuffer[i] ) );
}
cg_debug(cgws, CG_DBG_ERROR, "%s\n", srcline);
cg_debug(cgws, CG_DBG_ERROR, "%s\n", valline);

xfree(srcline);
xfree(valline);
```</t>
<t tx="jonathanhudson.20210906095043.1">@language md

#### Notes

When using laptop with a VPN connection to the office the RMI server reports back the hostname of the first ethernet adapter it finds which is incorrect and causes a connection fail with permission denied

To fix this you need to set the following in `setenv.sh`

```
# Use this when testing locally from Windows with multiple ipv4 addresses
CATALINA_OPTS="$CATALINA_OPTS -Djava.rmi.server.hostname=localhost"
```

This setting will then force the JMX connection to try and connect on localhost rather than the first ipv4 address which is typically 192.168.0.nn and results in a permission denied
</t>
<t tx="jonathanhudson.20210907100423.1">@language md

Hi,

Please can you update the dzdo config to enable the new linux user (sevice account) ‘d3s-desh’ to execute /bin/sh

This is so that we can run ansible jobs from the ‘d3s-desh’ user.

dzdo -l | grep /bin/sh

The above command will show the current user list for /bin/sh and it’s this list that we need ‘d3s-desh’ adding to 

This will be required on DEV/UAT along with our developments machines ( ob1-001122 and ob1-003059 are examples )

Any questions please let me know.

Many thanks
 
Jonathan
</t>
<t tx="jonathanhudson.20210916082444.1">@language md

#### Using awk to extract hashes from submission and storage log files

@language md

#### Submission Log Hash Extract

awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-14.log

awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-*.log | sort &gt; submission-hashes.txt

awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-*.log | sort | uniq -u &gt; submission-hashes-unique.txt


#### Store Log Hash Extract

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-14.log

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort &gt; store-hashes.txt

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -u &gt; store-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -D &gt; store-hash-duplicates.txt
</t>
<t tx="jonathanhudson.20210916082534.1">@language md

#### Commands

in /mnt/SAFE_DATA

### SUBMISSIONS LOGS

find . -path '*logs/submit*'

#### Submission Log Hash Extract

awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-14.log
awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-*.log | sort &gt; submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' d3s-submission-es.vm-ap6.2021-09-*.log | sort | uniq -u &gt; submission-hashes-unique.txt

### STORAGE LOGS

find . -path '*logs/store*'

unzip -d d3s-bg/logs/store/  d3s-bg/logs/store/safe-storage-log.d3s-bg.2021-09-29.log.zip
unzip -d d3s-dk/logs/store/  d3s-dk/logs/store/safe-storage-log.d3s-dk.2021-09-29.log.zip
unzip -d d3s-gr/logs/store/  d3s-gr/logs/store/safe-storage-log.d3s-gr.2021-09-29.log.zip
unzip -d d3s-gi/logs/store/  d3s-gi/logs/store/safe-storage-log.d3s-gi.2021-09-29.log.zip
unzip -d d3s-es/logs/store/  d3s-es/logs/store/safe-storage-log.d3s-es.2021-09-29.log.zip
unzip -d d3s-nl/logs/store/  d3s-nl/logs/store/safe-storage-log.d3s-nl.2021-09-30.log.zip

#### Store Log Hash Extract

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-14.log
awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort &gt; store-hashes.txt
awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -u &gt; store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -D &gt; store-hash-duplicates.txt

#### Generate Hashes

#### DK

awk -F "\"*,\"*" '{print $7}' ./d3s-dk/logs/submit/d3s-submission-dk.vm-ap6.2021-09-30.log | sort &gt; ./d3s-dk/logs/submit/dk-submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-dk/logs/submit/d3s-submission-dk.vm-ap6.2021-09-30.log | sort | uniq &gt; ./d3s-dk/logs/submit/dk-submission-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-dk/logs/store/storage-logs-d3s-dk-2021-09-29.log | sort &gt; ./d3s-dk/logs/store/dk-store-hashes.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-dk/logs/store/storage-logs-d3s-dk-2021-09-29.log | sort | uniq &gt; ./d3s-dk/logs/store/dk-store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-dk/logs/store/storage-logs-d3s-dk-2021-09-29.log | sort | uniq -u &gt; ./d3s-dk/logs/store/dk-store-hashes-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-dk/logs/store/storage-logs-d3s-dk-2021-09-29.log | sort | uniq -D &gt; ./d3s-dk/logs/store/dk-store-hash-duplicates.txt

#### BG

awk -F "\"*,\"*" '{print $7}' ./d3s-bg/logs/submit/d3s-submission-bg.vm-ap6.2021-09-30.log | sort &gt; ./d3s-bg/logs/submit/bg-submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-bg/logs/submit/d3s-submission-bg.vm-ap6.2021-09-30.log | sort | uniq &gt; ./d3s-bg/logs/submit/bg-submission-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-bg/logs/store/storage-logs-d3s-bg-2021-09-29.log | sort &gt; ./d3s-bg/logs/store/bg-store-hashes.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-bg/logs/store/storage-logs-d3s-bg-2021-09-29.log | sort | uniq &gt; ./d3s-bg/logs/store/bg-store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-bg/logs/store/storage-logs-d3s-bg-2021-09-29.log | sort | uniq -u &gt; ./d3s-bg/logs/store/bg-store-hashes-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-bg/logs/store/storage-logs-d3s-bg-2021-09-29.log | sort | uniq -D &gt; ./d3s-bg/logs/store/bg-store-hash-duplicates.txt

#### GI

awk -F "\"*,\"*" '{print $7}' ./d3s-gi/logs/submit/d3s-submission-gi.vm-ap6.2021-09-30.log | sort &gt; ./d3s-gi/logs/submit/gi-submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-gi/logs/submit/d3s-submission-gi.vm-ap6.2021-09-30.log | sort | uniq &gt; ./d3s-gi/logs/submit/gi-submission-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-gi/logs/store/storage-logs-d3s-gi-2021-09-29.log | sort &gt; ./d3s-gi/logs/store/gi-store-hashes.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gi/logs/store/storage-logs-d3s-gi-2021-09-29.log | sort | uniq &gt; ./d3s-gi/logs/store/gi-store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gi/logs/store/storage-logs-d3s-gi-2021-09-29.log | sort | uniq -u &gt; ./d3s-gi/logs/store/gi-store-hashes-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gi/logs/store/storage-logs-d3s-gi-2021-09-29.log | sort | uniq -D &gt; ./d3s-gi/logs/store/gi-store-hash-duplicates.txt

#### GR

awk -F "\"*,\"*" '{print $7}' ./d3s-gr/logs/submit/d3s-submission-gr.vm-ap6.2021-09-30.log | sort &gt; ./d3s-gr/logs/submit/gr-submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-gr/logs/submit/d3s-submission-gr.vm-ap6.2021-09-30.log | sort | uniq &gt; ./d3s-gr/logs/submit/gr-submission-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-gr/logs/store/storage-logs-d3s-gr-2021-09-29.log | sort &gt; ./d3s-gr/logs/store/gr-store-hashes.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gr/logs/store/storage-logs-d3s-gr-2021-09-29.log | sort | uniq &gt; ./d3s-gr/logs/store/gr-store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gr/logs/store/storage-logs-d3s-gr-2021-09-29.log | sort | uniq -u &gt; ./d3s-gr/logs/store/gr-store-hashes-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-gr/logs/store/storage-logs-d3s-gr-2021-09-29.log | sort | uniq -D &gt; ./d3s-gr/logs/store/gr-store-hash-duplicates.txt

#### ES

awk -F "\"*,\"*" '{print $7}' ./d3s-es/logs/submit/d3s-submission-es.vm-ap6.2021-09-30.log | sort &gt; ./d3s-es/logs/submit/es-submission-hashes.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-es/logs/submit/d3s-submission-es.vm-ap6.2021-09-30.log | sort | uniq &gt; ./d3s-es/logs/submit/es-submission-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-es/logs/store/storage-logs-d3s-es-2021-09-29.log | sort &gt; ./d3s-es/logs/store/es-store-hashes.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-es/logs/store/storage-logs-d3s-es-2021-09-29.log | sort | uniq &gt; ./d3s-es/logs/store/es-store-hashes-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-es/logs/store/storage-logs-d3s-es-2021-09-29.log | sort | uniq -u &gt; ./d3s-es/logs/store/es-store-hashes-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-es/logs/store/storage-logs-d3s-es-2021-09-29.log | sort | uniq -D &gt; ./d3s-es/logs/store/es-store-hash-duplicates.txt

#### NL

awk -F "\"*,\"*" '{print $7}' ./d3s-nl/logs/submit/d3s-submission-nl.vm-ap6.2021-10-01.log | sort &gt; ./d3s-nl/logs/submit/nl-submission-hashnl.txt
awk -F "\"*,\"*" '{print $7}' ./d3s-nl/logs/submit/d3s-submission-nl.vm-ap6.2021-10-01.log | sort | uniq &gt; ./d3s-nl/logs/submit/nl-submission-hashnl-unique.txt

awk -F "\"*,\"*" '{print $3}' ./d3s-nl/logs/store/storage-logs-d3s-nl-2021-09-30.log | sort &gt; ./d3s-nl/logs/store/nl-store-hashnl.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-nl/logs/store/storage-logs-d3s-nl-2021-09-30.log | sort | uniq &gt; ./d3s-nl/logs/store/nl-store-hashnl-unique.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-nl/logs/store/storage-logs-d3s-nl-2021-09-30.log | sort | uniq -u &gt; ./d3s-nl/logs/store/nl-store-hashnl-unique-only.txt
awk -F "\"*,\"*" '{print $3}' ./d3s-nl/logs/store/storage-logs-d3s-nl-2021-09-30.log | sort | uniq -D &gt; ./d3s-nl/logs/store/nl-store-hash-duplicatnl.txt








</t>
<t tx="jonathanhudson.20210916100148.1">@language md

In this scenario we needed to update a jar in Nexus as it had alerady been deployed and we didn't want to rerun the build and deploy again as reproducible build plugin was not enabled

```
mvn deploy:deploy-file -Durl=http://mn2regblm0001d0:7002/repository/maven-releases/ \
                       -DrepositoryId=nexus \
                       -Dfile=configuration-encryptor-core-1.0.0.0-RELEASE.jar \
                       -DpomFile=configuration-encryptor-core-1.0.0.0-RELEASE.pom \
                       -DgroupId=com.bet365.rps.config \
                       -DartifactId=configuration-encryptor-core \
                       -Dversion=1.0.0.0-RELEASE \
                       -Dpackaging=jar
```</t>
<t tx="jonathanhudson.20210916162241.1">@language md

### Zip Commands

#### Use to create zip and also update zip contents

```
zip &lt;file.zip&gt; files
```

#### Unzip file into folder of same name

```
unar -d &lt;file.zip&gt;
```

#### Check Integrity of zip

```
zip -t &lt;file.zip&gt;
```

#### Zip Information

```
zipinfo &lt;file.zip&gt;
```</t>
<t tx="jonathanhudson.20210916162721.1">@language md

Find unique or duplicates in file

```
awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -u &gt; store-hashes-unique.txt

awk -F "\"*,\"*" '{print $3}' storage-logs-d3s-es-2021-09-*.log | sort | uniq -D &gt; store-hash-duplicates.txt
```</t>
<t tx="jonathanhudson.20210917073524.1">@language md

#### Scripts

```
read -s PASSWD
export PASSWD
sudo PASSWD="$PASSWD" mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Regulatory\ Platform\ Services\ Technical\ Lead /mnt/windows
sudo PASSWD="$PASSWD" mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //aragorn/Teams/Regulatory\ Platform\ Services/Junior\ Regulatory\ Platform\ Services\ Developer /mnt/junior
sudo PASSWD="$PASSWD" mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //fs0001p/SAFE_DATA /mnt/SAFE_DATA_RO
```

</t>
<t tx="jonathanhudson.20210917073624.1">id -u jonathanhudson
247320266

id -g jonathanhudson
247200513

mount -t cifs -o username=jonathanhudson,dir_mode=0770,file_mode=0770,uid=247320266,gid=1101,iocharset=utf8 //fs0001p/SAFE_DATA /mnt/SAFE_DATA_RO
</t>
<t tx="jonathanhudson.20210924095213.1">@language md

#### Updating Limits

Notify Limits  

End of /etc/sysctl.conf add the line:  

```
fs.inotify.max_user_watches = 5242881
```
Apply the changes with the command  

```
sysctl -p
```
</t>
<t tx="jonathanhudson.20210927153358.1">@language md

#### Notes

Debug net
```
-Djavax.net.debug=ssl
```
#### Test script
```
# java -cp . SSLPoke rpuat.b365uat.com 443

# /opt/SoapUI-5.4.0/jre/bin/java -cp . SSLPoke rpuat.b365uat.com 443

# -Djavax.net.ssl.keyStore=path/to/keystore.jks

java -Djavax.net.ssl.trustStore=/data/code/safe-es-recon-test/../safe-certificates-common/src/environments/UAT/truststores/D3S_DSS/client/uat-client-truststore.jks -cp . SSLPoke rpuat.b365uat.com 443

```

#### Test code
```
import javax.net.ssl.SSLSocket;
import javax.net.ssl.SSLSocketFactory;
import java.io.*;

public class SSLPoke 
{
    public static void main(String[] args) 
    {
		if (args.length != 2) 
		{
			System.out.println("Usage: "+SSLPoke.class.getName()+" &lt;host&gt; &lt;port&gt;");
			System.exit(1);
		}
		try 
		{
			SSLSocketFactory sslsocketfactory = (SSLSocketFactory) SSLSocketFactory.getDefault();
			SSLSocket sslsocket = (SSLSocket) sslsocketfactory.createSocket(args[0], Integer.parseInt(args[1]));

			InputStream in = sslsocket.getInputStream();
			OutputStream out = sslsocket.getOutputStream();

			// Write a test byte to get a reaction :)
			out.write(1);

			while (in.available() &gt; 0) {
				System.out.print(in.read());
			}
			System.out.println("Successfully connected");

		} 
		catch (Exception exception) 
		{
			exception.printStackTrace();
		}
	}
}


```</t>
<t tx="jonathanhudson.20210930102844.1">&lt;?xml version="1.0" encoding="UTF-8"?&gt;

&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:util="http://www.springframework.org/schema/util"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd"&gt;

    &lt;bean name="SafeStorageLogTask" class="com.bet365.rps.safe.scheduler.task.ScheduledTask"&gt;
        &lt;property name="task"&gt;
            &lt;bean class="com.bet365.rps.task.safestoragelog.SafeStorageLogTask"&gt;
                &lt;property name="dataDirectory" value="/mnt/SAFE_DATA"/&gt;
                &lt;property name="logDirectory" value="/mnt/SAFE_DATA"/&gt;
                &lt;property name="logLookBack" value="3"/&gt;
                &lt;property name="zipPasswords" value="(enc)RytSbkXRKoJLCQofHMJuO9y+gig=.9LQ6fRNKjO6yKlh5.EllurZ4ly10oPri14vuuPvUqlKBs/3Mt"/&gt;
                &lt;property name="tenants"&gt;
                    &lt;util:map map-class="java.util.HashMap" key-type="java.lang.String" value-type="java.lang.String"&gt;
                        &lt;entry key="d3s-dk" value="Dictao1,Dictao2"/&gt;
                        &lt;entry key="d3s-bg" value="Dictao1,Dictao2"/&gt;
                        &lt;entry key="d3s-es" value="Dictao1,Dictao2"/&gt;
                        &lt;entry key="d3s-gi" value="Dictao1,Dictao2"/&gt;
                        &lt;entry key="d3s-gr" value="Dictao1,Dictao2"/&gt;
                    &lt;/util:map&gt;
                &lt;/property&gt;
                &lt;property name="nlKeyStorePassword"
                          value="encoded password here"/&gt;
                &lt;property name="nlTrustStorePassword"
                          value="encoded password here"/&gt;
                &lt;property name="coPrivateKeyPassword"
                          value="encoded password here"/&gt;
            &lt;/bean&gt;
        &lt;/property&gt;
        &lt;!--property name="cronExpression" value="0 1 0 * * *"/--&gt;
        &lt;property name="cronExpression" value="0 0/5 * * * *"/&gt;
        &lt;property name="startupMode" value="AUTOMATIC"/&gt;
        &lt;property name="retryPolicy"&gt;
            &lt;bean class="com.bet365.rps.safe.scheduler.task.RetryPolicy"&gt;
                &lt;property name="maxAttempts" value="5"/&gt;
                &lt;property name="interval" value="5"/&gt;
                &lt;property name="timeUnit" value="MINUTES"/&gt;
            &lt;/bean&gt;
        &lt;/property&gt;
        &lt;property name="clusterPolicy"&gt;
            &lt;bean class="com.bet365.rps.safe.scheduler.cluster.ClusterPolicy"&gt;
                &lt;property name="clusterId" value="SafeStorageLogTask-LCL"/&gt;
                &lt;property name="lockSuffix" value=""/&gt;
                &lt;property name="lockTimeout" value="60"/&gt;
                &lt;property name="timeUnit" value="MINUTES"/&gt;
                &lt;property name="cacheType" value="LOCAL"/&gt;
            &lt;/bean&gt;
        &lt;/property&gt;
    &lt;/bean&gt;

&lt;/beans&gt;
</t>
<t tx="jonathanhudson.20211001140846.1">@language md

Extract certificates from endpoint
```
openssl s_client -showcerts -verify 5 -connect stackexchange.com:443 &lt; /dev/null
```</t>
<t tx="jonathanhudson.20211001141004.1">@language md

#### Starting stopping

systemctl start docker

chmod o+rw /var/run/docker.sock

minikube delete

minikube start --driver=docker --embed-certs
minikube start --addons=ingress --cni=flannel --install-addons=true --driver=docker --embed-certs
minikube start --addons=ingress --cpus=4 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g --driver=docker --embed-certs

kubectl get nodes
kubectl get pods -A

minikube stop

minikube status

#### Deploying

kubectl apply -f https://raw.githubusercontent.com/ansible/awx-operator/0.13.0/deploy/awx-operator.yaml

kubectl apply -f awx-linux-postgres-secret.yml
kubectl apply -f awx-linux.yml

kubectl describe pod awx-operator-69c646c48f-674xg

kubectl delete deployment awx-operator

#### Managing

kubectl logs -f deployment/awx-operator

kubectl get pods -l "app.kubernetes.io/managed-by=awx-operator"

kubectl logs &lt;pod&gt;

kubectl describe &lt;pod&gt;

minikube service awx-local-service --url

##### Delete pods

kubectl delete pods awx-linux-postgres-0 --force

###### Disabling scheduling

kubectl cordon minikube 

###### Enabled scheduling

kubectl uncordon minikube

###### WIP Skip Verify
--insecure-skip-tls-verify

##### Deleting bad deployments

kubectl get all




</t>
<t tx="jonathanhudson.20211001144458.1">@language md


```
---
apiVersion: awx.ansible.com/v1beta1
kind: AWX
metadata:
  name: awx-linux
spec:
  service_type: nodeport
  ingress_type: none
  hostname: vm-linlap6.uk365office.co.uk
```

kubectl apply -f awx-linux.yml</t>
<t tx="jonathanhudson.20211001152121.1">@language md

```
kind: Service
apiVersion: v1
metadata:
  name: postgres
  namespace: default
spec:
  type: ExternalName
  # https://docs.docker.com/docker-for-mac/networking/#use-cases-and-workarounds
  externalName: host.docker.internal
  ports:
    - name: port
      port: 5432
```

and then to apply 

```
kubectl apply -f vm-postgres.yml</t>
<t tx="jonathanhudson.20211001155941.1">@language md

#### Kubernetes Access External Services
ManagedKube

ManagedKube

Sep 25, 2018·2 min read

There are often times when you will need to access an external service in Kubernetes but still want to use a static name.

For example, you have an external database like an AWS RDS (MySQL) hosted by Amazon. In your application, you simply want to refer to this database by the name mysql and not the ful URL of the name that AWS assigns to it.

You can add an external service mapping a hostname or by an IP.
Mapping by a hostname (CNAME)

You want your application to use the hostname mysql which will redirect it to mysql–instance1.123456789012.us-east-1.rds.amazonaws.com. We can have Kubernetes set this CNAME.

```
kind: Service
apiVersion: v1
metadata:
  name: mysql
spec:
  type: ExternalName
  externalName: mysql–instance1.123456789012.us-east-1.rds.amazonaws.com
```

Now, if you go to your pod, you can look up mysql and see that it points to mysql–instance1.123456789012.us-east-1.rds.amazonaws.com:

dig mysql

Mapping a hostname to an IP

You want your application to use the hostname mysql which will redirect to an IP address. We can have Kubernetes set this up for us:

```
---
kind: "Service"
apiVersion: "v1"
metadata:
  name: "mysql"
spec:
  ports:
    -
      name: "mysql"
      protocol: "TCP"
      port: 3306
      targetPort: 3306
      nodePort: 0---
kind: "Endpoints"
apiVersion: "v1"
metadata:
  name: "mysql"
subsets:
  -
    addresses:
      -
        ip: "1.1.1.1"
    ports:
      -
        port: 3306
        name: "mysql"
```
In your pod, you can check the connectivity. This will map the hostname mysql to the IP address 1.1.1.1.
</t>
<t tx="jonathanhudson.20211001162457.1">@language md

```
---
apiVersion: awx.ansible.com/v1beta1
kind: AWX
metadata:
  name: awx-linux
spec:
  service_type: nodeport
  ingress_type: none
  hostname: vm-linlap6.uk365office.co.uk
```

</t>
<t tx="jonathanhudson.20211001162513.1">@language md

```
---
apiVersion: v1
kind: Secret
metadata:
  name: awx-linux-postgres-configuration
  namespace: default
stringData:
  host: host.docker.internal
  port: '5432'
  database: 'awxdb'
  username: 'awxuser'
  password: 'password'
  sslmode: prefer
  type: unmanaged
type: Opaque
```
</t>
<t tx="jonathanhudson.20211001164416.1">@language md

```
psql -U postgres

createdb -U postgres awxdb

psql -U postgres -d awxdb

CREATE USER awxuser PASSWORD 'password';

GRANT ALL PRIVILEGES ON DATABASE awxdb TO awxuser;

SELECT * FROM pg_catalog.pg_tables where tableowner = 'awxuser';
```
</t>
</tnodes>
</leo_file>
